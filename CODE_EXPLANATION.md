# ğŸ“– GIáº¢I THÃCH CHI TIáº¾T TOÃ€N Bá»˜ CODE

## ğŸ“ Má»¤C Lá»¤C

1. [config.py - Cáº¥u hÃ¬nh toÃ n bá»™ project](#1-configpy)
2. [utils.py - CÃ¡c hÃ m tiá»‡n Ã­ch](#2-utilspy)
3. [data_loader.py - Xá»­ lÃ½ dá»¯ liá»‡u vÃ  DataLoader](#3-data_loaderpy)
4. [Luá»“ng hoáº¡t Ä‘á»™ng tá»•ng thá»ƒ](#4-luá»“ng-hoáº¡t-Ä‘á»™ng-tá»•ng-thá»ƒ)
5. [CÃ¢u há»i thÆ°á»ng gáº·p](#5-cÃ¢u-há»i-thÆ°á»ng-gáº·p)

---

## 1. config.py

### ğŸ¯ Má»¥c Ä‘Ã­ch
File cáº¥u hÃ¬nh chá»©a **Táº¤T Cáº¢** cÃ¡c tham sá»‘ cá»§a project. Khi muá»‘n thay Ä‘á»•i batch size, learning rate, sá»‘ epochs... chá»‰ cáº§n sá»­a file nÃ y.

### ğŸ“ Chi tiáº¿t tá»«ng pháº§n

#### **1.1. Import vÃ  Path Configuration**

```python
import torch
from pathlib import Path
import os

# TÆ°Æ¡ng thÃ­ch cáº£ local vÃ  Colab
try:
    # Náº¿u cháº¡y tá»« file .py (local)
    BASE_DIR = Path(__file__).parent.parent
except NameError:
    # Náº¿u cháº¡y trÃªn Colab/Jupyter (khÃ´ng cÃ³ __file__)
    BASE_DIR = Path("/content")
```

**Giáº£i thÃ­ch:**
- `__file__`: Biáº¿n Python chá»©a Ä‘Æ°á»ng dáº«n file hiá»‡n táº¡i
- **Local**: `BASE_DIR = d:\Corel\HK1_NAM3\NLP\NLP_DO_AN\`
- **Colab**: `BASE_DIR = /content/`
- `try-except`: Xá»­ lÃ½ lá»—i khi `__file__` khÃ´ng tá»“n táº¡i (Colab/Jupyter)

**Táº¡i sao cáº§n?**
- Code cháº¡y Ä‘Æ°á»£c cáº£ local vÃ  Colab mÃ  khÃ´ng cáº§n sá»­a Ä‘Æ°á»ng dáº«n

---

#### **1.2. Data Files**

```python
DATA_DIR = BASE_DIR / "data"
CHECKPOINT_DIR = BASE_DIR / "check_point"

TRAIN_EN = DATA_DIR / "train.en"
TRAIN_FR = DATA_DIR / "train.fr"
VAL_EN = DATA_DIR / "val.en"
VAL_FR = DATA_DIR / "val.fr"
TEST_EN = DATA_DIR / "test.en"
TEST_FR = DATA_DIR / "test.fr"
```

**Giáº£i thÃ­ch:**
- `Path()` object: Tá»± Ä‘á»™ng xá»­ lÃ½ dáº¥u `/` hoáº·c `\` tÃ¹y há»‡ Ä‘iá»u hÃ nh
- `TRAIN_EN`: ÄÆ°á»ng dáº«n tá»›i file train.en
- `TRAIN_FR`: File train.fr tÆ°Æ¡ng á»©ng (cÃ¹ng sá»‘ dÃ²ng vá»›i train.en)

**VÃ­ dá»¥:**
```
train.en dÃ²ng 1: "A man is walking."
train.fr dÃ²ng 1: "Un homme marche."
```

---

#### **1.3. Vocabulary Configuration**

```python
MAX_VOCAB_SIZE = 10000
MIN_FREQ = 1

PAD_TOKEN = "<pad>"  # Padding (Ä‘á»™ dÃ i khÃ´ng Ä‘á»§)
UNK_TOKEN = "<unk>"  # Unknown word (tá»« khÃ´ng cÃ³ trong vocab)
SOS_TOKEN = "<sos>"  # Start of sentence
EOS_TOKEN = "<eos>"  # End of sentence

SPECIAL_TOKENS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]

PAD_IDX = 0
UNK_IDX = 1
SOS_IDX = 2
EOS_IDX = 3
```

**Giáº£i thÃ­ch:**

1. **MAX_VOCAB_SIZE = 10000**:
   - Chá»‰ giá»¯ 10,000 tá»« phá»• biáº¿n nháº¥t
   - Tá»« hiáº¿m â†’ thay báº±ng `<unk>`
   - **Táº¡i sao?** Giáº£m kÃ­ch thÆ°á»›c model, tÄƒng tá»‘c training

2. **Special Tokens**:
   ```
   <pad>: ThÃªm vÃ o cÃ¢u ngáº¯n Ä‘á»ƒ báº±ng cÃ¢u dÃ i nháº¥t trong batch
   <unk>: Thay tháº¿ tá»« khÃ´ng biáº¿t
   <sos>: ÄÃ¡nh dáº¥u báº¯t Ä‘áº§u cÃ¢u (Start Of Sentence)
   <eos>: ÄÃ¡nh dáº¥u káº¿t thÃºc cÃ¢u (End Of Sentence)
   ```

3. **VÃ­ dá»¥:**
   ```python
   CÃ¢u gá»‘c: ["hello", "world"]
   Sau khi thÃªm: ["<sos>", "hello", "world", "<eos>"]
   Encode: [2, 245, 567, 3]
   
   CÃ¢u ngáº¯n hÆ¡n: ["hi"]
   ThÃªm: ["<sos>", "hi", "<eos>", "<pad>"]
   Encode: [2, 123, 3, 0]
   ```

---

#### **1.4. Model Configuration**

```python
EMBEDDING_DIM = 256
HIDDEN_SIZE = 512
NUM_LAYERS = 2
DROPOUT = 0.3
TEACHER_FORCING_RATIO = 0.5
```

**Giáº£i thÃ­ch:**

1. **EMBEDDING_DIM = 256**:
   - Má»—i tá»« â†’ vector 256 sá»‘ thá»±c
   - VÃ­ dá»¥: `"cat" â†’ [0.23, -0.45, 0.67, ..., 0.12]` (256 sá»‘)
   - **Táº¡i sao?** Biá»ƒu diá»…n Ã½ nghÄ©a tá»« trong khÃ´ng gian liÃªn tá»¥c

2. **HIDDEN_SIZE = 512**:
   - KÃ­ch thÆ°á»›c hidden state cá»§a LSTM
   - CÃ ng lá»›n â†’ model cÃ ng máº¡nh, nhÆ°ng cháº­m hÆ¡n

3. **NUM_LAYERS = 2**:
   - LSTM 2 táº§ng chá»“ng lÃªn nhau
   ```
   Input â†’ LSTM Layer 1 â†’ LSTM Layer 2 â†’ Output
   ```

4. **DROPOUT = 0.3**:
   - Táº¯t ngáº«u nhiÃªn 30% neurons khi training
   - **Táº¡i sao?** TrÃ¡nh overfitting

5. **TEACHER_FORCING_RATIO = 0.5**:
   - 50% thá»i gian: Decoder nháº­n tá»« Ä‘Ãºng tá»« ground truth
   - 50% thá»i gian: Decoder nháº­n tá»« dá»± Ä‘oÃ¡n cá»§a chÃ­nh nÃ³
   
   **VÃ­ dá»¥:**
   ```
   Ground truth: "le chat dort"
   
   BÆ°á»›c 1: Input <sos> â†’ Predict "le"
   BÆ°á»›c 2 (teacher forcing): Input "le" (Ä‘Ãºng) â†’ Predict "chat"
   BÆ°á»›c 3 (no teacher forcing): Input "chien" (sai) â†’ Predict "dort"
   ```

---

#### **1.5. Training Configuration**

```python
NUM_EPOCHS = 15
LEARNING_RATE = 0.001
BATCH_SIZE = 64

EARLY_STOPPING_PATIENCE = 3
SCHEDULER_PATIENCE = 2
SCHEDULER_FACTOR = 0.5
```

**Giáº£i thÃ­ch:**

1. **NUM_EPOCHS = 15**:
   - Model sáº½ xem toÃ n bá»™ data 15 láº§n
   - 1 epoch = 454 batches (29,000 / 64)

2. **LEARNING_RATE = 0.001**:
   - BÆ°á»›c nháº£y khi cáº­p nháº­t weights
   - QuÃ¡ lá»›n â†’ khÃ´ng há»™i tá»¥
   - QuÃ¡ nhá» â†’ há»c cháº­m

3. **BATCH_SIZE = 64**:
   - Xá»­ lÃ½ 64 cÃ¢u cÃ¹ng lÃºc
   - Lá»›n hÆ¡n â†’ nhanh hÆ¡n, nhÆ°ng tá»‘n RAM/VRAM

4. **EARLY_STOPPING_PATIENCE = 3**:
   ```
   Epoch 5: val_loss = 3.2 âœ… (best)
   Epoch 6: val_loss = 3.3 (khÃ´ng giáº£m, patience = 1)
   Epoch 7: val_loss = 3.4 (khÃ´ng giáº£m, patience = 2)
   Epoch 8: val_loss = 3.5 (khÃ´ng giáº£m, patience = 3)
   â†’ Dá»ªNG! TrÃ¡nh overfitting
   ```

5. **SCHEDULER_PATIENCE = 2, FACTOR = 0.5**:
   ```
   Val loss khÃ´ng giáº£m sau 2 epochs
   â†’ Learning rate = 0.001 * 0.5 = 0.0005
   ```

---

#### **1.6. Device Configuration**

```python
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

**Giáº£i thÃ­ch:**
- `cuda`: GPU (nhanh ~10-20 láº§n CPU)
- `cpu`: CPU (cháº­m, nhÆ°ng luÃ´n cÃ³)
- **Auto detect**: Náº¿u cÃ³ GPU thÃ¬ dÃ¹ng, khÃ´ng thÃ¬ dÃ¹ng CPU

**Kiá»ƒm tra:**
```python
print(DEVICE)
# Output: cuda (náº¿u cÃ³ GPU)
# Output: cpu (náº¿u khÃ´ng cÃ³ GPU)
```

---

## 2. utils.py

### ğŸ¯ Má»¥c Ä‘Ã­ch
Chá»©a cÃ¡c hÃ m tiá»‡n Ã­ch: Vocabulary class, tokenization, Ä‘á»c file, lÆ°u/load vocab.

---

### ğŸ“ Chi tiáº¿t tá»«ng class/function

#### **2.1. Class Vocabulary**

```python
class Vocabulary:
    def __init__(self, max_size=10000, min_freq=1, special_tokens=None):
        self.max_size = max_size
        self.min_freq = min_freq
        self.special_tokens = special_tokens or ["<pad>", "<unk>", "<sos>", "<eos>"]
        
        self.token2idx = {}  # word â†’ index
        self.idx2token = {}  # index â†’ word
        
        # Khá»Ÿi táº¡o special tokens
        for idx, token in enumerate(self.special_tokens):
            self.token2idx[token] = idx
            self.idx2token[idx] = token
```

**Giáº£i thÃ­ch:**
- `token2idx`: Dictionary mapping tá»« â†’ sá»‘
  ```python
  {"<pad>": 0, "<unk>": 1, "<sos>": 2, "<eos>": 3, "hello": 4, "world": 5}
  ```
- `idx2token`: Dictionary mapping sá»‘ â†’ tá»«
  ```python
  {0: "<pad>", 1: "<unk>", 2: "<sos>", 3: "<eos>", 4: "hello", 5: "world"}
  ```

---

#### **2.2. Vocabulary.build_vocab_from_iterator()**

```python
def build_vocab_from_iterator(self, iterator):
    # Äáº¿m táº§n suáº¥t
    counter = Counter()
    for tokens in iterator:
        counter.update(tokens)
    
    # Loáº¡i bá» special tokens náº¿u cÃ³ trong data
    for special in self.special_tokens:
        if special in counter:
            del counter[special]
    
    # Láº¥y top max_size - 4 tá»« phá»• biáº¿n nháº¥t
    most_common = counter.most_common(self.max_size - len(self.special_tokens))
    
    # ThÃªm vÃ o vocabulary
    for idx, (token, freq) in enumerate(most_common, start=len(self.special_tokens)):
        if freq >= self.min_freq:
            self.token2idx[token] = idx
            self.idx2token[idx] = token
```

**Giáº£i thÃ­ch tá»«ng bÆ°á»›c:**

**BÆ°á»›c 1: Äáº¿m táº§n suáº¥t**
```python
Input: [["hello", "world"], ["hello", "hi"], ["world", "peace"]]

Counter: {"hello": 2, "world": 2, "hi": 1, "peace": 1}
```

**BÆ°á»›c 2: Sáº¯p xáº¿p theo táº§n suáº¥t**
```python
most_common = [("hello", 2), ("world", 2), ("hi", 1), ("peace", 1)]
```

**BÆ°á»›c 3: Láº¥y top 10,000 - 4 = 9,996 tá»«**
```python
# Giáº£ sá»­ max_size = 10
# ÄÃ£ cÃ³ 4 special tokens
# â†’ Chá»‰ láº¥y 6 tá»« tiáº¿p theo

most_common[:6]
```

**BÆ°á»›c 4: Táº¡o mapping**
```python
token2idx = {
    "<pad>": 0,
    "<unk>": 1,
    "<sos>": 2,
    "<eos>": 3,
    "hello": 4,   # Tá»« phá»• biáº¿n nháº¥t
    "world": 5,   # Tá»« phá»• biáº¿n thá»© 2
    ...
}
```

---

#### **2.3. Vocabulary.encode() vÃ  decode()**

```python
def encode(self, tokens: List[str]) -> List[int]:
    return [self.token2idx.get(token, self.unk_idx) for token in tokens]

def decode(self, indices: List[int]) -> List[str]:
    return [self.idx2token.get(idx, "<unk>") for idx in indices]
```

**VÃ­ dá»¥:**

```python
vocab = Vocabulary()
vocab.token2idx = {"<pad>": 0, "<unk>": 1, "<sos>": 2, "<eos>": 3, "hello": 4}

# Encode
tokens = ["<sos>", "hello", "xyz", "<eos>"]
indices = vocab.encode(tokens)
# Output: [2, 4, 1, 3]
# "xyz" khÃ´ng cÃ³ trong vocab â†’ 1 (<unk>)

# Decode
indices = [2, 4, 1, 3]
tokens = vocab.decode(indices)
# Output: ["<sos>", "hello", "<unk>", "<eos>"]
```

---

#### **2.4. tokenize_sentence()**

```python
def tokenize_sentence(sentence: str, language: str = "en") -> List[str]:
    # Lowercase
    sentence = sentence.lower()
    
    # ThÃªm space trÆ°á»›c dáº¥u cÃ¢u
    sentence = re.sub(r"([.!?;,])", r" \1", sentence)
    
    # Split by whitespace
    tokens = sentence.split()
    
    return tokens
```

**VÃ­ dá»¥:**

```python
Input: "Hello, how are you?"

BÆ°á»›c 1: Lowercase
â†’ "hello, how are you?"

BÆ°á»›c 2: ThÃªm space trÆ°á»›c dáº¥u cÃ¢u
â†’ "hello , how are you ?"

BÆ°á»›c 3: Split
â†’ ["hello", ",", "how", "are", "you", "?"]
```

**Táº¡i sao tÃ¡ch dáº¥u cÃ¢u?**
- Má»—i dáº¥u cÃ¢u lÃ  1 token riÃªng
- Model há»c Ä‘Æ°á»£c Ã½ nghÄ©a dáº¥u cÃ¢u (cÃ¢u há»i, ngáº¡c nhiÃªn...)

---

#### **2.5. read_parallel_corpus()**

```python
def read_parallel_corpus(src_file: str, tgt_file: str, tokenize_fn=tokenize_sentence):
    src_sentences = []
    tgt_sentences = []
    
    with open(src_file, 'r', encoding='utf-8') as f_src, \
         open(tgt_file, 'r', encoding='utf-8') as f_tgt:
        
        for src_line, tgt_line in zip(f_src, f_tgt):
            src_line = src_line.strip()
            tgt_line = tgt_line.strip()
            
            if src_line and tgt_line:
                src_tokens = tokenize_fn(src_line, language="en")
                tgt_tokens = tokenize_fn(tgt_line, language="fr")
                
                src_sentences.append(src_tokens)
                tgt_sentences.append(tgt_tokens)
    
    return src_sentences, tgt_sentences
```

**Giáº£i thÃ­ch:**

```python
File train.en:
  DÃ²ng 1: "A man is walking."
  DÃ²ng 2: "The cat is sleeping."

File train.fr:
  DÃ²ng 1: "Un homme marche."
  DÃ²ng 2: "Le chat dort."

Sau khi Ä‘á»c:
src_sentences = [
    ["a", "man", "is", "walking", "."],
    ["the", "cat", "is", "sleeping", "."]
]

tgt_sentences = [
    ["un", "homme", "marche", "."],
    ["le", "chat", "dort", "."]
]
```

**Táº¡i sao dÃ¹ng `zip()`?**
- Äá»c 2 file song song, Ä‘áº£m báº£o dÃ²ng 1 file A tÆ°Æ¡ng á»©ng dÃ²ng 1 file B

---

#### **2.6. add_special_tokens()**

```python
def add_special_tokens(tokens: List[str], add_sos=True, add_eos=True) -> List[str]:
    result = tokens.copy()
    if add_sos:
        result = ["<sos>"] + result
    if add_eos:
        result = result + ["<eos>"]
    return result
```

**VÃ­ dá»¥:**

```python
Input: ["hello", "world"]

add_special_tokens(tokens, add_sos=True, add_eos=True)
â†’ ["<sos>", "hello", "world", "<eos>"]

add_special_tokens(tokens, add_sos=False, add_eos=True)
â†’ ["hello", "world", "<eos>"]
```

**Táº¡i sao cáº§n?**
- `<sos>`: BÃ¡o cho Decoder biáº¿t báº¯t Ä‘áº§u sinh cÃ¢u
- `<eos>`: BÃ¡o cho Decoder biáº¿t dá»«ng láº¡i

---

#### **2.7. save_vocab() vÃ  load_vocab()**

```python
def save_vocab(vocab, filepath):
    torch.save({
        'token2idx': vocab.token2idx,
        'idx2token': vocab.idx2token,
        'max_size': vocab.max_size,
        'min_freq': vocab.min_freq,
        'special_tokens': vocab.special_tokens
    }, filepath)
```

**Giáº£i thÃ­ch:**
- LÆ°u toÃ n bá»™ thÃ´ng tin vocab vÃ o file `.pth`
- **Táº¡i sao cáº§n?** KhÃ´ng pháº£i build láº¡i vocab má»—i láº§n cháº¡y

```python
# LÆ°u
save_vocab(src_vocab, "src_vocab.pth")

# Load
src_vocab = load_vocab("src_vocab.pth")
```

---

## 3. data_loader.py

### ğŸ¯ Má»¥c Ä‘Ã­ch
Xá»­ lÃ½ dá»¯ liá»‡u thÃ nh batch, sáº¯p xáº¿p, padding Ä‘á»ƒ Ä‘Æ°a vÃ o model.

---

### ğŸ“ Chi tiáº¿t tá»«ng class/function

#### **3.1. Class TranslationDataset**

```python
class TranslationDataset(Dataset):
    def __init__(self, src_sentences, tgt_sentences):
        assert len(src_sentences) == len(tgt_sentences)
        self.src_sentences = src_sentences
        self.tgt_sentences = tgt_sentences
    
    def __len__(self):
        return len(self.src_sentences)
    
    def __getitem__(self, idx):
        return self.src_sentences[idx], self.tgt_sentences[idx]
```

**Giáº£i thÃ­ch:**
- PyTorch Dataset wrapper
- `__getitem__`: Tráº£ vá» 1 cáº·p cÃ¢u (EN, FR) táº¡i index `idx`

**VÃ­ dá»¥:**

```python
src = [["hello", "world"], ["hi", "there"]]
tgt = [["bonjour", "monde"], ["salut", "lÃ "]]

dataset = TranslationDataset(src, tgt)

print(len(dataset))  # 2
print(dataset[0])    # (["hello", "world"], ["bonjour", "monde"])
```

---

#### **3.2. collate_batch_with_packing()**

**ÄÃ¢y lÃ  hÃ m QUAN TRá»ŒNG NHáº¤T!** Xá»­ lÃ½ 1 batch data.

```python
def collate_batch_with_packing(batch, src_vocab, tgt_vocab, device, max_len=50):
    # BÆ°á»›c 1: ThÃªm special tokens vÃ  encode
    batch_data = []
    for src_tokens, tgt_tokens in batch:
        src_tokens = add_special_tokens(src_tokens[:max_len-2], add_sos=True, add_eos=True)
        tgt_tokens = add_special_tokens(tgt_tokens[:max_len-2], add_sos=True, add_eos=True)
        
        src_indices = src_vocab.encode(src_tokens)
        tgt_indices = tgt_vocab.encode(tgt_tokens)
        
        batch_data.append((src_indices, len(src_indices), tgt_indices, len(tgt_indices)))
    
    # BÆ°á»›c 2: Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n
    batch_data.sort(key=lambda x: x[1], reverse=True)
    
    # BÆ°á»›c 3: Padding
    # ... (chi tiáº¿t bÃªn dÆ°á»›i)
```

**Giáº£i thÃ­ch tá»«ng bÆ°á»›c:**

### **BÆ°á»›c 1: Encode sentences**

```python
Input batch (2 cÃ¢u):
[
    (["hello", "world"], ["bonjour", "monde"]),
    (["hi"], ["salut"])
]

Sau khi thÃªm special tokens:
[
    (["<sos>", "hello", "world", "<eos>"], ["<sos>", "bonjour", "monde", "<eos>"]),
    (["<sos>", "hi", "<eos>"], ["<sos>", "salut", "<eos>"])
]

Sau khi encode:
[
    ([2, 523, 890, 3], 4, [2, 312, 567, 3], 4),
    ([2, 124, 3], 3, [2, 234, 3], 3)
]
```

### **BÆ°á»›c 2: Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n**

```python
TrÆ°á»›c sáº¯p xáº¿p:
[
    ([2, 523, 890, 3], 4, ...),  # Äá»™ dÃ i 4
    ([2, 124, 3], 3, ...)        # Äá»™ dÃ i 3
]

Sau sáº¯p xáº¿p:
[
    ([2, 523, 890, 3], 4, ...),  # CÃ¢u dÃ i nháº¥t lÃªn Ä‘áº§u
    ([2, 124, 3], 3, ...)        # CÃ¢u ngáº¯n hÆ¡n xuá»‘ng dÆ°á»›i
]
```

**Táº¡i sao pháº£i sáº¯p xáº¿p?**
- `pack_padded_sequence` yÃªu cáº§u batch pháº£i sáº¯p xáº¿p giáº£m dáº§n
- GiÃºp LSTM xá»­ lÃ½ hiá»‡u quáº£ hÆ¡n (bá» qua padding tokens)

### **BÆ°á»›c 3: Padding**

```python
max_src_len = 4
max_tgt_len = 4

Padding source:
[2, 523, 890, 3]       â†’ [2, 523, 890, 3]      (Ä‘á»§ dÃ i)
[2, 124, 3]            â†’ [2, 124, 3, 0]        (thÃªm 1 padding)

Padding target:
[2, 312, 567, 3]       â†’ [2, 312, 567, 3]
[2, 234, 3]            â†’ [2, 234, 3, 0]
```

### **BÆ°á»›c 4: Chuyá»ƒn sang tensor**

```python
src_batch = torch.tensor([
    [2, 523, 890, 3],
    [2, 124, 3, 0]
], device='cuda')

src_lengths = torch.tensor([4, 3], device='cpu')  # Pháº£i CPU!
```

**Táº¡i sao lengths pháº£i á»Ÿ CPU?**
- `pack_padded_sequence` yÃªu cáº§u lengths á»Ÿ CPU
- PyTorch bug náº¿u Ä‘á»ƒ GPU

---

#### **3.3. build_vocabularies()**

```python
def build_vocabularies(train_src_file, train_tgt_file, max_vocab_size=10000):
    # Äá»c training data
    src_sentences, tgt_sentences = read_parallel_corpus(
        train_src_file, 
        train_tgt_file,
        tokenize_fn=tokenize_sentence
    )
    
    # Build source vocabulary
    src_vocab = Vocabulary(max_size=max_vocab_size, ...)
    src_vocab.build_vocab_from_iterator(src_sentences)
    
    # Build target vocabulary
    tgt_vocab = Vocabulary(max_size=max_vocab_size, ...)
    tgt_vocab.build_vocab_from_iterator(tgt_sentences)
    
    return src_vocab, tgt_vocab
```

**Giáº£i thÃ­ch:**
1. Äá»c file train.en vÃ  train.fr
2. Tokenize táº¥t cáº£ cÃ¢u
3. Build 2 vocabularies riÃªng biá»‡t (EN vÃ  FR)
4. Tráº£ vá» 2 vocab objects

**Táº¡i sao build 2 vocab riÃªng?**
- Tiáº¿ng Anh vÃ  tiáº¿ng PhÃ¡p cÃ³ tá»« vá»±ng khÃ¡c nhau
- Má»—i ngÃ´n ngá»¯ 10,000 tokens riÃªng

---

#### **3.4. prepare_data_loaders()**

```python
def prepare_data_loaders(src_vocab, tgt_vocab, batch_size=64):
    # Load data
    train_src, train_tgt = read_parallel_corpus(TRAIN_EN, TRAIN_FR)
    train_dataset = TranslationDataset(train_src, train_tgt)
    
    # Create collate function
    def collate_fn_wrapper(batch):
        return collate_batch_with_packing(
            batch, src_vocab, tgt_vocab, DEVICE, MAX_SEQ_LENGTH
        )
    
    # Create DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn_wrapper,
        pin_memory=False
    )
    
    return train_loader, val_loader, test_loader
```

**Giáº£i thÃ­ch:**

1. **Load data**: Äá»c file vÃ  táº¡o Dataset
2. **collate_fn_wrapper**: Wrapper Ä‘á»ƒ truyá»n thÃªm tham sá»‘ vÃ o collate function
3. **DataLoader**: PyTorch DataLoader
   - `shuffle=True`: Trá»™n ngáº«u nhiÃªn data (chá»‰ cho train)
   - `collate_fn`: HÃ m xá»­ lÃ½ batch
   - `pin_memory=False`: KhÃ´ng pin memory (vÃ¬ Ä‘Ã£ chuyá»ƒn lÃªn GPU trong collate_fn)

**Output:**

```python
train_loader: 454 batches (29,000 / 64)
val_loader: 16 batches (1,014 / 64)
test_loader: 16 batches (1,000 / 64)
```

---

## 4. Luá»“ng hoáº¡t Ä‘á»™ng tá»•ng thá»ƒ

### ğŸ”„ **Tá»« file data â†’ Model input**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 1: Äá»ŒC FILE                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
train.en: "A man is walking in the street."
train.fr: "Un homme marche dans la rue."
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 2: TOKENIZATION                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
EN: ["a", "man", "is", "walking", "in", "the", "street", "."]
FR: ["un", "homme", "marche", "dans", "la", "rue", "."]
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 3: BUILD VOCABULARY (chá»‰ cháº¡y 1 láº§n)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
src_vocab: {"<pad>":0, "<unk>":1, ..., "man":523, "walking":1247}
tgt_vocab: {"<pad>":0, "<unk>":1, ..., "homme":312, "marche":456}
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 4: ADD SPECIAL TOKENS                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
EN: ["<sos>", "a", "man", "is", "walking", ..., ".", "<eos>"]
FR: ["<sos>", "un", "homme", "marche", ..., ".", "<eos>"]
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 5: ENCODE (word â†’ index)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
EN: [2, 12, 523, 45, 1247, 89, 12, 678, 5, 3]
FR: [2, 312, 456, 234, 67, 445, 5, 3]
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 6: Táº O BATCH (64 cÃ¢u)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
batch = [
    (cÃ¢u_1_EN, cÃ¢u_1_FR),
    (cÃ¢u_2_EN, cÃ¢u_2_FR),
    ...
    (cÃ¢u_64_EN, cÃ¢u_64_FR)
]
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 7: Sáº®P Xáº¾P THEO Äá»˜ DÃ€I GIáº¢M Dáº¦N                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
CÃ¢u dÃ i nháº¥t lÃªn Ä‘áº§u, cÃ¢u ngáº¯n nháº¥t xuá»‘ng cuá»‘i
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 8: PADDING                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ThÃªm <pad> (0) vÃ o cÃ¢u ngáº¯n Ä‘á»ƒ báº±ng cÃ¢u dÃ i nháº¥t
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BÆ¯á»šC 9: CHUYá»‚N SANG TENSOR GPU                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
src_batch: torch.tensor([[...], [...], ...], device='cuda')
src_lengths: torch.tensor([25, 23, 20, ...], device='cpu')
tgt_batch: torch.tensor([[...], [...], ...], device='cuda')
tgt_lengths: torch.tensor([28, 25, 22, ...], device='cpu')
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sáº´N SÃ€NG CHO MODEL!                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ§  **Trong quÃ¡ trÃ¬nh training**

```python
for epoch in range(NUM_EPOCHS):
    for src_batch, src_lengths, tgt_batch, tgt_lengths in train_loader:
        # src_batch: (64, 25) - 64 cÃ¢u, má»—i cÃ¢u tá»‘i Ä‘a 25 tokens
        # src_lengths: (64,) - [25, 23, 20, ..., 8]
        # tgt_batch: (64, 28) - 64 cÃ¢u tiáº¿ng PhÃ¡p
        # tgt_lengths: (64,) - [28, 25, 22, ..., 10]
        
        # Forward pass
        output = model(src_batch, src_lengths, tgt_batch)
        
        # Compute loss
        loss = criterion(output, tgt_batch)
        
        # Backward + Update
        loss.backward()
        optimizer.step()
```

---

## 5. CÃ¢u há»i thÆ°á»ng gáº·p

### â“ **Táº¡i sao pháº£i sáº¯p xáº¿p batch theo Ä‘á»™ dÃ i giáº£m dáº§n?**

**Tráº£ lá»i:**
- `pack_padded_sequence` **yÃªu cáº§u báº¯t buá»™c** batch pháº£i sáº¯p xáº¿p giáº£m dáº§n
- Náº¿u khÃ´ng sáº¯p xáº¿p â†’ lá»—i runtime

**Lá»£i Ã­ch:**
```python
KhÃ´ng cÃ³ packing:
CÃ¢u 1: [1, 2, 3, 4, 5, 0, 0, 0]  # LSTM xá»­ lÃ½ cáº£ 8 tokens (lÃ£ng phÃ­)
CÃ¢u 2: [6, 7, 8, 0, 0, 0, 0, 0]  # LSTM xá»­ lÃ½ cáº£ 8 tokens (lÃ£ng phÃ­)

CÃ³ packing:
CÃ¢u 1: [1, 2, 3, 4, 5]  # LSTM chá»‰ xá»­ lÃ½ 5 tokens (hiá»‡u quáº£)
CÃ¢u 2: [6, 7, 8]        # LSTM chá»‰ xá»­ lÃ½ 3 tokens (hiá»‡u quáº£)
```

---

### â“ **Táº¡i sao cáº§n special tokens?**

**Tráº£ lá»i:**

1. **`<pad>` (padding)**:
   - CÃ¢u ngáº¯n cáº§n padding Ä‘á»ƒ báº±ng cÃ¢u dÃ i nháº¥t
   - Model há»c bá» qua padding (khÃ´ng tÃ­nh loss)

2. **`<unk>` (unknown)**:
   - Tá»« khÃ´ng cÃ³ trong vocab (tá»« hiáº¿m)
   - Thay báº±ng `<unk>` thay vÃ¬ bÃ¡o lá»—i

3. **`<sos>` (start of sentence)**:
   - Decoder cáº§n biáº¿t báº¯t Ä‘áº§u tá»« Ä‘Ã¢u
   - Input Ä‘áº§u tiÃªn cá»§a Decoder luÃ´n lÃ  `<sos>`

4. **`<eos>` (end of sentence)**:
   - Decoder biáº¿t khi nÃ o dá»«ng sinh tá»«
   - Khi predict `<eos>` â†’ dá»«ng láº¡i

**VÃ­ dá»¥:**
```python
Decoder:
Input: <sos> â†’ Predict: "le"
Input: "le" â†’ Predict: "chat"
Input: "chat" â†’ Predict: "dort"
Input: "dort" â†’ Predict: <eos>
â†’ Dá»«ng! Output: "le chat dort"
```

---

### â“ **Táº¡i sao lengths pháº£i á»Ÿ CPU?**

**Tráº£ lá»i:**
- Bug cá»§a PyTorch: `pack_padded_sequence` yÃªu cáº§u lengths á»Ÿ CPU
- Náº¿u Ä‘á»ƒ GPU â†’ lá»—i runtime

```python
src_lengths = torch.tensor(src_lengths, device='cpu')  # âœ… ÄÃºng
src_lengths = torch.tensor(src_lengths, device='cuda')  # âŒ Lá»—i
```

---

### â“ **Táº¡i sao pin_memory=False?**

**Tráº£ lá»i:**
- `pin_memory=True`: DÃ¹ng khi tensor á»Ÿ CPU, muá»‘n chuyá»ƒn nhanh lÃªn GPU
- NhÆ°ng trong `collate_batch_with_packing`, tensor Ä‘Ã£ á»Ÿ GPU rá»“i
- PyTorch khÃ´ng thá»ƒ pin tensor GPU â†’ lá»—i

```python
# Trong collate_fn:
src_batch = torch.tensor(..., device='cuda')  # ÄÃ£ á»Ÿ GPU

# Trong DataLoader:
pin_memory=False  # Pháº£i False vÃ¬ tensor Ä‘Ã£ á»Ÿ GPU
```

---

### â“ **Teacher forcing lÃ  gÃ¬?**

**Tráº£ lá»i:**
Ká»¹ thuáº­t training Decoder: Ä‘Ã´i khi cho Decoder nháº­n tá»« Ä‘Ãºng, Ä‘Ã´i khi cho nháº­n tá»« dá»± Ä‘oÃ¡n.

**VÃ­ dá»¥:**
```python
Ground truth: "le chat dort"

BÆ°á»›c 1: Input <sos> â†’ Predict "le"

BÆ°á»›c 2 (teacher_forcing=True):
  Input: "le" (tá»« ground truth) â†’ Predict "chat"

BÆ°á»›c 3 (teacher_forcing=False):
  Input: "chien" (tá»« dá»± Ä‘oÃ¡n sai) â†’ Predict "dort"
```

**Táº¡i sao cáº§n?**
- Teacher forcing = 1.0: Model há»c nhanh, nhÆ°ng khÃ´ng robust
- Teacher forcing = 0.0: Model há»c cháº­m, nhÆ°ng robust hÆ¡n
- Teacher forcing = 0.5: CÃ¢n báº±ng giá»¯a 2 cÃ¡i

---

### â“ **Táº¡i sao MAX_VOCAB_SIZE = 10,000?**

**Tráº£ lá»i:**

1. **Tá»« hiáº¿m xuáº¥t hiá»‡n Ã­t, khÃ´ng quan trá»ng**:
   ```
   "cat": 1,000 láº§n
   "dog": 800 láº§n
   "supercalifragilisticexpialidocious": 1 láº§n â†’ Loáº¡i bá»
   ```

2. **Giáº£m kÃ­ch thÆ°á»›c model**:
   ```
   Vocab 50,000: Embedding = 50,000 x 256 = 12.8M params
   Vocab 10,000: Embedding = 10,000 x 256 = 2.56M params
   â†’ Nháº¹ hÆ¡n 5 láº§n!
   ```

3. **Training nhanh hÆ¡n**:
   - Vocab nhá» â†’ softmax nhanh hÆ¡n
   - Vocab 10,000: ~10ms/batch
   - Vocab 50,000: ~50ms/batch

---

### â“ **Batch size lá»›n hay nhá»?**

**So sÃ¡nh:**

| Batch Size | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm |
|------------|---------|------------|
| **32** | á»”n Ä‘á»‹nh, Ã­t RAM | Cháº­m (nhiá»u iteration) |
| **64** | CÃ¢n báº±ng (khuyáº¿n nghá»‹) | - |
| **128** | Nhanh, gradient á»•n Ä‘á»‹nh | Tá»‘n RAM/VRAM |
| **256** | Ráº¥t nhanh | CÃ³ thá»ƒ khÃ´ng fit GPU |

**Khuyáº¿n nghá»‹:**
- GPU T4 (Colab): Batch size = 64-128
- GPU V100: Batch size = 128-256
- CPU: Batch size = 32

---

## ğŸ“š TÃ“M Táº®T

### ğŸ“¦ **3 file code chÃ­nh:**

1. **config.py**: Cáº¥u hÃ¬nh (hyperparameters, paths, device)
2. **utils.py**: Vocabulary, tokenization, Ä‘á»c file
3. **data_loader.py**: Dataset, DataLoader, collate function

### ğŸ”„ **Quy trÃ¬nh:**

```
File â†’ Tokenize â†’ Build Vocab â†’ Encode â†’ Batch â†’ Sort â†’ Pad â†’ Tensor â†’ Model
```

### ğŸ¯ **CÃ¡c khÃ¡i niá»‡m quan trá»ng:**

- **Vocabulary**: Mapping word â†” index
- **Special tokens**: `<pad>`, `<unk>`, `<sos>`, `<eos>`
- **Batch**: NhÃ³m 64 cÃ¢u xá»­ lÃ½ cÃ¹ng lÃºc
- **Padding**: ThÃªm `<pad>` Ä‘á»ƒ cÃ¢u ngáº¯n báº±ng cÃ¢u dÃ i
- **Sorting**: Sáº¯p xáº¿p giáº£m dáº§n Ä‘á»ƒ dÃ¹ng `pack_padded_sequence`
- **Teacher forcing**: ÄÃ´i khi cho Decoder nháº­n tá»« Ä‘Ãºng

---

**Hy vá»ng giáº£i thÃ­ch nÃ y giÃºp báº¡n hiá»ƒu rÃµ toÃ n bá»™ code!** ğŸš€

*Náº¿u cÃ²n cÃ¢u há»i gÃ¬, cá»© há»i tiáº¿p!*
