# üöÄ H∆Ø·ªöNG D·∫™N N√ÇNG BLEU L√äN 45%

## üìä M·ª•c ti√™u
- **Hi·ªán t·∫°i:** 32.67% BLEU
- **M·ª•c ti√™u:** 45% BLEU
- **C·∫ßn tƒÉng:** ~12-13%

---

## ‚úÖ ƒê√£ t·∫°o s·∫µn

### 1. `src/attention_decoder.py` (270 d√≤ng)
Ch·ª©a:
- ‚úÖ **Attention** class - Bahdanau Attention mechanism
- ‚úÖ **AttentionDecoder** class - Decoder v·ªõi attention
- ‚úÖ **Seq2SeqWithAttention** class - Model ho√†n ch·ªânh
- ‚úÖ H√†m count_parameters
- ‚úÖ Example usage

### 2. `train_with_attention.py` (200 d√≤ng)
Ch·ª©a:
- ‚úÖ CONFIG optimized (vocab=15K, epochs=20, patience=5)
- ‚úÖ build_attention_model() function
- ‚úÖ train_with_scheduler() - v·ªõi ReduceLROnPlateau
- ‚úÖ Modified train_epoch() v√† evaluate() cho attention
- ‚úÖ visualize_attention() ƒë·ªÉ v·∫Ω heatmap

---

## üî® C√ÅCH S·ª¨ D·ª§NG

### Option 1: Copy-paste v√†o Notebook hi·ªán t·∫°i (Nhanh nh·∫•t)

M·ªü notebook `NLP_Final_Project_Seq2Seq_Translation.ipynb`, th√™m c√°c cells sau:

#### Cell 1: Import Attention modules
```python
# Import Attention Decoder
import sys
sys.path.append('src')

from attention_decoder import (
    Attention, 
    AttentionDecoder, 
    Seq2SeqWithAttention
)

print("‚úÖ Imported Attention modules")
```

#### Cell 2: Build model v·ªõi Attention
```python
# ============================================
# BUILD MODEL WITH ATTENTION
# ============================================

# Update CONFIG
CONFIG['max_vocab_size'] = 15000  # TƒÉng vocab
CONFIG['num_epochs'] = 20         # TƒÉng epochs
CONFIG['early_stopping_patience'] = 5

INPUT_DIM = len(src_vocab)
OUTPUT_DIM = len(tgt_vocab)

# Encoder (gi·ªØ nguy√™n)
enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)

# Attention mechanism (NEW!)
attn = Attention(HID_DIM, HID_DIM)

# Decoder with Attention (NEW!)
dec = AttentionDecoder(
    OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM,
    N_LAYERS, DROPOUT, attn
)

# Seq2Seq with Attention
model_attention = Seq2SeqWithAttention(enc, dec, device).to(device)

# Count parameters
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'Model has {count_parameters(model_attention):,} trainable parameters')
print('‚úÖ Model with Attention created!')
```

#### Cell 3: Modified Training Functions
```python
# ============================================
# MODIFIED TRAIN/EVAL (for Attention)
# ============================================

def train_epoch_attention(model, iterator, optimizer, criterion, clip, device):
    """Train epoch for attention model"""
    model.train()
    epoch_loss = 0
    
    for batch in iterator:
        src, src_len = batch['src'].to(device), batch['src_len'].to(device)
        trg = batch['trg'].to(device)
        
        optimizer.zero_grad()
        
        # Forward (returns outputs AND attentions)
        outputs, attentions = model(src, src_len, trg)
        
        # Reshape
        output_dim = outputs.shape[-1]
        outputs = outputs[:, 1:, :].contiguous().view(-1, output_dim)
        trg = trg[:, 1:].contiguous().view(-1)
        
        loss = criterion(outputs, trg)
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)

def evaluate_attention(model, iterator, criterion, device):
    """Evaluate attention model"""
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in iterator:
            src, src_len = batch['src'].to(device), batch['src_len'].to(device)
            trg = batch['trg'].to(device)
            
            outputs, attentions = model(src, src_len, trg, teacher_forcing_ratio=0)
            
            output_dim = outputs.shape[-1]
            outputs = outputs[:, 1:, :].contiguous().view(-1, output_dim)
            trg = trg[:, 1:].contiguous().view(-1)
            
            loss = criterion(outputs, trg)
            epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)

print('‚úÖ Training functions ready!')
```

#### Cell 4: Train v·ªõi Learning Rate Scheduler
```python
# ============================================
# TRAINING WITH LR SCHEDULER
# ============================================

from torch.optim.lr_scheduler import ReduceLROnPlateau
import time
import math

# Optimizer
optimizer = optim.Adam(model_attention.parameters(), lr=CONFIG['learning_rate'])

# Loss
criterion = nn.CrossEntropyLoss(ignore_index=src_vocab.pad_idx)

# Learning Rate Scheduler
scheduler = ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=2, verbose=True
)

# Training loop
best_val_loss = float('inf')
patience_counter = 0

print("="*60)
print("  TRAINING WITH ATTENTION - Target BLEU: 45%")
print("="*60)

for epoch in range(CONFIG['num_epochs']):
    start_time = time.time()
    
    # Train
    train_loss = train_epoch_attention(
        model_attention, train_iterator, optimizer, 
        criterion, CONFIG['clip'], device
    )
    
    # Evaluate
    val_loss = evaluate_attention(
        model_attention, val_iterator, criterion, device
    )
    
    # Update LR
    scheduler.step(val_loss)
    
    end_time = time.time()
    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)
    
    # Check improvement
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        
        # Save checkpoint
        torch.save({
            'epoch': epoch,
            'model_state_dict': model_attention.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
        }, 'check_point/best_model_attention.pth')
        
        print(f'‚úÖ Saved checkpoint')
    else:
        patience_counter += 1
    
    # Print stats
    print(f'\nEpoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.0f}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
    print(f'\tVal Loss: {val_loss:.3f} | Val PPL: {math.exp(val_loss):7.3f}')
    print(f'\tPatience: {patience_counter}/{CONFIG["early_stopping_patience"]}')
    
    # Early stopping
    if patience_counter >= CONFIG['early_stopping_patience']:
        print(f'\n‚èπÔ∏è Early stopping at epoch {epoch+1}')
        break

print("\n‚úÖ Training completed!")
```

#### Cell 5: Evaluate BLEU
```python
# ============================================
# EVALUATE BLEU SCORE
# ============================================

# Load best model
checkpoint = torch.load('check_point/best_model_attention.pth')
model_attention.load_state_dict(checkpoint['model_state_dict'])

print(f"Loaded checkpoint from epoch {checkpoint['epoch']+1}")
print(f"Best validation loss: {checkpoint['val_loss']:.3f}")

# Calculate BLEU on test set
print("\n" + "="*60)
print("  CALCULATING BLEU SCORE ON TEST SET")
print("="*60)

bleu_score = calculate_bleu_on_test_set(
    test_data, src_vocab, tgt_vocab, model_attention, device
)

print("\n" + "="*60)
print(f"  üéØ FINAL BLEU SCORE: {bleu_score:.2f}%")
print("="*60)

# Compare with baseline
print(f"\nüìä Comparison:")
print(f"  Baseline (no Attention): 32.67%")
print(f"  With Attention:          {bleu_score:.2f}%")
print(f"  Improvement:             +{bleu_score - 32.67:.2f}%")

if bleu_score >= 45:
    print("\nüéâ ACHIEVED TARGET OF 45%!")
else:
    print(f"\n‚ö†Ô∏è Need +{45 - bleu_score:.2f}% more to reach 45%")
```

---

### Option 2: Ch·∫°y script Python ƒë·ªôc l·∫≠p

```bash
# Trong terminal PowerShell
cd D:\Corel\HK1_NAM3\NLP\NLP_DO_AN

# Ch·∫°y training script
python train_with_attention.py
```

**L∆∞u √Ω:** C·∫ßn s·ª≠a script ƒë·ªÉ uncomment c√°c d√≤ng load data.

---

## ‚è±Ô∏è Th·ªùi gian d·ª± ki·∫øn

- **Training:** ~2-2.5 gi·ªù (20 epochs, early stopping ~epoch 15)
- **Evaluation:** ~5-10 ph√∫t
- **T·ªïng:** ~2.5 gi·ªù

---

## üìà K·∫øt qu·∫£ mong ƒë·ª£i

### Sau khi train xong:

| Metric | Baseline | With Attention | Improvement |
|--------|----------|----------------|-------------|
| **BLEU** | 32.67% | **~44-47%** | **+12-15%** ‚úÖ |
| Train Loss | 2.85 | ~2.3-2.5 | Lower |
| Val Loss | 3.24 | ~2.7-2.9 | Lower |
| Parameters | 20M | ~23M | +3M |

### L√Ω do c·∫£i thi·ªán:

1. ‚úÖ **Attention mechanism** (+10-12%) - Gi·∫£i quy·∫øt bottleneck
2. ‚úÖ **Vocab 15K** (+1%) - Gi·∫£m <unk> tokens
3. ‚úÖ **Epochs 20** (+1-2%) - Train l√¢u h∆°n
4. ‚úÖ **LR Scheduler** (+1%) - T·ªëi ∆∞u learning rate

---

## üêõ X·ª≠ l√Ω l·ªói

### L·ªói 1: Out of Memory
```python
# Gi·∫£m batch_size
CONFIG['batch_size'] = 32  # Thay v√¨ 64
```

### L·ªói 2: Import error
```python
# Ki·ªÉm tra path
import sys
sys.path.append('src')  # ƒê·∫£m b·∫£o c√≥ d√≤ng n√†y
```

### L·ªói 3: Model architecture mismatch
```python
# ƒê·∫£m b·∫£o Encoder c√≥ method forward tr·∫£ v·ªÅ 3 values:
# encoder_outputs, hidden, cell
```

---

## üìä Visualize Attention (Bonus)

Sau khi train xong, visualize attention:

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Translate 1 c√¢u
src_sentence = "A dog is running in the grass."
src_indices = preprocess_sentence(src_sentence, src_vocab)

# Get translation + attention weights
with torch.no_grad():
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    src_len = torch.LongTensor([len(src_indices)]).to(device)
    
    # Encode
    encoder_outputs, hidden, cell = model_attention.encoder(src_tensor, src_len)
    
    # Decode step by step (collect attentions)
    attentions_list = []
    input = torch.LongTensor([tgt_vocab.sos_idx]).unsqueeze(0).to(device)
    
    for t in range(50):
        output, hidden, cell, attention = model_attention.decoder(
            input, hidden, cell, encoder_outputs
        )
        attentions_list.append(attention.cpu().numpy())
        
        pred_token = output.argmax(1).item()
        if pred_token == tgt_vocab.eos_idx:
            break
        input = torch.LongTensor([pred_token]).unsqueeze(0).to(device)

# Plot attention heatmap
attentions = np.vstack(attentions_list)

plt.figure(figsize=(10, 8))
sns.heatmap(attentions, cmap='Blues', cbar=True)
plt.xlabel('Source tokens')
plt.ylabel('Target tokens')
plt.title('Attention Weights Visualization')
plt.savefig('attention_heatmap.png', dpi=150)
plt.show()
```

---

## üéØ Next Steps n·∫øu ch∆∞a ƒë·∫°t 45%

N·∫øu sau khi train ch·ªâ ƒë·∫°t ~42-43%, th√™m:

### 1. Bidirectional Encoder (+2%)
```python
self.rnn = nn.LSTM(
    emb_dim, hid_dim, n_layers,
    bidirectional=True,  # Add this
    dropout=dropout
)
```

### 2. Beam Search (+2%)
- Implement beam_search() function
- Use beam_width=5

---

## üìù Checklist

- [ ] Copy code v√†o notebook
- [ ] Run cells 1-5 theo th·ª© t·ª±
- [ ] ƒê·ª£i training ho√†n th√†nh (~2.5 gi·ªù)
- [ ] Check BLEU score
- [ ] N·∫øu ‚â•45% ‚Üí ‚úÖ Done!
- [ ] N·∫øu <45% ‚Üí Th√™m Bidirectional Encoder

---

**Good luck! üöÄ M·ª•c ti√™u 45% trong t·∫ßm tay!**
