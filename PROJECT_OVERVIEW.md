# ğŸ“š Tá»”NG QUAN Äá»’ ÃN NLP - Dá»ŠCH MÃY ANH-PHÃP

## ğŸ¯ Má»¤C TIÃŠU BÃ€I Táº¬P

XÃ¢y dá»±ng há»‡ thá»‘ng **dá»‹ch mÃ¡y tá»± Ä‘á»™ng tá»« tiáº¿ng Anh sang tiáº¿ng PhÃ¡p** sá»­ dá»¥ng mÃ´ hÃ¬nh **Encoder-Decoder LSTM** (khÃ´ng dÃ¹ng Attention).

### YÃªu cáº§u Ä‘áº§u ra:
- âœ… Model dá»‹ch Ä‘Æ°á»£c cÃ¢u tiáº¿ng Anh â†’ tiáº¿ng PhÃ¡p
- âœ… BLEU score trÃªn táº­p test
- âœ… MÃ£ nguá»“n cháº¡y Ä‘Æ°á»£c tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i trÃªn Google Colab
- âœ… BÃ¡o cÃ¡o PDF Ä‘áº§y Ä‘á»§ (sÆ¡ Ä‘á»“, biá»ƒu Ä‘á»“, phÃ¢n tÃ­ch)
- âœ… Checkpoint file (.pth) Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng model

---

## ğŸ“Š Cáº¤U TRÃšC BÃ€I Táº¬P (10 ÄIá»‚M)

| Task | Ná»™i dung | Äiá»ƒm | Tráº¡ng thÃ¡i |
|------|----------|------|------------|
| **Task 1** | Thiáº¿t láº­p mÃ´i trÆ°á»ng, config, utils | 3.0 | âœ… HoÃ n thÃ nh |
| **Task 2** | Xá»­ lÃ½ dá»¯ liá»‡u, DataLoader | 2.0 | âœ… HoÃ n thÃ nh |
| **Task 3** | XÃ¢y dá»±ng model Encoder-Decoder | 3.0 | â³ ChÆ°a lÃ m |
| **Task 4** | Training loop vá»›i early stopping | 1.5 | â³ ChÆ°a lÃ m |
| **Task 5** | HÃ m translate() vÃ  Ä‘Ã¡nh giÃ¡ BLEU | 2.0 | â³ ChÆ°a lÃ m |
| **Task 6-8** | PhÃ¢n tÃ­ch lá»—i, bÃ¡o cÃ¡o, sÆ¡ Ä‘á»“ | 2.0 | â³ ChÆ°a lÃ m |

**Tá»•ng Ä‘iá»ƒm hiá»‡n táº¡i: 5.0/13.5**

---

## ğŸ—‚ï¸ Dá»® LIá»†U Äáº¦U VÃ€O

### Dataset: Multi30K English-French
```
data/
â”œâ”€â”€ train.en (29,000 cÃ¢u tiáº¿ng Anh)
â”œâ”€â”€ train.fr (29,000 cÃ¢u tiáº¿ng PhÃ¡p tÆ°Æ¡ng á»©ng)
â”œâ”€â”€ val.en   (1,014 cÃ¢u validation)
â”œâ”€â”€ val.fr   (1,014 cÃ¢u validation)
â”œâ”€â”€ test.en  (1,000 cÃ¢u test)
â””â”€â”€ test.fr  (1,000 cÃ¢u test)
```

### Äáº·c Ä‘iá»ƒm dá»¯ liá»‡u:
- **Parallel corpus**: Má»—i cÃ¢u tiáº¿ng Anh cÃ³ 1 cÃ¢u tiáº¿ng PhÃ¡p tÆ°Æ¡ng á»©ng
- **Äá»™ dÃ i cÃ¢u**: Trung bÃ¬nh 10-15 tá»«, tá»‘i Ä‘a 50 tá»«
- **Domain**: MÃ´ táº£ hÃ¬nh áº£nh (image captions)

---

## ğŸ”„ QUY TRÃŒNH Xá»¬LÃ - Tá»ª INPUT Äáº¾N OUTPUT

### ğŸ“¥ **BÆ¯á»šC 1: Xá»¬ LÃ Dá»® LIá»†U THÃ”**

**Input:** 
```
train.en: "A man is walking in the street."
train.fr: "Un homme marche dans la rue."
```

**Xá»­ lÃ½:**
1. **Tokenization** (tÃ¡ch tá»«):
   ```python
   EN: ["a", "man", "is", "walking", "in", "the", "street", "."]
   FR: ["un", "homme", "marche", "dans", "la", "rue", "."]
   ```

2. **Build Vocabulary** (táº¡o tá»« Ä‘iá»ƒn):
   - Äáº¿m táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»«ng tá»«
   - Láº¥y 10,000 tá»« phá»• biáº¿n nháº¥t
   - Táº¡o mapping: `word â†” index`
   ```python
   "man" â†’ 523
   "walking" â†’ 1247
   ```

3. **Encode thÃ nh sá»‘**:
   ```python
   EN: [2, 523, 45, 1247, 89, 12, 678, 5, 3]  # 2=<sos>, 3=<eos>
   FR: [2, 312, 891, 456, 234, 67, 445, 5, 3]
   ```

**Output:** 
- `src_vocab`: Tá»« Ä‘iá»ƒn tiáº¿ng Anh (10,000 tokens)
- `tgt_vocab`: Tá»« Ä‘iá»ƒn tiáº¿ng PhÃ¡p (10,000 tokens)

---

### ğŸ“¦ **BÆ¯á»šC 2: Táº O BATCHES**

**Input:** 29,000 cÃ¢u Ä‘Ã£ encode

**Xá»­ lÃ½:**
1. **Chia thÃ nh batches** (64 cÃ¢u/batch):
   ```
   Batch 1: 64 cÃ¢u
   Batch 2: 64 cÃ¢u
   ...
   Batch 454: 64 cÃ¢u
   ```

2. **Sáº¯p xáº¿p theo Ä‘á»™ dÃ i giáº£m dáº§n** (trong má»—i batch):
   ```
   CÃ¢u 1: 25 tokens (dÃ i nháº¥t)
   CÃ¢u 2: 23 tokens
   ...
   CÃ¢u 64: 8 tokens (ngáº¯n nháº¥t)
   ```

3. **Padding** (thÃªm <pad> cho cÃ¢u ngáº¯n):
   ```
   CÃ¢u 1: [2, 523, 45, ..., 3]           (25 tokens)
   CÃ¢u 64: [2, 89, 12, 3, 0, 0, 0, ...]  (8 tokens + 17 padding)
   ```

**Output:**
- `train_loader`: 454 batches
- `val_loader`: 16 batches
- `test_loader`: 16 batches

**Táº¡i sao cáº§n lÃ m váº­y?**
- Sáº¯p xáº¿p giáº£m dáº§n â†’ dÃ¹ng `pack_padded_sequence` â†’ LSTM xá»­ lÃ½ nhanh hÆ¡n
- Padding â†’ táº¥t cáº£ cÃ¢u cÃ¹ng Ä‘á»™ dÃ i â†’ xá»­ lÃ½ song song trÃªn GPU

---

### ğŸ§  **BÆ¯á»šC 3: XÃ‚Y Dá»°NG MODEL**

#### Kiáº¿n trÃºc Encoder-Decoder:

```
INPUT (English)                    OUTPUT (French)
    â†“                                     â†“
["a", "man", "is", "walking"]    ["un", "homme", "marche"]
    â†“                                     â†“
[2, 523, 45, 1247, 3]           [2, 312, 891, 456, 3]
    â†“                                     â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚    ENCODER      â”‚                      â”‚
â”‚  (LSTM 2 layers)â”‚                      â”‚
â”‚   Hidden: 512   â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
         â”‚ Context Vector                â”‚
         â”‚ (hidden + cell state)         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                         â”‚   DECODER   â”‚
                         â”‚(LSTM 2 layers)â”‚
                         â”‚  Hidden: 512 â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Encoder:**
- Äá»c cÃ¢u tiáº¿ng Anh tá»« trÃ¡i â†’ pháº£i
- Má»—i tá»« â†’ embedding vector (256 chiá»u)
- LSTM xá»­ lÃ½ chuá»—i â†’ táº¡o context vector
- Context vector = tÃ³m táº¯t toÃ n bá»™ cÃ¢u tiáº¿ng Anh

**Decoder:**
- Nháº­n context vector tá»« Encoder
- Sinh tá»«ng tá»« tiáº¿ng PhÃ¡p tá»« trÃ¡i â†’ pháº£i
- Má»—i bÆ°á»›c sinh 1 tá»« dá»±a trÃªn:
  - Context vector
  - Tá»« Ä‘Ã£ sinh trÆ°á»›c Ä‘Ã³

**Output:** Model Ä‘Ã£ khá»Ÿi táº¡o, sáºµn sÃ ng training

---

### ğŸ‹ï¸ **BÆ¯á»šC 4: TRAINING (Há»ŒC)**

**Input:** 
- Model chÆ°a train (random weights)
- 454 batches dá»¯ liá»‡u training

**QuÃ¡ trÃ¬nh 1 epoch:**
```
Epoch 1:
  Batch 1/454: Loss = 8.523
  Batch 2/454: Loss = 8.234
  ...
  Batch 454/454: Loss = 6.123
  â†’ Train Loss = 7.123
  
  Validation:
  â†’ Val Loss = 5.234
  
  âœ… Val loss giáº£m â†’ LÆ°u model
```

**CÃ¡c ká»¹ thuáº­t quan trá»ng:**

1. **Teacher Forcing (50%)**:
   - 50% láº§n: Decoder nháº­n tá»« Ä‘Ãºng tá»« ground truth
   - 50% láº§n: Decoder nháº­n tá»« dá»± Ä‘oÃ¡n cá»§a chÃ­nh nÃ³
   - â†’ Model há»c nhanh hÆ¡n, á»•n Ä‘á»‹nh hÆ¡n

2. **Early Stopping (patience=3)**:
   ```
   Epoch 5: val_loss = 3.2 âœ… (best)
   Epoch 6: val_loss = 3.3 (tÄƒng láº§n 1)
   Epoch 7: val_loss = 3.4 (tÄƒng láº§n 2)
   Epoch 8: val_loss = 3.5 (tÄƒng láº§n 3)
   â†’ Dá»«ng training! TrÃ¡nh overfitting
   ```

3. **Learning Rate Scheduling**:
   ```
   Epoch 1-3: lr = 0.001
   Val loss khÃ´ng giáº£m sau 2 epochs
   â†’ Epoch 4: lr = 0.0005 (giáº£m 50%)
   ```

4. **Gradient Clipping**:
   - Giá»›i háº¡n gradient â‰¤ 1.0
   - TrÃ¡nh exploding gradients

**Output:**
- `best_model.pth`: Model cÃ³ val_loss tháº¥p nháº¥t
- `train_losses`, `val_losses`: Lá»‹ch sá»­ loss Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“

---

### ğŸ” **BÆ¯á»šC 5: Dá»ŠCH CÃ‚U Má»šI (INFERENCE)**

**Input:** CÃ¢u tiáº¿ng Anh má»›i
```
"The cat is sleeping on the bed."
```

**QuÃ¡ trÃ¬nh dá»‹ch (Greedy Decoding):**

```
Step 0: Tokenize + Encode
  â†’ [2, 12, 234, 67, 456, 89, 12, 890, 5, 3]

Step 1: Encoder xá»­ lÃ½
  â†’ Context vector = [0.23, -0.45, 0.67, ...]

Step 2: Decoder báº¯t Ä‘áº§u vá»›i <sos>
  Input: <sos> â†’ Output: "le" (xÃ¡c suáº¥t cao nháº¥t)

Step 3: Decoder nháº­n "le"
  Input: "le" â†’ Output: "chat"

Step 4: Decoder nháº­n "chat"
  Input: "chat" â†’ Output: "dort"

Step 5: Decoder nháº­n "dort"
  Input: "dort" â†’ Output: "sur"

...

Step N: Decoder sinh <eos>
  â†’ Dá»«ng láº¡i!
```

**Output:**
```
"le chat dort sur le lit ."
```

---

### ğŸ“Š **BÆ¯á»šC 6: ÄÃNH GIÃ (EVALUATION)**

#### BLEU Score:
Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¢u dá»‹ch vÃ  cÃ¢u tham chiáº¿u.

**VÃ­ dá»¥:**
```
Reference: "le chat dort sur le lit ."
Predicted: "le chat est sur le lit ."

BLEU-1 (1-gram): 85.7% (6/7 tá»« trÃ¹ng)
BLEU-2 (2-gram): 66.7% (4/6 cáº·p tá»« trÃ¹ng)
BLEU-4 (4-gram): 50.0%

â†’ BLEU Score: 67.5% (trung bÃ¬nh cÃ³ trá»ng sá»‘)
```

**Cháº¡y trÃªn toÃ n bá»™ test set:**
```python
for 1,000 cÃ¢u trong test:
    translated = model.translate(cÃ¢u_tiáº¿ng_Anh)
    bleu_score = compute_bleu(translated, cÃ¢u_tham_chiáº¿u)

â†’ Average BLEU = 28.5% (vÃ­ dá»¥)
```

**Benchmark:**
- BLEU < 20%: KÃ©m
- BLEU 20-30%: Trung bÃ¬nh (model cÆ¡ báº£n)
- BLEU 30-40%: KhÃ¡
- BLEU > 40%: Tá»‘t (cáº§n Attention, Transformer)

---

## ğŸ¯ CÃCH Äáº T Káº¾T QUáº¢ CAO NHáº¤T

### âœ… **Cáº¤P Äá»˜ CÆ  Báº¢N (7-8 Ä‘iá»ƒm)**

1. **HoÃ n thÃ nh Ä‘áº§y Ä‘á»§ Task 1-5**
2. **Hyperparameters máº·c Ä‘á»‹nh:**
   - Batch size: 64
   - Embedding: 256
   - Hidden: 512
   - Layers: 2
   - Dropout: 0.3
   - Epochs: 15

3. **BLEU score:** 18-25%

### â­ **Cáº¤P Äá»˜ Tá»T (8-9 Ä‘iá»ƒm)**

1. **Tá»‘i Æ°u hyperparameters:**
   ```python
   BATCH_SIZE = 128          # TÄƒng batch size
   EMBEDDING_DIM = 512       # TÄƒng embedding
   HIDDEN_SIZE = 1024        # TÄƒng hidden size
   NUM_LAYERS = 3            # ThÃªm layer
   DROPOUT = 0.5             # TÄƒng dropout
   TEACHER_FORCING_RATIO = 0.7  # TÄƒng teacher forcing
   ```

2. **Ká»¹ thuáº­t bá»• sung:**
   - Learning rate decay
   - Gradient clipping = 1.0
   - Weight initialization (Xavier/He)

3. **BLEU score:** 25-30%

### ğŸ† **Cáº¤P Äá»˜ XUáº¤T Sáº®C (9-10 Ä‘iá»ƒm)**

1. **ThÃªm Attention Mechanism:**
   ```python
   class Attention(nn.Module):
       # Decoder chÃº Ã½ Ä‘áº¿n tá»«ng tá»« cá»§a Encoder
       # Thay vÃ¬ chá»‰ dÃ¹ng context vector cá»‘ Ä‘á»‹nh
   ```

2. **Data Augmentation:**
   - Back-translation (dá»‹ch ngÆ°á»£c láº¡i)
   - Paraphrase (diá»…n Ä‘áº¡t láº¡i)

3. **Ensemble Models:**
   - Train 3-5 models khÃ¡c nhau
   - Average predictions

4. **Beam Search (thay Greedy):**
   - Giá»¯ top-5 candidates má»—i bÆ°á»›c
   - Chá»n sequence cÃ³ xÃ¡c suáº¥t cao nháº¥t

5. **BLEU score:** 30-35%+

6. **BÃ¡o cÃ¡o cháº¥t lÆ°á»£ng:**
   - PhÃ¢n tÃ­ch sÃ¢u 10-20 vÃ­ dá»¥ lá»—i
   - So sÃ¡nh vá»›i baseline
   - Äá» xuáº¥t cáº£i tiáº¿n cá»¥ thá»ƒ
   - Váº½ sÆ¡ Ä‘á»“ kiáº¿n trÃºc Ä‘áº¹p
   - Biá»ƒu Ä‘á»“ loss, attention weights

---

## ğŸ“‹ CHECKLIST HOÃ€N THÃ€NH

### Task 1: MÃ´i trÆ°á»ng (3.0Ä‘) âœ…
- [x] File `config.py` vá»›i táº¥t cáº£ hyperparameters
- [x] File `utils.py` vá»›i Vocabulary class
- [x] File `requirements.txt`
- [x] File `README.md`

### Task 2: Data Processing (2.0Ä‘) âœ…
- [x] Tokenization Ä‘Æ¡n giáº£n (lowercase + regex)
- [x] Build vocabularies (10,000 tokens/ngÃ´n ngá»¯)
- [x] Sáº¯p xáº¿p batch theo Ä‘á»™ dÃ i giáº£m dáº§n
- [x] Padding sequences
- [x] DataLoader vá»›i batch size 32-128

### Task 3: Model (3.0Ä‘) â³
- [ ] Class `Encoder` (LSTM 2 layers)
- [ ] Class `Decoder` (LSTM 2 layers)
- [ ] Class `Seq2Seq` (káº¿t há»£p Encoder-Decoder)
- [ ] Context vector tá»« hidden + cell state
- [ ] Teacher forcing trong training

### Task 4: Training (1.5Ä‘) â³
- [ ] HÃ m `train_epoch()`
- [ ] HÃ m `evaluate()` (validation)
- [ ] Early stopping (patience=3)
- [ ] Learning rate scheduler
- [ ] LÆ°u checkpoint model tá»‘t nháº¥t
- [ ] Váº½ biá»ƒu Ä‘á»“ train/val loss

### Task 5: Evaluation (2.0Ä‘) â³
- [ ] HÃ m `translate()` vá»›i greedy decoding
- [ ] TÃ­nh BLEU score trÃªn test set
- [ ] Test trÃªn 5-10 cÃ¢u máº«u
- [ ] So sÃ¡nh vá»›i ground truth

### Task 6-8: BÃ¡o cÃ¡o (2.0Ä‘) â³
- [ ] SÆ¡ Ä‘á»“ kiáº¿n trÃºc model
- [ ] Biá»ƒu Ä‘á»“ train/val loss
- [ ] Báº£ng káº¿t quáº£ BLEU score
- [ ] PhÃ¢n tÃ­ch 5 vÃ­ dá»¥ dá»‹ch sai
- [ ] Äá» xuáº¥t cáº£i tiáº¿n
- [ ] Export PDF

---

## ğŸš€ TIMELINE Äá»€ XUáº¤T

**Tá»•ng thá»i gian: 5-7 ngÃ y**

| NgÃ y | CÃ´ng viá»‡c | Thá»i gian |
|------|-----------|-----------|
| NgÃ y 1 | Task 1-2: Setup + Data | 4-6 giá» |
| NgÃ y 2 | Task 3: Implement Model | 4-6 giá» |
| NgÃ y 3 | Task 4: Training (cháº¡y overnight) | 8-12 giá» |
| NgÃ y 4 | Task 5: Evaluation + Debug | 3-4 giá» |
| NgÃ y 5 | Task 6-8: BÃ¡o cÃ¡o + PhÃ¢n tÃ­ch | 4-6 giá» |
| NgÃ y 6 | Review + HoÃ n thiá»‡n | 2-3 giá» |
| NgÃ y 7 | Buffer (dá»± phÃ²ng) | - |

**Deadline:** 14/12/2025 (23:59)  
**CÃ²n láº¡i:** 7 ngÃ y

---

## ğŸ”¥ LÆ¯U Ã QUAN TRá»ŒNG

### âš ï¸ **Äiá»ƒm dá»… máº¥t Ä‘iá»ƒm:**

1. **KhÃ´ng sáº¯p xáº¿p batch** â†’ pack_padded_sequence lá»—i
2. **QuÃªn padding** â†’ tensor shape khÃ´ng Ä‘á»u
3. **Teacher forcing = 1.0** â†’ model khÃ´ng há»c Ä‘Æ°á»£c tá»± sinh
4. **KhÃ´ng early stopping** â†’ overfitting
5. **BLEU < 15%** â†’ model há»c kÃ©m
6. **BÃ¡o cÃ¡o thiáº¿u sÆ¡ Ä‘á»“/biá»ƒu Ä‘á»“** â†’ máº¥t Ä‘iá»ƒm trÃ¬nh bÃ y

### âœ… **CÃ¡ch Ä‘áº£m báº£o Ä‘iá»ƒm cao:**

1. **Code cháº¡y Ä‘Æ°á»£c tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i** (quan trá»ng nháº¥t!)
2. **Comment code rÃµ rÃ ng** (giáº£i thÃ­ch tá»«ng bÆ°á»›c)
3. **BLEU â‰¥ 20%** (cháº¥p nháº­n Ä‘Æ°á»£c)
4. **BÃ¡o cÃ¡o Ä‘áº§y Ä‘á»§:**
   - Giá»›i thiá»‡u bÃ i toÃ¡n
   - SÆ¡ Ä‘á»“ kiáº¿n trÃºc
   - Káº¿t quáº£ (báº£ng, biá»ƒu Ä‘á»“)
   - PhÃ¢n tÃ­ch lá»—i (â‰¥5 vÃ­ dá»¥)
   - Káº¿t luáº­n + Ä‘á» xuáº¥t
5. **Ná»™p Ä‘á»§ file:**
   - PDF bÃ¡o cÃ¡o
   - Notebook (.ipynb)
   - Checkpoint (.pth)

---

## ğŸ“ Cáº¤U TRÃšC FOLDER CUá»I CÃ™NG

```
NLP_DO_AN/
â”œâ”€â”€ data/                          # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ train.en, train.fr
â”‚   â”œâ”€â”€ val.en, val.fr
â”‚   â””â”€â”€ test.en, test.fr
â”‚
â”œâ”€â”€ src/                           # Source code
â”‚   â”œâ”€â”€ config.py        âœ…        # ÄÃ£ cÃ³
â”‚   â”œâ”€â”€ utils.py         âœ…        # ÄÃ£ cÃ³
â”‚   â”œâ”€â”€ data_loader.py   âœ…        # ÄÃ£ cÃ³
â”‚   â”œâ”€â”€ model.py         â³        # Cáº§n lÃ m
â”‚   â”œâ”€â”€ train.py         â³        # Cáº§n lÃ m
â”‚   â””â”€â”€ evaluate.py      â³        # Cáº§n lÃ m
â”‚
â”œâ”€â”€ check_point/                   # Model weights
â”‚   â”œâ”€â”€ best_model.pth   â³
â”‚   â”œâ”€â”€ src_vocab.pth    âœ…
â”‚   â””â”€â”€ tgt_vocab.pth    âœ…
â”‚
â”œâ”€â”€ report/                        # BÃ¡o cÃ¡o
â”‚   â”œâ”€â”€ PROGRESS_REPORT.md  âœ…
â”‚   â”œâ”€â”€ figures/          â³       # SÆ¡ Ä‘á»“, biá»ƒu Ä‘á»“
â”‚   â””â”€â”€ final_report.pdf  â³
â”‚
â”œâ”€â”€ notebooks/                     # Notebook
â”‚   â””â”€â”€ NLP_Do_An.ipynb   â³
â”‚
â”œâ”€â”€ README.md             âœ…
â”œâ”€â”€ requirements.txt      âœ…
â”œâ”€â”€ COLAB_GUIDE.md        âœ…
â””â”€â”€ PROJECT_OVERVIEW.md   âœ… (file nÃ y)
```

---

## ğŸ“ TÃ€I LIá»†U THAM KHáº¢O

1. **Paper gá»‘c - Sequence to Sequence:**
   - Sutskever et al. (2014) - "Sequence to Sequence Learning with Neural Networks"

2. **Tutorial hay:**
   - PyTorch Seq2Seq Tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html

3. **BLEU Score:**
   - Papineni et al. (2002) - "BLEU: a Method for Automatic Evaluation of Machine Translation"

---

**Good luck! ğŸš€**

*Táº¡o bá»Ÿi: GitHub Copilot*  
*NgÃ y: 07/12/2025*
