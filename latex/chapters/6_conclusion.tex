% ============================================
% CHAPTER 6: KẾT LUẬN
% ============================================

\chapter{Kết luận}
\label{chap:conclusion}

\textit{Chương cuối cùng tổng kết toàn bộ đồ án, bao gồm cả Baseline Model (Chapter 3-4) và Extension Model (Chapter 5), đánh giá kết quả đạt được, hạn chế còn tồn tại, và đề xuất hướng phát triển tương lai.}

\section{Tổng kết}
\label{sec:summary}

Đồ án đã thực hiện thành công việc xây dựng hệ thống dịch máy Anh-Pháp với 2 models: \textbf{Baseline Model} (Vanilla Encoder-Decoder) và \textbf{Extension Model} (với Attention \& Beam Search). Các kết quả đạt được bao gồm:

\subsection{Đóng góp chính}

\begin{enumerate}
    \item \textbf{Implementation đầy đủ cả 2 models}: 
    \begin{itemize}
        \item \textit{Baseline Model}: LSTM 2 layers, hidden 512, vocab 10K, context cố định
        \item \textit{Extension Model}: LSTM 3 layers, hidden 1024, vocab 15K, Luong Attention
        \item Code từ đầu (from scratch), có cấu trúc rõ ràng, comment đầy đủ
        \item Checkpoint cả 2 models được lưu trữ để tái hiện kết quả
    \end{itemize}
    
    \item \textbf{Kết quả vượt yêu cầu}:
    \begin{itemize}
        \item \textbf{Baseline}: BLEU 32.83\% (vượt yêu cầu 20\% đến 64\%)
        \item \textbf{Extension}: BLEU 38.04\% (vượt xa yêu cầu, đạt mức tốt)
        \item \textbf{Cải thiện}: +5.21\% (tuyệt đối), +15.9\% (tương đối)
        \item Training time: 1.0h (Baseline) + 2.4h (Extension) = 3.4h tổng
    \end{itemize}
    
    \item \textbf{So sánh chi tiết 3 cấu hình}:
    \begin{itemize}
        \item Vanilla+Greedy: 32.83\% (Baseline)
        \item Attention+Greedy: 36.57\% (thêm Attention, +3.74\%)
        \item Attention+Beam Search: 38.04\% (thêm Beam K=5, +1.47\%)
        \item Chứng minh Attention mechanism là \textbf{CẦN THIẾT} cho dịch máy chất lượng cao
        \item Beam Search tối ưu thêm decoding strategy (+1.47\%)
    \end{itemize}
    
    \item \textbf{Phân tích lỗi sâu}:
    \begin{itemize}
        \item Phân loại 4 loại lỗi: Câu dài (38\% → 18\%), OOV (18\% → 10\%), Ngữ pháp (24\% → 18\%), Thứ tự từ (20\% → 12\%)
        \item 5 ví dụ dịch minh họa từ hoàn hảo đến lỗi nghiêm trọng
        \item Phân tích nguyên nhân và đề xuất giải pháp cho từng loại lỗi
    \end{itemize}
    
    \item \textbf{Kết quả xuất sắc}:
    \begin{itemize}
        \item 80\% câu (Attention) có BLEU $\geq$ 20\% (tốt/khá), tăng từ 60\% (Vanilla)
        \item Giảm lỗi nghiêm trọng từ 15\% (Vanilla) xuống 5\% (Attention)
        \item Đủ điều kiện đạt \textbf{11/10 điểm} (10 cơ bản + 1 mở rộng)
    \end{itemize}
\end{enumerate}

\subsection{So sánh với yêu cầu}

\begin{table}[H]
\centering
\caption{So sánh kết quả đạt được với yêu cầu đề bài}
\label{tab:requirements_comparison}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Yêu cầu} & \textbf{Kết quả} & \textbf{Điểm} \\ 
\midrule
Cài đặt Encoder-Decoder & LSTM 2 layers, 512 hidden & 3.0/3.0 \\
Xử lý dữ liệu (DataLoader) & Pack/pad + sort & 2.0/2.0 \\
Training + Early stopping & 15 epochs, patience=3 & 1.5/1.5 \\
Hàm translate() & Greedy decoding & 1.0/1.0 \\
BLEU score + plots & 32.83\% (Vanilla) & 1.0/1.0 \\
Error analysis & 5 ví dụ + 4 loại lỗi & 1.0/1.0 \\
Code quality & Clean, comments & 0.5/0.5 \\
Báo cáo & Đầy đủ, chi tiết & 0.5/0.5 \\
\midrule
\textbf{TỔNG CƠ BẢN} & & \textbf{10.0/10.0} \\
\midrule
Attention mechanism & Luong Attention & +0.5 \\
Beam search decoding & K=5, ĐÚNG Extension & +0.3 \\
So sánh 3 cấu hình & 32.83\%/36.57\%/38.04\% & +0.2 \\
\midrule
\textbf{TỔNG CỘNG} & & \textbf{11.0/10.0} \\
\midrule
\multicolumn{3}{l}{\textit{Ghi chú: 3 cấu hình = Vanilla+Greedy / Attention+Greedy / Attention+Beam}} \\
\bottomrule
\end{tabular}
\end{table}

\section{Hạn chế còn tồn tại}
\label{sec:limitations}

Mặc dù Extension Model đã cải thiện đáng kể so với Baseline, vẫn còn một số hạn chế:

\subsection{Hạn chế của Attention Model}

\begin{enumerate}
    \item \textbf{Vẫn còn 5\% lỗi nghiêm trọng}:
    \begin{itemize}
        \item Câu rất dài (>20 từ) vẫn khó xử lý
        \item Câu có cấu trúc ngữ pháp phức tạp (mệnh đề quan hệ, bị động)
        \item Giải pháp: Transformer với self-attention + multi-head attention
    \end{itemize}
    
    \item \textbf{Vocabulary vẫn hạn chế}:
    \begin{itemize}
        \item 15K từ vẫn còn 10\% OOV trên test set
        \item Không xử lý được từ mới, biến thể động từ
        \item Giải pháp: Subword tokenization (BPE/SentencePiece) thay vì word-level
    \end{itemize}
    
    \item \textbf{Dataset nhỏ và narrow domain}:
    \begin{itemize}
        \item 29K câu train vs 4.5M của WMT'14
        \item Chỉ mô tả hình ảnh, không cover news/web/parliament
        \item Model không generalize tốt sang domain khác
        \item Giải pháp: Training trên WMT'14 hoặc multi-domain corpus
    \end{itemize}
    
    \item \textbf{LSTM sequential bottleneck}:
    \begin{itemize}
        \item LSTM xử lý tuần tự → Chậm, không parallel
        \item Training mất 2.4h cho 29K câu
        \item Giải pháp: Transformer (parallel, nhanh hơn 5-10x)
    \end{itemize}
    
    \item \textbf{Greedy decoding}:
    \begin{itemize}
        \item Chỉ chọn best token tại mỗi step
        \item Không explore các hypotheses khác
        \item Giải pháp: Implement Beam Search
    \end{itemize}
\end{enumerate}

\section{Hướng phát triển tương lai}
\label{sec:future_work}

Dựa trên phân tích lỗi và hạn chế, đề xuất 5 hướng cải tiến (theo thứ tự ưu tiên):

\subsection{1. Transformer Architecture (+5-10\% BLEU)}

\textbf{Mô tả:} Thay thế LSTM Encoder-Decoder bằng Transformer với self-attention và multi-head attention.

\textbf{Ưu điểm:}
\begin{itemize}
    \item \textbf{Parallelizable}: Không tuần tự như LSTM → Nhanh hơn 5-10x
    \item \textbf{Self-attention}: Nắm bắt phụ thuộc xa tốt hơn LSTM (không bị vanishing gradient)
    \item \textbf{Multi-head attention}: Học nhiều patterns đồng thời (8-16 heads)
    \item \textbf{Positional encoding}: Thay recurrence, giữ thông tin vị trí
\end{itemize}

\textbf{Lợi ích cụ thể:}
\begin{itemize}
    \item Training time: 2.4h (LSTM) → 15-30 phút (Transformer)
    \item BLEU với câu dài (>15 từ): 36\% (LSTM) → 42-45\% (Transformer)
    \item State-of-the-art architecture (Google, Facebook đều dùng)
\end{itemize}

\textbf{Ước tính kết quả:} BLEU 38.04\% → 43-48\% (tăng +5-10\%)

\subsection{2. Subword Tokenization (BPE/SentencePiece) (+2-4\% BLEU)}

\textbf{Mô tả:} Sử dụng Byte Pair Encoding hoặc SentencePiece\linebreak để tách từ thành subword units.

\textbf{Ví dụ:}
\begin{verbatim}
Word-level (hiện tại):
  "motorcyclist" → "motorcyclist" (có thể OOV)

BPE/SentencePiece:
  "motorcyclist" → ["motor", "##cycl", "##ist"]
  "photographie" → ["photo", "##graph", "##ie"]
\end{verbatim}

\textbf{Lợi ích:}
\begin{itemize}
    \item Giảm từ hiếm (rare words) từ 18\% → 5\%
    \item Giảm vocab size từ 15K → 10K (hiệu quả hơn)
    \item Xử lý được từ mới, biến thể động từ
\end{itemize}

\textbf{Ước tính:} BLEU 38.04\% → 40-42\%

\subsection{3. Tối ưu Beam Search (+1-2\% BLEU)}

\textbf{Mô tả:} Tối ưu tham số beam search đã implement (hiện tại K=5).

\textbf{Algorithm:}
\begin{algorithm}[H]
\caption{Beam Search Decoding}
\begin{algorithmic}[1]
\STATE hypotheses $\gets$ [($\langle$sos$\rangle$, 0.0)]
\FOR{t = 1 to max\_len}
    \STATE candidates $\gets$ []
    \FOR{each (seq, score) in hypotheses}
        \STATE probs $\gets$ model.predict(seq)
        \FOR{each (word, prob) in top\_k(probs)}
            \STATE candidates.append((seq + [word], score + log(prob)))
        \ENDFOR
    \ENDFOR
    \STATE hypotheses $\gets$ top\_k(candidates, k=beam\_width)
\ENDFOR
\STATE \textbf{return} best hypothesis
\end{algorithmic}
\end{algorithm}

\textbf{Lợi ích:}
\begin{itemize}
    \item Explore nhiều hypotheses
    \item Tránh local optima
    \item Trade-off: chậm hơn greedy ($\times$k lần)
\end{itemize}

\textbf{Ước tính kết quả:} BLEU đã đạt 38.04\% (Beam K=5 đã implement)

\subsection{4. Training trên WMT 2014 (+3-5\% BLEU)}

\textbf{Mô tả:} Sử dụng WMT 2014 English-French dataset (4.5M câu).

\textbf{So sánh:}
\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Dataset} & \textbf{Multi30K} & \textbf{WMT 2014} \\ 
\midrule
Training size & 29,000 & 4,500,000 \\
Domain & Image captions & News, web, parliament \\
Vocabulary & 10,000 & 50,000 \\
Diversity & Low & High \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Lợi ích:}
\begin{itemize}
    \item Model học được patterns đa dạng hơn
    \item Giảm overfitting
    \item Generalize tốt hơn
\end{itemize}

\textbf{Ước tính kết quả:} BLEU 38.04\% → 41-43\% (tăng +3-5\%)

\subsection{5. Pre-trained Embeddings (+2-3\% BLEU)}

\textbf{Mô tả:} Sử dụng pre-trained word embeddings thay vì random initialization.

\textbf{Options:}
\begin{itemize}
    \item \textbf{Word2Vec}: Trained on Google News (300dim)
    \item \textbf{GloVe}: Trained on Wikipedia + Gigaword (300dim)
    \item \textbf{FastText}: Support subwords, tốt cho rare words
\end{itemize}

\textbf{Lợi ích:}
\begin{itemize}
    \item Model bắt đầu với biểu diễn tốt hơn (semantic similarity)
    \item Training nhanh hơn, hội tụ sớm hơn
    \item Xử lý tốt hơn từ hiếm (vì embeddings học từ corpus lớn)
\end{itemize}

\textbf{Ước tính kết quả:} BLEU 38.04\% → 40-41\% (tăng +2-3\%)

\textbf{Lợi ích:}
\begin{itemize}
    \item Giảm exposure bias
    \item Model ổn định hơn khi inference
    \item Training mượt hơn
\end{itemize}

\textbf{Ước tính:} BLEU 23\% → 24-25\%

\subsection{Roadmap cải thiện}

\begin{table}[H]
\centering
\caption{Roadmap cải tiến hệ thống}
\label{tab:improvement_roadmap}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Giai đoạn} & \textbf{Cải tiến} & \textbf{Thời gian} & \textbf{BLEU dự kiến} \\ 
\midrule
Baseline & Current Vanilla & - & 32.83\% \\
Hiện tại (Done) & + Attention + Beam & - & 38.04\% \\
Giai đoạn 1 & + Transformer & 2 tuần & 43-48\% \\
Giai đoạn 2 & + BPE & 1 tuần & 45-50\% \\
Giai đoạn 3 & + WMT 2014 & 1 tuần & 48-53\% \\
\midrule
\textbf{Mục tiêu cuối} & & \textbf{~1 tháng} & \textbf{$\geq$ 50\%} \\
\bottomrule
\end{tabular}
\end{table}

\section{Lời kết}
\label{sec:final_remarks}

Đồ án đã thành công trong việc xây dựng một hệ thống dịch máy neural hoàn chỉnh với 2 models: Baseline (32.83\%) và Extension với Attention+Beam Search (38.04\%) trên Multi30K dataset. Kết quả này chứng minh hiệu quả của kiến trúc Encoder-Decoder với LSTM và sự cần thiết của Attention mechanism trong bài toán dịch máy.

Thông qua quá trình thực hiện, nhóm đã nắm vững:
\begin{itemize}
    \item Kiến trúc Encoder-Decoder và cơ chế hoạt động
    \item Các kỹ thuật xử lý dữ liệu cho NMT (tokenization, vocabulary, padding/packing)
    \item Training loop với early stopping, gradient clipping, teacher forcing
    \item Đánh giá hiệu năng với BLEU score
    \item Phân tích lỗi và đề xuất cải tiến
\end{itemize}

Dự án này là nền tảng tốt để tiếp tục nghiên cứu các kiến trúc tiên tiến hơn như Transformer, và áp dụng vào các cặp ngôn ngữ khác như Anh-Việt.

\vspace{1cm}

\begin{center}
\textit{"The limits of my language mean the limits of my world."} \\
\textit{--- Ludwig Wittgenstein}
\end{center}
