% ============================================
% PHỤ LỤC B: MÃ NGUỒN (RÚT GỌN)
% ============================================

\chapter{Mã nguồn chính}
\label{app:source_code}

\textit{Lưu ý: Code dưới đây đã được rút gọn để giảm thời gian compile. Code đầy đủ có trong Jupyter notebook đính kèm.}

\section{Xây dựng Vocabulary}

\begin{lstlisting}[language=Python, caption={Vocabulary class - vocab.py}]
import torch
from collections import Counter
from typing import List

class Vocabulary:
    """Build and manage vocabulary for NMT."""
    
    def __init__(self, max_size=10000, min_freq=2):
        self.max_size = max_size
        self.min_freq = min_freq
        
        # Special tokens
        self.pad_token = '<pad>'
        self.unk_token = '<unk>'
        self.sos_token = '<sos>'
        self.eos_token = '<eos>'
        
        # Initialize mappings
        self.word2idx = {}
        self.idx2word = {}
        self.word_freq = Counter()
        
        # Add special tokens
        self._add_special_tokens()
    
    def _add_special_tokens(self):
        """Add special tokens to vocabulary."""
        special_tokens = [
            self.pad_token, 
            self.unk_token, 
            self.sos_token, 
            self.eos_token
        ]
        for token in special_tokens:
            idx = len(self.word2idx)
            self.word2idx[token] = idx
            self.idx2word[idx] = token
    
    def build_vocab(self, sentences: List[List[str]]):
        """Build vocabulary from sentences."""
        # Count word frequencies
        for sentence in sentences:
            self.word_freq.update(sentence)
        
        # Get most common words (excluding special tokens)
        most_common = self.word_freq.most_common(self.max_size - 4)
        
        # Add words meeting min_freq threshold
        for word, freq in most_common:
            if freq >= self.min_freq:
                if word not in self.word2idx:
                    idx = len(self.word2idx)
                    self.word2idx[word] = idx
                    self.idx2word[idx] = word
    
    def encode(self, sentence: List[str]) -> List[int]:
        """Convert words to indices."""
        return [
            self.word2idx.get(word, self.word2idx[self.unk_token]) 
            for word in sentence
        ]
    
    def decode(self, indices: List[int]) -> List[str]:
        """Convert indices to words."""
        return [self.idx2word.get(idx, self.unk_token) for idx in indices]
    
    def __len__(self):
        return len(self.word2idx)
    
    @property
    def pad_idx(self):
        return self.word2idx[self.pad_token]
    
    @property
    def unk_idx(self):
        return self.word2idx[self.unk_token]
    
    @property
    def sos_idx(self):
        return self.word2idx[self.sos_token]
    
    @property
    def eos_idx(self):
        return self.word2idx[self.eos_token]
\end{lstlisting}

\section{Encoder Module}

\begin{lstlisting}[language=Python, caption={Encoder class - encoder.py}]
import torch
import torch.nn as nn

class Encoder(nn.Module):
    """LSTM-based Encoder for Seq2Seq model."""
    
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        """
        Args:
            input_dim: Vocabulary size (source language)
            emb_dim: Embedding dimension
            hid_dim: Hidden state dimension
            n_layers: Number of LSTM layers
            dropout: Dropout probability
        """
        super(Encoder, self).__init__()
        
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        # Embedding layer
        self.embedding = nn.Embedding(input_dim, emb_dim)
        
        # LSTM layer (bidirectional=False)
        self.rnn = nn.LSTM(
            emb_dim, 
            hid_dim, 
            n_layers, 
            dropout=dropout if n_layers > 1 else 0,
            batch_first=True
        )
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, src_len):
        """
        Args:
            src: Source sentence [batch_size, src_len]
            src_len: Actual lengths before padding
        
        Returns:
            outputs: All hidden states [batch_size, src_len, hid_dim]
            hidden: Final hidden state [n_layers, batch_size, hid_dim]
            cell: Final cell state [n_layers, batch_size, hid_dim]
        """
        # Embedding: [batch_size, src_len] -> [batch_size, src_len, emb_dim]
        embedded = self.dropout(self.embedding(src))
        
        # Pack padded sequence
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, 
            src_len.cpu(), 
            batch_first=True, 
            enforce_sorted=False
        )
        
        # LSTM forward pass
        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)
        
        # Unpack sequence
        outputs, _ = nn.utils.rnn.pad_packed_sequence(
            packed_outputs, 
            batch_first=True
        )
        
        # outputs: [batch_size, src_len, hid_dim]
        # hidden: [n_layers, batch_size, hid_dim]
        # cell: [n_layers, batch_size, hid_dim]
        return outputs, hidden, cell
\end{lstlisting}

\section{Decoder Module}

\begin{lstlisting}[language=Python, caption={Decoder class - decoder.py}]
import torch
import torch.nn as nn

class Decoder(nn.Module):
    """LSTM-based Decoder for Seq2Seq model."""
    
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        """
        Args:
            output_dim: Vocabulary size (target language)
            emb_dim: Embedding dimension
            hid_dim: Hidden state dimension
            n_layers: Number of LSTM layers
            dropout: Dropout probability
        """
        super(Decoder, self).__init__()
        
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        # Embedding layer
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        # LSTM layer
        self.rnn = nn.LSTM(
            emb_dim, 
            hid_dim, 
            n_layers, 
            dropout=dropout if n_layers > 1 else 0,
            batch_first=True
        )
        
        # Fully connected output layer
        self.fc_out = nn.Linear(hid_dim, output_dim)
        
        # Dropout layer
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input, hidden, cell):
        """
        Args:
            input: Current target token [batch_size, 1]
            hidden: Previous hidden state [n_layers, batch_size, hid_dim]
            cell: Previous cell state [n_layers, batch_size, hid_dim]
        
        Returns:
            prediction: Output logits [batch_size, output_dim]
            hidden: Updated hidden state
            cell: Updated cell state
        """
        # input: [batch_size, 1]
        
        # Embedding: [batch_size, 1] -> [batch_size, 1, emb_dim]
        embedded = self.dropout(self.embedding(input))
        
        # LSTM forward pass
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        # output: [batch_size, 1, hid_dim]
        
        # Prediction: [batch_size, 1, hid_dim] -> [batch_size, output_dim]
        prediction = self.fc_out(output.squeeze(1))
        
        return prediction, hidden, cell
\end{lstlisting}

\section{Seq2Seq Model}

\begin{lstlisting}[language=Python, caption={Seq2Seq class - seq2seq.py}]
import torch
import torch.nn as nn
import random

class Seq2Seq(nn.Module):
    """Complete Seq2Seq model."""
    
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        # Check compatibility
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions of encoder and decoder must match"
        assert encoder.n_layers == decoder.n_layers, \
            "Number of layers must match"
    
    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        """
        Args:
            src: Source sentence [batch_size, src_len]
            src_len: Source lengths (before padding)
            trg: Target sentence [batch_size, trg_len]
            teacher_forcing_ratio: Probability of using teacher forcing
        
        Returns:
            outputs: Model predictions [batch_size, trg_len, output_dim]
        """
        batch_size = trg.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.output_dim
        
        # Tensor to store decoder outputs
        outputs = torch.zeros(
            batch_size, trg_len, trg_vocab_size
        ).to(self.device)
        
        # Encoder forward pass
        enc_outputs, hidden, cell = self.encoder(src, src_len)
        
        # First input: <sos> token
        input = trg[:, 0].unsqueeze(1)  # [batch_size, 1]
        
        # Decode step by step
        for t in range(1, trg_len):
            # Decoder forward pass
            output, hidden, cell = self.decoder(input, hidden, cell)
            # output: [batch_size, output_dim]
            
            # Store prediction
            outputs[:, t, :] = output
            
            # Decide next input (teacher forcing or predicted token)
            teacher_force = random.random() < teacher_forcing_ratio
            
            # Get predicted token
            top1 = output.argmax(1).unsqueeze(1)  # [batch_size, 1]
            
            # Choose next input
            input = trg[:, t].unsqueeze(1) if teacher_force else top1
        
        return outputs
\end{lstlisting}

\section{Training Functions}

\begin{lstlisting}[language=Python, caption={Training loop - train.py}]
import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils import clip_grad_norm_
import time
import math

def train_epoch(model, iterator, optimizer, criterion, clip, device):
    """Train for one epoch."""
    model.train()
    
    epoch_loss = 0
    
    for batch in iterator:
        src, src_len = batch['src'], batch['src_len']
        trg = batch['trg']
        
        src = src.to(device)
        trg = trg.to(device)
        src_len = src_len.to(device)
        
        # Zero gradients
        optimizer.zero_grad()
        
        # Forward pass
        output = model(src, src_len, trg)
        # output: [batch_size, trg_len, output_dim]
        
        # Reshape for loss calculation
        output_dim = output.shape[-1]
        output = output[:, 1:, :].contiguous().view(-1, output_dim)
        trg = trg[:, 1:].contiguous().view(-1)
        
        # Calculate loss (ignore <pad> token)
        loss = criterion(output, trg)
        
        # Backward pass
        loss.backward()
        
        # Gradient clipping
        clip_grad_norm_(model.parameters(), clip)
        
        # Update parameters
        optimizer.step()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion, device):
    """Evaluate model."""
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
        for batch in iterator:
            src, src_len = batch['src'], batch['src_len']
            trg = batch['trg']
            
            src = src.to(device)
            trg = trg.to(device)
            src_len = src_len.to(device)
            
            # Forward pass (no teacher forcing)
            output = model(src, src_len, trg, teacher_forcing_ratio=0)
            
            # Reshape
            output_dim = output.shape[-1]
            output = output[:, 1:, :].contiguous().view(-1, output_dim)
            trg = trg[:, 1:].contiguous().view(-1)
            
            # Calculate loss
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)

def train_model(model, train_iterator, val_iterator, 
                optimizer, criterion, n_epochs, clip, device,
                patience=3, checkpoint_path='best_model.pth'):
    """Full training loop with early stopping."""
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(n_epochs):
        start_time = time.time()
        
        # Train
        train_loss = train_epoch(
            model, train_iterator, optimizer, criterion, clip, device
        )
        
        # Evaluate
        val_loss = evaluate(model, val_iterator, criterion, device)
        
        end_time = time.time()
        epoch_mins, epoch_secs = divmod(end_time - start_time, 60)
        
        # Check improvement
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            # Save checkpoint
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'val_loss': val_loss,
            }, checkpoint_path)
            print(f'| Epoch: {epoch+1:02} | Saved checkpoint')
        else:
            patience_counter += 1
        
        # Print stats
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs:.0f}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\tVal Loss: {val_loss:.3f} | Val PPL: {math.exp(val_loss):7.3f}')
        
        # Early stopping
        if patience_counter >= patience:
            print(f'\nEarly stopping at epoch {epoch+1}')
            break
    
    return best_val_loss
\end{lstlisting}

\section{Inference Functions}

\begin{lstlisting}[language=Python, caption={Inference - translate.py}]
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

def translate_sentence(sentence, src_vocab, trg_vocab, model, device, max_len=50):
    """Translate a single sentence."""
    model.eval()
    
    # Tokenize
    tokens = sentence.lower().split()
    
    # Convert to indices
    src_indices = [src_vocab.sos_idx] + src_vocab.encode(tokens) + [src_vocab.eos_idx]
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    src_len = torch.LongTensor([len(src_indices)]).to(device)
    
    with torch.no_grad():
        # Encode
        enc_outputs, hidden, cell = model.encoder(src_tensor, src_len)
        
        # Initialize decoder input
        trg_indices = [trg_vocab.sos_idx]
        
        # Decode step by step
        for _ in range(max_len):
            trg_tensor = torch.LongTensor([trg_indices[-1]]).unsqueeze(0).to(device)
            
            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)
            pred_token = output.argmax(1).item()
            
            trg_indices.append(pred_token)
            
            # Stop at <eos>
            if pred_token == trg_vocab.eos_idx:
                break
        
    # Convert indices to words
    trg_tokens = trg_vocab.decode(trg_indices[1:-1])  # Remove <sos> and <eos>
    
    return ' '.join(trg_tokens)

def calculate_bleu(candidate, reference):
    """Calculate BLEU score for a single sentence."""
    candidate_tokens = candidate.split()
    reference_tokens = reference.split()
    
    # Use smoothing for short sentences
    smoothie = SmoothingFunction().method4
    
    score = sentence_bleu(
        [reference_tokens], 
        candidate_tokens, 
        smoothing_function=smoothie
    )
    
    return score * 100  # Convert to percentage

def calculate_bleu_on_test_set(test_data, src_vocab, trg_vocab, model, device):
    """Calculate BLEU on entire test set."""
    model.eval()
    
    total_bleu = 0
    bleu_scores = []
    
    for src_sentence, ref_sentence in test_data:
        # Translate
        pred_sentence = translate_sentence(
            src_sentence, src_vocab, trg_vocab, model, device
        )
        
        # Calculate BLEU
        bleu = calculate_bleu(pred_sentence, ref_sentence)
        bleu_scores.append(bleu)
        total_bleu += bleu
    
    avg_bleu = total_bleu / len(test_data)
    
    return avg_bleu, bleu_scores
\end{lstlisting}

\section{Data Preprocessing}

\begin{lstlisting}[language=Python, caption={Data preprocessing - data.py}]
import re
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

def tokenize(text):
    """Simple tokenization using regex."""
    text = text.lower()
    text = re.sub(r"([.!?])", r" \1", text)
    text = re.sub(r"[^a-zA-Z.!?]+", r" ", text)
    return text.split()

def load_data(src_path, trg_path):
    """Load parallel data."""
    with open(src_path, 'r', encoding='utf-8') as f:
        src_sentences = [line.strip() for line in f]
    
    with open(trg_path, 'r', encoding='utf-8') as f:
        trg_sentences = [line.strip() for line in f]
    
    return list(zip(src_sentences, trg_sentences))

class TranslationDataset(Dataset):
    """Dataset for translation."""
    
    def __init__(self, data, src_vocab, trg_vocab):
        self.data = data
        self.src_vocab = src_vocab
        self.trg_vocab = trg_vocab
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        src_sentence, trg_sentence = self.data[idx]
        
        # Tokenize
        src_tokens = tokenize(src_sentence)
        trg_tokens = tokenize(trg_sentence)
        
        # Convert to indices
        src_indices = [self.src_vocab.sos_idx] + \
                      self.src_vocab.encode(src_tokens) + \
                      [self.src_vocab.eos_idx]
        
        trg_indices = [self.trg_vocab.sos_idx] + \
                      self.trg_vocab.encode(trg_tokens) + \
                      [self.trg_vocab.eos_idx]
        
        return {
            'src': torch.LongTensor(src_indices),
            'trg': torch.LongTensor(trg_indices),
            'src_len': len(src_indices)
        }

def collate_fn(batch):
    """Collate function for DataLoader."""
    # Sort by source length (descending)
    batch = sorted(batch, key=lambda x: x['src_len'], reverse=True)
    
    src_batch = [item['src'] for item in batch]
    trg_batch = [item['trg'] for item in batch]
    src_len_batch = [item['src_len'] for item in batch]
    
    # Pad sequences
    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)
    trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=0)
    
    return {
        'src': src_padded,
        'trg': trg_padded,
        'src_len': torch.LongTensor(src_len_batch)
    }
\end{lstlisting}

\section{Attention Mechanism (Extension)}

\subsection{Luong Attention}

\begin{lstlisting}[language=Python, caption={Luong Attention - attention.py}]
import torch
import torch.nn as nn
import torch.nn.functional as F

class LuongAttention(nn.Module):
    """
    Luong Attention (Multiplicative/General)
    Score function: score(hi, st) = hi^T * Wa * st
    """
    def __init__(self, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.attn = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, decoder_hidden, encoder_outputs):
        """
        Args:
            decoder_hidden: [batch_size, hidden_size] - st (state at step t)
            encoder_outputs: [batch_size, src_len, hidden_size] - [h1, h2, ..., hn]
        
        Returns:
            context: [batch_size, hidden_size] - ct
            attention_weights: [batch_size, src_len] - alpha_t
        """
        batch_size = encoder_outputs.size(0)
        src_len = encoder_outputs.size(1)
        
        # 1. Calculate attention scores
        # Wa * st: [batch_size, hidden_size]
        decoder_hidden_transformed = self.attn(decoder_hidden)
        
        # hi^T * (Wa * st) for all i
        scores = torch.bmm(
            encoder_outputs, 
            decoder_hidden_transformed.unsqueeze(2)
        )
        # scores: [batch_size, src_len, 1]
        scores = scores.squeeze(2)  # [batch_size, src_len]
        
        # 2. Softmax to get attention weights
        attention_weights = F.softmax(scores, dim=1)  # [batch_size, src_len]
        
        # 3. Calculate context vector: ct = sum(alpha_ti * hi)
        context = torch.bmm(
            attention_weights.unsqueeze(1), 
            encoder_outputs
        )
        # context: [batch_size, 1, hidden_size]
        context = context.squeeze(1)  # [batch_size, hidden_size]
        
        return context, attention_weights
\end{lstlisting}

\subsection{Encoder with Attention}

\begin{lstlisting}[language=Python, caption={Encoder with Attention - encoder\_attention.py}]
class EncoderWithAttention(nn.Module):
    """
    Encoder returns ALL hidden states (not just the last one)
    """
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(
            emb_dim, hid_dim, n_layers,
            dropout=dropout, batch_first=True
        )
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, src_len):
        """
        Args:
            src: [batch_size, src_len]
            src_len: [batch_size]
        
        Returns:
            outputs: [batch_size, src_len, hidden_size] - ALL hidden states
            hidden: [n_layers, batch_size, hidden_size]
            cell: [n_layers, batch_size, hidden_size]
        """
        # Embedding
        embedded = self.dropout(self.embedding(src))
        # embedded: [batch_size, src_len, emb_dim]
        
        # Pack sequence
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, src_len.cpu(), batch_first=True
        )
        
        # LSTM
        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)
        
        # Unpack to get all hidden states
        outputs, _ = nn.utils.rnn.pad_packed_sequence(
            packed_outputs, batch_first=True
        )
        # outputs: [batch_size, src_len, hidden_size]
        
        return outputs, hidden, cell
\end{lstlisting}

\subsection{Decoder with Attention}

\begin{lstlisting}[language=Python, caption={Decoder with Attention - decoder\_attention.py}]
class DecoderWithAttention(nn.Module):
    """
    Decoder with Attention mechanism
    """
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):
        super().__init__()
        
        self.output_dim = output_dim
        self.hid_dim = hid_dim
        self.n_layers = n_layers
        self.attention = attention
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(
            emb_dim, hid_dim, n_layers,
            dropout=dropout, batch_first=True
        )
        self.dropout = nn.Dropout(dropout)
        
        # Layer to combine context vector with hidden state
        # Input: [context; hidden] -> Output: hidden_size
        self.fc_out = nn.Linear(hid_dim * 2, output_dim)
    
    def forward(self, input, hidden, cell, encoder_outputs):
        """
        Args:
            input: [batch_size] - token at step t
            hidden: [n_layers, batch_size, hid_dim]
            cell: [n_layers, batch_size, hid_dim]
            encoder_outputs: [batch_size, src_len, hid_dim]
        
        Returns:
            prediction: [batch_size, output_dim]
            hidden: [n_layers, batch_size, hid_dim]
            cell: [n_layers, batch_size, hid_dim]
            attention_weights: [batch_size, src_len]
        """
        # input: [batch_size] -> [batch_size, 1]
        input = input.unsqueeze(1)
        
        # Embedding
        embedded = self.dropout(self.embedding(input))
        # embedded: [batch_size, 1, emb_dim]
        
        # LSTM
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: [batch_size, 1, hid_dim]
        
        # Get last layer hidden state
        decoder_hidden = hidden[-1]  # [batch_size, hid_dim]
        
        # Calculate attention
        context, attention_weights = self.attention(
            decoder_hidden, encoder_outputs
        )
        # context: [batch_size, hid_dim]
        # attention_weights: [batch_size, src_len]
        
        # Combine context and decoder hidden
        # [context; st]
        output = output.squeeze(1)  # [batch_size, hid_dim]
        combined = torch.cat((context, output), dim=1)
        # combined: [batch_size, hid_dim * 2]
        
        # Predict
        prediction = self.fc_out(combined)  # [batch_size, output_dim]
        
        return prediction, hidden, cell, attention_weights
\end{lstlisting}

\subsection{Seq2Seq with Attention}

\begin{lstlisting}[language=Python, caption={Seq2Seq with Attention - seq2seq\_attention.py}]
class Seq2SeqWithAttention(nn.Module):
    """Complete Seq2Seq model with Attention mechanism."""
    
    def __init__(self, encoder, decoder, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hid_dim == decoder.hid_dim, \
            "Hidden dimensions must be equal"
        assert encoder.n_layers == decoder.n_layers, \
            "Number of layers must be equal"
    
    def forward(self, src, src_len, tgt, teacher_forcing_ratio=0.5):
        """
        Args:
            src: [batch_size, src_len]
            src_len: [batch_size]
            tgt: [batch_size, tgt_len]
            teacher_forcing_ratio: float
        
        Returns:
            outputs: [batch_size, tgt_len, output_dim]
        """
        batch_size = src.shape[0]
        tgt_len = tgt.shape[1]
        tgt_vocab_size = self.decoder.output_dim
        
        # Tensor to store outputs
        outputs = torch.zeros(
            batch_size, tgt_len, tgt_vocab_size
        ).to(self.device)
        
        # Encoder: get ALL hidden states
        encoder_outputs, hidden, cell = self.encoder(src, src_len)
        # encoder_outputs: [batch_size, src_len, hid_dim]
        
        # First token is <sos>
        input = tgt[:, 0]
        
        # Decode step by step
        for t in range(1, tgt_len):
            # Decoder with attention
            output, hidden, cell, attention_weights = self.decoder(
                input, hidden, cell, encoder_outputs
            )
            
            # Store prediction
            outputs[:, t] = output
            
            # Teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = tgt[:, t] if teacher_force else top1
        
        return outputs
\end{lstlisting}
