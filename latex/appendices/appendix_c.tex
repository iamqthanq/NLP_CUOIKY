% ============================================
% PHỤ LỤC C: CHECKPOINT VÀ LIÊN KẾT
% ============================================

\chapter{Checkpoint và liên kết}
\label{app:checkpoints}

\section{Google Drive Links}

Các file checkpoint và mã nguồn đầy đủ có thể được tải xuống từ các liên kết sau:

\subsection{Model Checkpoints}

\begin{itemize}
    \item \textbf{Best Model Checkpoint}: \texttt{best\_model.pth} \\
    \textit{Size}: ~50 MB \\
    \textit{Contains}: Model state dict, optimizer state, training metrics \\
    \textit{Epoch}: 12 (early stopping) \\
    \textit{Validation Loss}: 3.245 \\
    \textit{BLEU Score}: 23.4\% \\
    \url{https://drive.google.com/file/d/<YOUR_FILE_ID>/view?usp=sharing}
    
    \item \textbf{Source Vocabulary}: \texttt{src\_vocab.pth} \\
    \textit{Size}: ~2 MB \\
    \textit{Contains}: English vocabulary (10,000 words) \\
    \url{https://drive.google.com/file/d/<YOUR_FILE_ID>/view?usp=sharing}
    
    \item \textbf{Target Vocabulary}: \texttt{tgt\_vocab.pth} \\
    \textit{Size}: ~2 MB \\
    \textit{Contains}: French vocabulary (10,000 words) \\
    \url{https://drive.google.com/file/d/<YOUR_FILE_ID>/view?usp=sharing}
\end{itemize}

\subsection{Source Code Repository}

\begin{itemize}
    \item \textbf{GitHub Repository}: \\
    \url{https://github.com/<YOUR_USERNAME>/nlp-final-project}
    
    \item \textbf{Google Colab Notebook}: \\
    \url{https://colab.research.google.com/drive/<YOUR_NOTEBOOK_ID>}
\end{itemize}

\section{Hướng dẫn sử dụng checkpoint}

\subsection{Tải xuống checkpoint}

\begin{lstlisting}[language=bash, caption={Download checkpoints}]
# Tạo thư mục checkpoint
mkdir -p check_point

# Download using gdown (Google Drive CLI tool)
pip install gdown

# Download model
gdown https://drive.google.com/uc?id=<YOUR_FILE_ID> -O check_point/best_model.pth

# Download vocabularies
gdown https://drive.google.com/uc?id=<YOUR_FILE_ID> -O check_point/src_vocab.pth
gdown https://drive.google.com/uc?id=<YOUR_FILE_ID> -O check_point/tgt_vocab.pth
\end{lstlisting}

\subsection{Load checkpoint và inference}

\begin{lstlisting}[language=Python, caption={Load and use checkpoint}]
import torch
from vocab import Vocabulary
from encoder import Encoder
from decoder import Decoder
from seq2seq import Seq2Seq
from translate import translate_sentence

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load vocabularies
src_vocab = torch.load('check_point/src_vocab.pth')
tgt_vocab = torch.load('check_point/tgt_vocab.pth')

# Model parameters (must match training config)
INPUT_DIM = len(src_vocab)
OUTPUT_DIM = len(tgt_vocab)
EMB_DIM = 256
HID_DIM = 512
N_LAYERS = 2
DROPOUT = 0.3

# Initialize model
enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)
model = Seq2Seq(enc, dec, device).to(device)

# Load checkpoint
checkpoint = torch.load('check_point/best_model.pth', map_location=device)
model.load_state_dict(checkpoint['model_state_dict'])

print(f"Loaded checkpoint from epoch {checkpoint['epoch']}")
print(f"Validation Loss: {checkpoint['val_loss']:.3f}")

# Translate examples
test_sentences = [
    "A dog is running in the grass.",
    "A man is playing football.",
    "A woman in a red dress is dancing.",
]

model.eval()
for sentence in test_sentences:
    translation = translate_sentence(
        sentence, src_vocab, tgt_vocab, model, device
    )
    print(f"Source: {sentence}")
    print(f"Translation: {translation}\n")
\end{lstlisting}

\section{Cấu trúc thư mục dự án}

\begin{lstlisting}[caption={Project directory structure}]
nlp-final-project/
├── check_point/              # Model checkpoints
│   ├── best_model.pth       # Best model weights
│   ├── src_vocab.pth        # Source vocabulary
│   └── tgt_vocab.pth        # Target vocabulary
│
├── data/                     # Multi30K dataset
│   ├── train.en             # Training data (English)
│   ├── train.fr             # Training data (French)
│   ├── val.en               # Validation data (English)
│   ├── val.fr               # Validation data (French)
│   ├── test.en              # Test data (English)
│   └── test.fr              # Test data (French)
│
├── src/                      # Source code
│   ├── vocab.py             # Vocabulary class
│   ├── encoder.py           # Encoder module
│   ├── decoder.py           # Decoder module
│   ├── seq2seq.py           # Seq2Seq model
│   ├── train.py             # Training functions
│   ├── translate.py         # Inference functions
│   └── data.py              # Data preprocessing
│
├── notebooks/                # Jupyter notebooks
│   └── NLP_Final_Project_Seq2Seq_Translation.ipynb
│
├── report/                   # Report documents
│   ├── BAO_CAO_CUOI_KY.pdf
│   └── latex/               # LaTeX source
│
├── requirements.txt          # Python dependencies
└── README.md                # Project documentation
\end{lstlisting}

\section{Yêu cầu hệ thống}

\subsection{Phần cứng}

\begin{itemize}
    \item \textbf{RAM}: Tối thiểu 8 GB (khuyến nghị 16 GB)
    \item \textbf{GPU}: NVIDIA GPU với tối thiểu 4 GB VRAM (khuyến nghị T4/V100)
    \item \textbf{Disk}: 500 MB cho checkpoints + dataset
\end{itemize}

\subsection{Phần mềm}

\begin{itemize}
    \item \textbf{Python}: 3.8 hoặc cao hơn
    \item \textbf{PyTorch}: 2.0.1 (hoặc tương thích)
    \item \textbf{CUDA}: 11.8 (nếu sử dụng GPU)
    \item \textbf{OS}: Linux/Windows/MacOS
\end{itemize}

\section{Thời gian thực thi}

\begin{table}[H]
\centering
\caption{Thời gian thực thi trên các thiết bị}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Device} & \textbf{Training (1 epoch)} & \textbf{Full Training} & \textbf{Inference (1 sentence)} \\ 
\midrule
Tesla T4 (Google Colab) & 7-8 minutes & ~1.5 hours & ~0.1 seconds \\
V100 (32GB) & 3-4 minutes & ~45 minutes & ~0.05 seconds \\
CPU (16 cores) & 45-60 minutes & ~10 hours & ~0.5 seconds \\
\bottomrule
\end{tabular}
\end{table}

\section{Xử lý lỗi thường gặp}

\subsection{Out of Memory (OOM)}

\begin{lstlisting}[language=Python, caption={Giảm batch size nếu gặp OOM}]
# Giảm batch_size trong config
CONFIG['batch_size'] = 32  # Thay vì 64

# Hoặc sử dụng gradient accumulation
for i, batch in enumerate(train_iterator):
    loss = train_step(batch)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
\end{lstlisting}

\subsection{CUDA not available}

\begin{lstlisting}[language=Python, caption={Fallback to CPU}]
import torch

# Kiểm tra CUDA
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device('cpu')
    print("CUDA not available. Using CPU.")
    # Giảm batch_size để tăng tốc trên CPU
    CONFIG['batch_size'] = 16
\end{lstlisting}

\subsection{Vocabulary mismatch}

\begin{lstlisting}[language=Python, caption={Kiểm tra vocabulary compatibility}]
# Khi load checkpoint, kiểm tra kích thước vocabulary
checkpoint = torch.load('best_model.pth')
model_vocab_size = checkpoint['model_state_dict']['encoder.embedding.weight'].shape[0]

if model_vocab_size != len(src_vocab):
    raise ValueError(
        f"Vocabulary size mismatch: "
        f"Model expects {model_vocab_size}, got {len(src_vocab)}"
    )
\end{lstlisting}

\section{Liên hệ và hỗ trợ}

\begin{itemize}
    \item \textbf{Email}: your.email@example.com
    \item \textbf{GitHub Issues}: \url{https://github.com/<YOUR_USERNAME>/nlp-final-project/issues}
    \item \textbf{Documentation}: \url{https://github.com/<YOUR_USERNAME>/nlp-final-project/wiki}
\end{itemize}
