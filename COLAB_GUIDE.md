# üéì H∆Ø·ªöNG D·∫™N CHUY·ªÇN SANG GOOGLE COLAB

## üéØ T·∫°i sao n√™n d√πng Google Colab?

Theo **L∆∞u √Ω quan tr·ªçng m·ª•c 11** c·ªßa th·∫ßy:
> "M√£ ngu·ªìn ph·∫£i ch·∫°y ƒë∆∞·ª£c t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi tr√™n Google Colab ho·∫∑c m√°y local"

**L·ª£i √≠ch:**
- ‚úÖ GPU mi·ªÖn ph√≠ (T4/P100) ‚Üí Training nhanh h∆°n **10-20 l·∫ßn**
- ‚úÖ Kh√¥ng c·∫ßn c√†i ƒë·∫∑t m√¥i tr∆∞·ªùng ph·ª©c t·∫°p
- ‚úÖ Export notebook (.ipynb) + PDF b√°o c√°o d·ªÖ d√†ng
- ‚úÖ Checkpoint l∆∞u tr·ª±c ti·∫øp Google Drive

---

## üìã B∆Ø·ªöC 1: T·∫°o Notebook tr√™n Colab

1. Truy c·∫≠p: https://colab.research.google.com/
2. Ch·ªçn **File ‚Üí New Notebook**
3. ƒê·ªïi t√™n: `NLP_Do_An_EnFr_Translation.ipynb`
4. **B·∫¨T GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **GPU** ‚Üí Save

---

## üìÅ B∆Ø·ªöC 2: Upload Data l√™n Colab

### C√°ch 1: Upload tr·ª±c ti·∫øp (nhanh, nh∆∞ng m·∫•t khi runtime restart)

```python
# Cell 1: Upload data files
from google.colab import files
import os

# T·∫°o th∆∞ m·ª•c data
!mkdir -p /content/data

# Upload 6 files: train.en, train.fr, val.en, val.fr, test.en, test.fr
# Click "Choose Files" v√† ch·ªçn t·∫•t c·∫£ 6 files t·ª´ th∆∞ m·ª•c data/ tr√™n m√°y
uploaded = files.upload()

# Di chuy·ªÉn v√†o th∆∞ m·ª•c data
for filename in uploaded.keys():
    !mv {filename} /content/data/
    
print("‚úÖ ƒê√£ upload xong data!")
```

### C√°ch 2: L∆∞u tr√™n Google Drive (khuy·∫øn ngh·ªã - b·ªÅn v·ªØng)

```python
# Cell 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Cell 2: T·∫°o th∆∞ m·ª•c project tr√™n Drive
!mkdir -p "/content/drive/MyDrive/NLP_Do_An"
!mkdir -p "/content/drive/MyDrive/NLP_Do_An/data"
!mkdir -p "/content/drive/MyDrive/NLP_Do_An/check_point"

# Sau ƒë√≥ upload 6 files data v√†o:
# Google Drive ‚Üí MyDrive ‚Üí NLP_Do_An ‚Üí data/
# (K√©o th·∫£ t·ª´ m√°y local v√†o Drive)

# Cell 3: Symbolic link
!ln -s "/content/drive/MyDrive/NLP_Do_An/data" /content/data
!ln -s "/content/drive/MyDrive/NLP_Do_An/check_point" /content/check_point

print("‚úÖ Data s·∫µn s√†ng!")
```

---

## üîß B∆Ø·ªöC 3: C√†i ƒë·∫∑t Dependencies

```python
# Cell 2: Install dependencies
!pip install -q spacy torch nltk matplotlib seaborn tqdm

# Download spaCy models
!python -m spacy download en_core_web_sm
!python -m spacy download fr_core_news_sm

print("‚úÖ C√†i ƒë·∫∑t ho√†n t·∫•t!")
```

---

## üìù B∆Ø·ªöC 4: Upload v√† Ch·∫°y File Config

### üéØ C√°ch 1: Upload file config.py (KHUY·∫æN NGH·ªä - Nhanh nh·∫•t)

```python
# Cell 3: Upload v√† ch·∫°y file config.py
from google.colab import files
import sys

# Upload file config.py t·ª´ m√°y local (trong th∆∞ m·ª•c src/)
print("üì§ Ch·ªçn file config.py t·ª´ th∆∞ m·ª•c src/...")
uploaded = files.upload()

# L∆∞u v√†o th∆∞ m·ª•c hi·ªán t·∫°i
!mkdir -p /content/src
for filename in uploaded.keys():
    with open(f'/content/src/{filename}', 'wb') as f:
        f.write(uploaded[filename])

# Import v√† ch·∫°y config
sys.path.append('/content/src')
from config import *

print(f"‚úÖ Config ƒë√£ load!")
print(f"üöÄ Device: {DEVICE}")
print(f"üìä Batch size: {BATCH_SIZE}, Max vocab: {MAX_VOCAB_SIZE}")
```

### üéØ C√°ch 2: Copy code t·ª´ config.py v√†o cell (N·∫øu kh√¥ng mu·ªën upload file)

```python
# Cell 3: Configuration (copy t·ª´ src/config.py)
import torch
from pathlib import Path

# Paths
DATA_DIR = Path("/content/data")
CHECKPOINT_DIR = Path("/content/check_point")

# Vocabulary
MAX_VOCAB_SIZE = 10000
PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN = "<pad>", "<unk>", "<sos>", "<eos>"
PAD_IDX, UNK_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3
SPECIAL_TOKENS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]
MIN_FREQ = 1

# Data files
TRAIN_EN = DATA_DIR / "train.en"
TRAIN_FR = DATA_DIR / "train.fr"
VAL_EN = DATA_DIR / "val.en"
VAL_FR = DATA_DIR / "val.fr"
TEST_EN = DATA_DIR / "test.en"
TEST_FR = DATA_DIR / "test.fr"

# Training config
BATCH_SIZE = 64
MAX_SEQ_LENGTH = 50
EMBEDDING_DIM = 256
HIDDEN_SIZE = 512
NUM_LAYERS = 2
DROPOUT = 0.3
TEACHER_FORCING_RATIO = 0.5
LEARNING_RATE = 0.001
NUM_EPOCHS = 15
EARLY_STOPPING_PATIENCE = 3

# Device
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üöÄ Device: {DEVICE}")
```
 
---

## üìù B∆Ø·ªöC 5: Upload v√† Ch·∫°y File Utils & Data Loader

### Cell 4: Upload utils.py

```python
# Cell 4: Upload v√† ch·∫°y utils.py
print("üì§ Ch·ªçn file utils.py t·ª´ th∆∞ m·ª•c src/...")
uploaded = files.upload()

for filename in uploaded.keys():
    with open(f'/content/src/{filename}', 'wb') as f:
        f.write(uploaded[filename])

# Import utils
from utils import *
print("‚úÖ Utils ƒë√£ load!")
```

**HO·∫∂C copy to√†n b·ªô code t·ª´ `src/utils.py`** v√†o cell n√†y (n·∫øu kh√¥ng mu·ªën upload).

### Cell 5: Upload data_loader.py

```python
# Cell 5: Upload v√† ch·∫°y data_loader.py
print("üì§ Ch·ªçn file data_loader.py t·ª´ th∆∞ m·ª•c src/...")
uploaded = files.upload()

for filename in uploaded.keys():
    with open(f'/content/src/{filename}', 'wb') as f:
        f.write(uploaded[filename])

# Import data_loader
from data_loader import *
print("‚úÖ Data loader ƒë√£ load!")
```

**HO·∫∂C copy to√†n b·ªô code t·ª´ `src/data_loader.py`** v√†o cell n√†y.

### Cell 6: Build Vocabularies

```python
# ============ BUILD VOCABULARIES ============
print("Building vocabularies...")
src_vocab, tgt_vocab = build_vocabularies(TRAIN_EN, TRAIN_FR, MAX_VOCAB_SIZE)

# Save vocabularies
save_vocab(src_vocab, CHECKPOINT_DIR / "src_vocab.pth")
save_vocab(tgt_vocab, CHECKPOINT_DIR / "tgt_vocab.pth")

print(f"‚úÖ English vocab: {len(src_vocab)} tokens")
print(f"‚úÖ French vocab: {len(tgt_vocab)} tokens")
```

### Cell 7: Prepare DataLoaders

```python
# ============ PREPARE DATALOADERS ============
train_loader, val_loader, test_loader = prepare_data_loaders(
    src_vocab, tgt_vocab, BATCH_SIZE
)

# Test m·ªôt batch
for src_batch, src_lengths, tgt_batch, tgt_lengths in train_loader:
    print(f"‚úÖ Source batch: {src_batch.shape}")
    print(f"‚úÖ Target batch: {tgt_batch.shape}")
    print(f"‚úÖ Source lengths (sorted): {src_lengths[:5]}")
    break
```

---

## üèóÔ∏è B∆Ø·ªöC 5: Model (Task 3 - Ch∆∞a c√≥ code)

### Cell 8: Encoder

```python
# ============ ENCODER ============
import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers, 
                           dropout=dropout, batch_first=True)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, src, src_lengths):
        # src: (batch_size, seq_len)
        embedded = self.dropout(self.embedding(src))
        
        # Pack padded sequence
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=True
        )
        
        # LSTM
        packed_output, (hidden, cell) = self.lstm(packed)
        
        # Unpack
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        # Context vector = hidden state c·ªßa layer cu·ªëi
        return hidden, cell
```

### Cell 9: Decoder

```python
# ============ DECODER ============
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers,
                           dropout=dropout, batch_first=True)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, input, hidden, cell):
        # input: (batch_size, 1)
        input = input.unsqueeze(1) if input.dim() == 1 else input
        
        embedded = self.dropout(self.embedding(input))
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(1))
        
        return prediction, hidden, cell
```

### Cell 10: Seq2Seq

```python
# ============ SEQ2SEQ ============
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
    
    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):
        batch_size = src.shape[0]
        tgt_len = tgt.shape[1]
        tgt_vocab_size = self.decoder.fc_out.out_features
        
        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)
        
        # Encoder
        hidden, cell = self.encoder(src, src_lengths)
        
        # Decoder input: <sos>
        input = tgt[:, 0]
        
        for t in range(1, tgt_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t] = output
            
            # Teacher forcing
            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = tgt[:, t] if teacher_force else top1
        
        return outputs

# Initialize model
encoder = Encoder(len(src_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)
decoder = Decoder(len(tgt_vocab), EMBEDDING_DIM, HIDDEN_SIZE, NUM_LAYERS, DROPOUT)
model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)

print(f"‚úÖ Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
```

---

## üèãÔ∏è B∆Ø·ªöC 6: Training (Task 4)

```python
# ============ TRAINING ============
import torch.optim as optim
from tqdm import tqdm

# Loss & optimizer
criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)

# Training function
def train_epoch(model, loader, optimizer, criterion):
    model.train()
    epoch_loss = 0
    
    for src, src_len, tgt, tgt_len in tqdm(loader, desc="Training"):
        optimizer.zero_grad()
        
        output = model(src, src_len, tgt, TEACHER_FORCING_RATIO)
        
        # Reshape for loss
        output_dim = output.shape[-1]
        output = output[:, 1:].reshape(-1, output_dim)
        tgt = tgt[:, 1:].reshape(-1)
        
        loss = criterion(output, tgt)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)
        optimizer.step()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(loader)

# Validation function
def evaluate(model, loader, criterion):
    model.eval()
    epoch_loss = 0
    
    with torch.no_grad():
        for src, src_len, tgt, tgt_len in loader:
            output = model(src, src_len, tgt, 0)  # No teacher forcing
            
            output_dim = output.shape[-1]
            output = output[:, 1:].reshape(-1, output_dim)
            tgt = tgt[:, 1:].reshape(-1)
            
            loss = criterion(output, tgt)
            epoch_loss += loss.item()
    
    return epoch_loss / len(loader)

# Training loop
best_val_loss = float('inf')
patience_counter = 0
train_losses, val_losses = [], []

for epoch in range(NUM_EPOCHS):
    train_loss = train_epoch(model, train_loader, optimizer, criterion)
    val_loss = evaluate(model, val_loader, criterion)
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    print(f"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}")
    
    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), CHECKPOINT_DIR / "best_model.pth")
        print("‚úÖ Saved best model!")
    else:
        patience_counter += 1
        if patience_counter >= EARLY_STOPPING_PATIENCE:
            print("‚ö†Ô∏è Early stopping!")
            break
    
    scheduler.step(val_loss)

print("‚úÖ Training completed!")
```

---

## üìä B∆Ø·ªöC 7: Visualization

```python
# ============ PLOT LOSSES ============
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training & Validation Loss')
plt.grid(True)
plt.show()
```

---

## üîç B∆Ø·ªöC 8: Evaluation & Translation

```python
# ============ TRANSLATE FUNCTION ============
def translate(sentence, src_vocab, tgt_vocab, model, device, max_len=50):
    model.eval()
    
    # Tokenize
    tokens = tokenize_sentence(sentence)
    tokens = add_special_tokens(tokens, add_sos=True, add_eos=True)
    
    # Encode
    src_indices = src_vocab.encode(tokens)
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)
    src_len = torch.LongTensor([len(src_indices)])
    
    with torch.no_grad():
        # Encoder
        hidden, cell = model.encoder(src_tensor, src_len)
        
        # Decoder (greedy)
        input = torch.LongTensor([tgt_vocab.sos_idx]).to(device)
        output_tokens = []
        
        for _ in range(max_len):
            output, hidden, cell = model.decoder(input, hidden, cell)
            pred_token = output.argmax(1).item()
            
            if pred_token == tgt_vocab.eos_idx:
                break
            
            output_tokens.append(pred_token)
            input = torch.LongTensor([pred_token]).to(device)
    
    # Decode
    translated = tgt_vocab.decode(output_tokens)
    return ' '.join(translated)

# Test
test_sentences = [
    "A man is walking in the street.",
    "The cat is sleeping on the bed.",
    "I love programming."
]

for sent in test_sentences:
    translated = translate(sent, src_vocab, tgt_vocab, model, DEVICE)
    print(f"EN: {sent}")
    print(f"FR: {translated}")
    print()
```

---

## üì• B∆Ø·ªöC 9: Export Notebook

1. **Download notebook**: File ‚Üí Download ‚Üí Download .ipynb
2. **Export PDF**: File ‚Üí Print ‚Üí Save as PDF
3. **Download checkpoint**: 
   ```python
   from google.colab import files
   files.download('/content/check_point/best_model.pth')
   files.download('/content/check_point/src_vocab.pth')
   files.download('/content/check_point/tgt_vocab.pth')
   ```

---

## ‚úÖ CHECKLIST HO√ÄN T·∫§T

- [ ] T·∫°o notebook tr√™n Colab
- [ ] B·∫≠t GPU (Runtime ‚Üí Change runtime type)
- [ ] Upload data l√™n Colab
- [ ] C√†i ƒë·∫∑t dependencies
- [ ] Copy code Task 1-2 (config, utils, data_loader)
- [ ] Implement Task 3 (Encoder-Decoder model)
- [ ] Implement Task 4 (Training loop)
- [ ] Implement Task 5 (translate function)
- [ ] Test translate tr√™n v√†i c√¢u
- [ ] ƒê√°nh gi√° BLEU score tr√™n test set
- [ ] V·∫Ω bi·ªÉu ƒë·ªì train/val loss
- [ ] Ph√¢n t√≠ch 5 v√≠ d·ª• l·ªói d·ªãch
- [ ] Download notebook (.ipynb)
- [ ] Export PDF b√°o c√°o
- [ ] Download checkpoint files

---

## üéØ L∆ØU √ù QUAN TR·ªåNG

1. **GPU Runtime**: Colab free c√≥ gi·ªõi h·∫°n ~12 gi·ªù/session. N√™n:
   - Save checkpoint th∆∞·ªùng xuy√™n
   - Mount Google Drive ƒë·ªÉ l∆∞u checkpoint t·ª± ƒë·ªông

2. **Disconnect**: N·∫øu b·ªã disconnect, ch·∫°y l·∫°i t·ª´ Cell "Mount Drive" l√† c√≥ th·ªÉ load l·∫°i checkpoint.

3. **B√°o c√°o PDF**: 
   - Ph·∫£i c√≥: s∆° ƒë·ªì ki·∫øn tr√∫c, bi·ªÉu ƒë·ªì loss, BLEU score, 5 v√≠ d·ª• d·ªãch + ph√¢n t√≠ch
   - In tr·ª±c ti·∫øp t·ª´ notebook ho·∫∑c vi·∫øt ri√™ng trong Word/LaTeX

4. **N·ªôp b√†i**: 
   - 01 file PDF b√°o c√°o (ƒë·∫ßy ƒë·ªß n·ªôi dung)
   - M√£ ngu·ªìn: notebook (.ipynb) + checkpoint (.pth) (n√©n th√†nh .zip)

---

**Good luck! üöÄ**
