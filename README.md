# Äá»“ Ã¡n NLP - Dá»‹ch mÃ¡y Anh-PhÃ¡p vá»›i LSTM Encoder-Decoder

## ğŸ“‹ Giá»›i thiá»‡u
Äá»“ Ã¡n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn: XÃ¢y dá»±ng mÃ´ hÃ¬nh Encoder-Decoder LSTM vá»›i context vector cá»‘ Ä‘á»‹nh Ä‘á»ƒ dá»‹ch tá»« tiáº¿ng Anh sang tiáº¿ng PhÃ¡p.

**Dataset**: Multi30K (en-fr)
- Train: 29,000 cáº·p cÃ¢u
- Validation: 1,000 cáº·p cÃ¢u  
- Test: 1,000 cáº·p cÃ¢u

## ğŸ¯ Má»¥c tiÃªu
1. Hiá»ƒu vÃ  triá»ƒn khai Encoder-Decoder LSTM vá»›i context vector cá»‘ Ä‘á»‹nh
2. Xá»­ lÃ½ dá»¯ liá»‡u chuá»—i, huáº¥n luyá»‡n, Ä‘Ã¡nh giÃ¡ báº±ng BLEU score
3. PhÃ¢n tÃ­ch lá»—i dá»‹ch thuáº­t vÃ  Ä‘á» xuáº¥t cáº£i tiáº¿n (attention, beam search...)

## ğŸ“ Cáº¥u trÃºc project

```
NLP_DO_AN/
â”‚
â”œâ”€â”€ data/                       # Dá»¯ liá»‡u huáº¥n luyá»‡n
â”‚   â”œâ”€â”€ train.en / train.fr
â”‚   â”œâ”€â”€ val.en / val.fr
â”‚   â””â”€â”€ test.en / test.fr
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ config.py              # Cáº¥u hÃ¬nh (Task 1) âœ…
â”‚   â”œâ”€â”€ utils.py               # Utility functions (Task 1) âœ…
â”‚   â”œâ”€â”€ data_loader.py         # Data processing (Task 2) âœ…
â”‚   â”œâ”€â”€ model.py               # Encoder-Decoder LSTM (Task 3)
â”‚   â”œâ”€â”€ train.py               # Training loop (Task 4)
â”‚   â””â”€â”€ evaluate.py            # Evaluation & translate() (Task 5)
â”‚
â”œâ”€â”€ check_point/               # LÆ°u model checkpoints
â”‚   â”œâ”€â”€ src_vocab.pth
â”‚   â”œâ”€â”€ tgt_vocab.pth
â”‚   â””â”€â”€ best_model.pth
â”‚
â”œâ”€â”€ report/                    # BÃ¡o cÃ¡o PDF
â”‚
â””â”€â”€ requirements.txt           # Dependencies âœ…
```

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### 1. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

**TrÃªn mÃ¡y local (Windows):**
```powershell
# Táº¡o virtual environment
python -m venv venv
.\venv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Download spaCy models
python -m spacy download en_core_web_sm
python -m spacy download fr_core_news_sm
```

**TrÃªn Google Colab:**
```python
# Clone/upload project lÃªn Colab
# CÃ i Ä‘áº·t dependencies
!pip install spacy torch torchtext
!python -m spacy download en_core_web_sm
!python -m spacy download fr_core_news_sm

# Upload data files vÃ o /content/data/
```

### 2. Kiá»ƒm tra data loading (Task 1 & 2) âœ…

```powershell
cd src
python data_loader.py
```

Káº¿t quáº£ mong Ä‘á»£i:
- Build vocabulary: English ~10,000 tokens, French ~10,000 tokens
- DataLoader: batch size 64, sorted by length
- Test má»™t batch thÃ nh cÃ´ng

### 3. Training model (Task 3 & 4)

```powershell
python train.py
```

### 4. Evaluation & Translation (Task 5)

```powershell
python evaluate.py
```

## ğŸ“Š Tiáº¿n Ä‘á»™ hiá»‡n táº¡i

- [x] **Task 1**: Thiáº¿t láº­p mÃ´i trÆ°á»ng + cáº¥u trÃºc project
  - âœ… `config.py`: Cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§ theo yÃªu cáº§u
  - âœ… `utils.py`: Vocabulary, tokenization, helper functions
  - âœ… `requirements.txt`: Dependencies

- [x] **Task 2**: Xá»­ lÃ½ dá»¯ liá»‡u & DataLoader
  - âœ… Tokenization Ä‘Æ¡n giáº£n (lowercase + split)
  - âœ… Vocabulary building (giá»›i háº¡n 10,000 tá»« phá»• biáº¿n nháº¥t)
  - âœ… Special tokens: `<pad>`, `<unk>`, `<sos>`, `<eos>`
  - âœ… Padding/Packing: Sort batch theo Ä‘á»™ dÃ i, sá»­ dá»¥ng pack_padded_sequence
  - âœ… DataLoader: batch size 32-128 (máº·c Ä‘á»‹nh 64)

- [ ] **Task 3**: XÃ¢y dá»±ng mÃ´ hÃ¬nh Encoder-Decoder LSTM
- [ ] **Task 4**: Viáº¿t vÃ²ng train + val, early stopping, checkpoint
- [ ] **Task 5**: Viáº¿t hÃ m translate() + greedy decoding
- [ ] **Task 6**: ÄÃ¡nh giÃ¡ BLEU score + biá»ƒu Ä‘á»“ loss
- [ ] **Task 7**: PhÃ¢n tÃ­ch 5 vÃ­ dá»¥ lá»—i + Ä‘á» xuáº¥t cáº£i tiáº¿n
- [ ] **Task 8**: LÆ°u checkpoint + export bÃ¡o cÃ¡o PDF

## ğŸ”§ Cáº¥u hÃ¬nh mÃ´ hÃ¬nh

**Theo yÃªu cáº§u Ä‘á» bÃ i:**
- Embedding dimension: 256-512 (máº·c Ä‘á»‹nh 256)
- Hidden size: 512
- Number of LSTM layers: 2
- Dropout: 0.3-0.5 (máº·c Ä‘á»‹nh 0.3)
- Teacher forcing ratio: 0.5
- Optimizer: Adam(lr=0.001)
- Scheduler: ReduceLROnPlateau
- Early stopping: patience=3 epochs
- Loss: CrossEntropyLoss (ignore_index=pad_idx)

**Training:**
- Epochs: 10-20 (máº·c Ä‘á»‹nh 15)
- Batch size: 32-128 (máº·c Ä‘á»‹nh 64)
- Max sequence length: 50
- Vocab size: 10,000 (má»—i ngÃ´n ngá»¯)

## ğŸ’¡ Khuyáº¿n nghá»‹ mÃ´i trÆ°á»ng

### âœ… **Google Colab** (Khuyáº¿n nghá»‹)
**Æ¯u Ä‘iá»ƒm:**
- GPU miá»…n phÃ­ (T4/P100) â†’ Training nhanh hÆ¡n 10-20 láº§n
- KhÃ´ng cáº§n setup mÃ´i trÆ°á»ng phá»©c táº¡p
- Dá»… export notebook (.ipynb) + PDF bÃ¡o cÃ¡o
- Checkpoint lÆ°u trá»±c tiáº¿p Google Drive

**CÃ¡ch chuyá»ƒn sang Colab:**
1. Táº¡o notebook má»›i trÃªn Colab
2. Upload thÆ° má»¥c `data/` lÃªn `/content/data/`
3. Copy code tá»« `src/*.py` vÃ o cÃ¡c cells
4. Run tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
5. Export notebook + PDF

### ğŸ–¥ï¸ **MÃ¡y Local**
**Æ¯u Ä‘iá»ƒm:**
- Code/debug nhanh
- ToÃ n quyá»n kiá»ƒm soÃ¡t

**YÃªu cáº§u:**
- GPU NVIDIA (khuyáº¿n nghá»‹) hoáº·c cháº¥p nháº­n training cháº­m
- Python >= 3.8
- PyTorch vá»›i CUDA support

## ğŸ“ˆ Thang Ä‘iá»ƒm (10 Ä‘iá»ƒm)

1. **Triá»ƒn khai mÃ´ hÃ¬nh Encoder-Decoder LSTM** (3.0Ä‘)
2. **Xá»­ lÃ½ dá»¯ liá»‡u, DataLoader, padding/packing** (2.0Ä‘) âœ…
3. **Huáº¥n luyá»‡n á»•n Ä‘á»‹nh, early stopping, checkpoint** (1.5Ä‘)
4. **HÃ m translate() hoáº¡t Ä‘á»™ng vá»›i cÃ¢u má»›i** (1.0Ä‘)
5. **ÄÃ¡nh giÃ¡ BLEU score + biá»ƒu Ä‘á»“ loss** (1.0Ä‘)
6. **PhÃ¢n tÃ­ch 5 vÃ­ dá»¥ lá»—i + Ä‘á» xuáº¥t cáº£i tiáº¿n** (1.0Ä‘)
7. **Cháº¥t lÆ°á»£ng mÃ£ nguá»“n (sáº¡ch, comment, cáº¥u trÃºc)** (0.5Ä‘) âœ…
8. **BÃ¡o cÃ¡o (Ä‘áº§y Ä‘á»§, rÃµ rÃ ng, biá»ƒu Ä‘á»“, trÃ­ch dáº«n)** (0.5Ä‘)
9. **Äiá»ƒm cá»™ng (má»Ÿ rá»™ng: attention/beam search)** (1.0Ä‘)

## ğŸ“ LÆ°u Ã½ quan trá»ng

1. âš ï¸ **Váº¤N Äá»€ DATA**: File `val.fr` bá»‹ thiáº¿u â†’ Cáº§n táº£i láº¡i tá»« dataset gá»‘c
2. âœ… MÃ£ nguá»“n pháº£i cháº¡y Ä‘Æ°á»£c tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i trÃªn Google Colab hoáº·c mÃ¡y local
3. âœ… BÃ¡o cÃ¡o PDF pháº£i bao gá»“m: sÆ¡ Ä‘á»“ kiáº¿n trÃºc, biá»ƒu Ä‘á»“ loss, BLEU score, 5 vÃ­ dá»¥ dá»‹ch, phÃ¢n tÃ­ch lá»—i
4. âœ… Checkpoint mÃ´ hÃ¬nh (`best_model.pth`) báº¯t buá»™c ná»™p
5. âŒ KhÃ´ng sao chÃ©p mÃ£ â†’ Sáº½ bá»‹ 0 Ä‘iá»ƒm

## ğŸ“š TÃ i liá»‡u tham kháº£o

- Sutskever et al. (2014). *Sequence to Sequence Learning with Neural Networks*
- PyTorch Documentation: [torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)
- Multi30K Dataset: [https://github.com/multi30k/dataset](https://github.com/multi30k/dataset)

---

**Deadline**: 14/12/2025 (23:59)  
**HÃ¬nh thá»©c ná»™p**: 01 file PDF + mÃ£ nguá»“n (zip) qua E-Learning  
**KhÃ´ng cháº¥p nháº­n ná»™p trá»…**
