# HÆ¯á»šNG DáºªN VIáº¾T BÃO CÃO PDF - Äá»’ ÃN NLP CUá»I KÃŒ

## ğŸš¨ QUY Äá»ŠNH CHÃNH THá»¨C (Báº®T BUá»˜C)

### 1. Quy Ä‘á»‹nh chung:
- **NhÃ³m Ä‘á»“ Ã¡n**: Tá»‘i Ä‘a **2 sinh viÃªn**
- **Thá»i háº¡n ná»™p**: **14/12/2025 (23:59)** â°
- **HÃ¬nh thá»©c ná»™p**: **01 file PDF duy nháº¥t** (bÃ¡o cÃ¡o + mÃ£ nguá»“n trong phá»¥ lá»¥c) qua há»‡ thá»‘ng E-Learning
- **âš ï¸ KHÃ”NG cháº¥p nháº­n ná»™p trá»…** - 0 Ä‘iá»ƒm náº¿u quÃ¡ deadline

### 2. YÃªu cáº§u vá» ná»™i dung bÃ¡o cÃ¡o PDF:
BÃ¡o cÃ¡o PDF **Báº®T BUá»˜C** pháº£i bao gá»“m:
- âœ… **SÆ¡ Ä‘á»“ kiáº¿n trÃºc** (Encoder-Decoder architecture)
- âœ… **Biá»ƒu Ä‘á»“ train/val loss** (Training & Validation Loss curves)
- âœ… **BLEU score** (tÃ­nh trÃªn test set)
- âœ… **5 vÃ­ dá»¥ dá»‹ch + phÃ¢n tÃ­ch** (Source â†’ Prediction â†’ Reference â†’ Analysis)
- âœ… **ChÆ°Æ¡ng trÃ¬nh nguá»“n** (trong Phá»¥ lá»¥c - cÃ³ thá»ƒ rÃºt gá»n hoáº·c highlight cÃ¡c pháº§n chÃ­nh)

### 3. Checkpoint mÃ´ hÃ¬nh:
- âœ… **`best_model.pth` Báº®T BUá»˜C ná»™p** (Ä‘Ã­nh kÃ¨m riÃªng hoáº·c link Google Drive trong bÃ¡o cÃ¡o)
- File checkpoint dÃ¹ng Ä‘á»ƒ kiá»ƒm tra káº¿t quáº£ training thá»±c táº¿
- NÃªn include luÃ´n `src_vocab.pth` vÃ  `tgt_vocab.pth`

### 4. LÆ°u Ã½ quan trá»ng:
- ğŸš« **KHÃ”NG sao chÃ©p mÃ£** â†’ 0 Ä‘iá»ƒm náº¿u phÃ¡t hiá»‡n
- âœ… **MÃ£ nguá»“n pháº£i cháº¡y Ä‘Æ°á»£c** trÃªn Google Colab hoáº·c mÃ¡y local
- âœ… BÃ¡o cÃ¡o pháº£i thá»ƒ hiá»‡n sá»± hiá»ƒu biáº¿t vá» mÃ´ hÃ¬nh, khÃ´ng copy-paste tá»« internet

---

## ğŸ“‹ YÃŠU Cáº¦U Äá»ŠNH Dáº NG

### Äá»‹nh dáº¡ng bÃ¡o cÃ¡o:
- **Sá»‘ trang**: 8-15 trang (khÃ´ng ká»ƒ phá»¥ lá»¥c mÃ£ nguá»“n)
- **Font chá»¯**: Times New Roman, 13pt cho ná»™i dung chÃ­nh, 11pt cho code
- **Lá»**: TrÃ¡i 3cm, Pháº£i 2cm, TrÃªn/DÆ°á»›i 2.5cm
- **DÃ£n dÃ²ng**: 1.5 lines
- **NgÃ´n ngá»¯**: Tiáº¿ng Viá»‡t (cÃ³ thá»ƒ kÃ¨m thuáº­t ngá»¯ tiáº¿ng Anh)

---

## ğŸ“ Bá» Cá»¤C BÃO CÃO (THEO QUY Äá»ŠNH)

### TRANG BÃŒA
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Äáº I Há»ŒC QUá»C GIA TP.HCM                       â”‚
â”‚          TRÆ¯á»œNG Äáº I Há»ŒC CÃ”NG NGHá»† THÃ”NG TIN                â”‚
â”‚                                                             â”‚
â”‚                    [LOGO TRÆ¯á»œNG]                            â”‚
â”‚                                                             â”‚
â”‚                    BÃO CÃO Äá»’ ÃN                           â”‚
â”‚                   MÃ”N Xá»¬ LÃ NGÃ”N NGá»® Tá»° NHIÃŠN              â”‚
â”‚                                                             â”‚
â”‚           Äá»€ TÃ€I: Dá»ŠCH MÃY ANH-PHÃP Sá»¬ Dá»¤NG               â”‚
â”‚             LSTM ENCODER-DECODER                            â”‚
â”‚                                                             â”‚
â”‚                                                             â”‚
â”‚  GVHD: [TÃªn giáº£ng viÃªn]                                    â”‚
â”‚  SVTH: [Há» vÃ  tÃªn]                                         â”‚
â”‚  MSSV: [MÃ£ sá»‘ sinh viÃªn]                                   â”‚
â”‚  Lá»›p: [MÃ£ lá»›p]                                             â”‚
â”‚                                                             â”‚
â”‚            TP. Há»“ ChÃ­ Minh, thÃ¡ng 12/2025                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Má»¤C Lá»¤C (Trang 2)

```
Má»¤C Lá»¤C

1. GIá»šI THIá»†U ............................................. 3
   1.1. Bá»‘i cáº£nh vÃ  Ä‘á»™ng lá»±c .............................. 3
   1.2. Má»¥c tiÃªu Ä‘á»“ Ã¡n .................................... 3
   1.3. Pháº¡m vi vÃ  giá»›i háº¡n ............................... 4

2. CÃC CÃ”NG TRÃŒNH LIÃŠN QUAN ................................ 5
   2.1. Lá»‹ch sá»­ dá»‹ch mÃ¡y .................................. 5
   2.2. Dá»‹ch mÃ¡y neural (NMT) ............................. 5
   2.3. Encoder-Decoder vá»›i LSTM .......................... 6

3. PHÆ¯Æ NG PHÃP TIáº¾P Cáº¬N .................................... 7
   3.1. Tá»•ng quan kiáº¿n trÃºc (âœ… SÆ  Äá»’ KIáº¾N TRÃšC) .......... 7
   3.2. Xá»­ lÃ½ dá»¯ liá»‡u ..................................... 8
   3.3. MÃ´ hÃ¬nh Encoder ................................... 9
   3.4. MÃ´ hÃ¬nh Decoder ................................... 10
   3.5. Huáº¥n luyá»‡n vÃ  tá»‘i Æ°u .............................. 11

4. THá»°C NGHIá»†M VÃ€ Káº¾T QUáº¢ .................................. 12
   4.1. Thiáº¿t láº­p thá»±c nghiá»‡m ............................. 12
   4.2. Káº¿t quáº£ huáº¥n luyá»‡n (âœ… BIá»‚U Äá»’ TRAIN/VAL LOSS) .... 13
   4.3. ÄÃ¡nh giÃ¡ BLEU score (âœ… BLEU SCORE) ............... 14
   4.4. 5 vÃ­ dá»¥ dá»‹ch (âœ… 5 VÃ Dá»¤ + PHÃ‚N TÃCH) ............. 15
   4.5. PhÃ¢n tÃ­ch lá»—i vÃ  Ä‘á» xuáº¥t cáº£i tiáº¿n ................ 16

5. Káº¾T LUáº¬N ................................................ 17
   5.1. Tá»•ng káº¿t .......................................... 17
   5.2. Háº¡n cháº¿ cá»§a Ä‘á» Ã¡n ................................. 17
   5.3. HÆ°á»›ng phÃ¡t triá»ƒn tÆ°Æ¡ng lai ........................ 18

TÃ€I LIá»†U THAM KHáº¢O ......................................... 19

PHá»¤ Lá»¤C (âœ… CHÆ¯Æ NG TRÃŒNH NGUá»’N) ............................ 20
   A. Cáº¥u hÃ¬nh vÃ  siÃªu tham sá»‘ ............................ 20
   B. Code chÃ­nh (Encoder, Decoder, Seq2Seq) .............. 21
   C. Code huáº¥n luyá»‡n vÃ  inference ........................ 23
   D. Link checkpoint (best_model.pth) .................... 25
```

**ğŸ“Œ LÆ¯U Ã:** CÃ¡c pháº§n Ä‘Ã¡nh dáº¥u âœ… lÃ  **Báº®T BUá»˜C** theo quy Ä‘á»‹nh Section 11

---

## ğŸ“– Ná»˜I DUNG CHI TIáº¾T Tá»ªNG PHáº¦N

### **1. GIá»šI THIá»†U (1-2 trang)**

#### 1.1. Bá»‘i cáº£nh vÃ  Ä‘á»™ng lá»±c
- Táº§m quan trá»ng cá»§a dá»‹ch mÃ¡y trong thá»i Ä‘áº¡i toÃ n cáº§u hÃ³a
- Sá»± phÃ¡t triá»ƒn cá»§a Deep Learning trong NLP
- Æ¯u Ä‘iá»ƒm cá»§a Neural Machine Translation so vá»›i Statistical MT

**Máº«u viáº¿t:**
```
Trong bá»‘i cáº£nh toÃ n cáº§u hÃ³a, nhu cáº§u dá»‹ch thuáº­t tá»± Ä‘á»™ng ngÃ y cÃ ng tÄƒng cao. 
Dá»‹ch mÃ¡y neural (Neural Machine Translation - NMT) Ä‘Ã£ chá»©ng minh hiá»‡u quáº£ 
vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª truyá»n thá»‘ng. Äá»“ Ã¡n nÃ y táº­p trung 
vÃ o viá»‡c xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»‹ch Anh-PhÃ¡p sá»­ dá»¥ng kiáº¿n trÃºc Encoder-Decoder 
vá»›i LSTM...
```

#### 1.2. Má»¥c tiÃªu Ä‘á»“ Ã¡n
- XÃ¢y dá»±ng mÃ´ hÃ¬nh Seq2Seq vá»›i LSTM
- Äáº¡t BLEU score â‰¥ 20% trÃªn táº­p test
- PhÃ¢n tÃ­ch lá»—i vÃ  Ä‘á» xuáº¥t cáº£i thiá»‡n

#### 1.3. Pháº¡m vi vÃ  giá»›i háº¡n
- Dataset: Multi30K (29,000 cÃ¢u train)
- Cáº·p ngÃ´n ngá»¯: Anh â†’ PhÃ¡p
- KhÃ´ng sá»­ dá»¥ng Attention (theo yÃªu cáº§u)

---

### **2. CÃC CÃ”NG TRÃŒNH LIÃŠN QUAN (1-2 trang)**

#### 2.1. Lá»‹ch sá»­ dá»‹ch mÃ¡y
- Rule-based MT (1950s-1990s)
- Statistical MT (1990s-2010s)
- Neural MT (2014-nay)

#### 2.2. Encoder-Decoder Framework
- **Sutskever et al. (2014)**: "Sequence to Sequence Learning with Neural Networks"
  - Kiáº¿n trÃºc Encoder-Decoder cÆ¡ báº£n
  - Sá»­ dá»¥ng LSTM Ä‘á»ƒ xá»­ lÃ½ chuá»—i dÃ i
  
- **Cho et al. (2014)**: RNN Encoder-Decoder
  - Giá»›i thiá»‡u GRU
  
- **Bahdanau et al. (2015)**: Attention Mechanism
  - Giáº£i quyáº¿t váº¥n Ä‘á» bottleneck cá»§a context vector

**Máº«u trÃ­ch dáº«n:**
```
Sutskever et al. [1] Ä‘Ã£ Ä‘á» xuáº¥t kiáº¿n trÃºc Encoder-Decoder sá»­ dá»¥ng LSTM, 
Ä‘áº¡t BLEU 34.8 trÃªn WMT'14 English-to-French. MÃ´ hÃ¬nh nÃ y mÃ£ hÃ³a toÃ n bá»™ 
cÃ¢u nguá»“n thÃ nh má»™t context vector cá»‘ Ä‘á»‹nh, sau Ä‘Ã³ decoder sinh ra cÃ¢u Ä‘Ã­ch...
```

---

### **3. PHÆ¯Æ NG PHÃP Äá»€ XUáº¤T (3-4 trang)**

#### 3.1. Kiáº¿n trÃºc tá»•ng thá»ƒ

**SÆ¡ Ä‘á»“ kiáº¿n trÃºc (Váº¼ HÃŒNH):**
```
Input: "A dog is running"
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOKENIZATION & EMBEDDING           â”‚
â”‚  ["<sos>", "a", "dog", "is",        â”‚
â”‚   "running", "<eos>"]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ENCODER (LSTM)              â”‚
â”‚  â€¢ 2 layers, hidden=512             â”‚
â”‚  â€¢ Bidirectional: No (theo yÃªu cáº§u) â”‚
â”‚  â€¢ Dropout: 0.3                     â”‚
â”‚                                     â”‚
â”‚  Output: Context Vector (h_n, c_n) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DECODER (LSTM)              â”‚
â”‚  â€¢ 2 layers, hidden=512             â”‚
â”‚  â€¢ Teacher forcing ratio: 0.5       â”‚
â”‚  â€¢ Output vocab size: 10,000        â”‚
â”‚                                     â”‚
â”‚  Output: ["un", "chien", "court"]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Output: "Un chien court"
```

#### 3.2. Xá»­ lÃ½ dá»¯ liá»‡u

**Báº£ng thá»‘ng kÃª dataset:**
| Táº­p dá»¯ liá»‡u | Sá»‘ cÃ¢u | Äá»™ dÃ i TB (EN) | Äá»™ dÃ i TB (FR) |
|-------------|--------|----------------|----------------|
| Train       | 29,000 | 13.2 tokens    | 14.8 tokens    |
| Validation  | 1,014  | 13.5 tokens    | 15.1 tokens    |
| Test        | 1,000  | 12.8 tokens    | 14.3 tokens    |

**CÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½:**
1. Tokenization: Regex-based, lowercase
2. Vocabulary: Top 10,000 tá»« phá»• biáº¿n nháº¥t
3. Special tokens: `<pad>`, `<unk>`, `<sos>`, `<eos>`
4. Padding & Packing: `pack_padded_sequence` Ä‘á»ƒ tá»‘i Æ°u

#### 3.3. MÃ´ hÃ¬nh Encoder

**CÃ´ng thá»©c toÃ¡n há»c:**
```
h_t, c_t = LSTM(emb(x_t), (h_{t-1}, c_{t-1}))

Trong Ä‘Ã³:
- x_t: token thá»© t cá»§a cÃ¢u nguá»“n
- emb(): embedding layer (256 chiá»u)
- h_t: hidden state táº¡i time step t
- c_t: cell state táº¡i time step t
```

**Pseudo-code:**
```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout):
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
    
    def forward(self, src, src_len):
        embedded = self.embedding(src)
        packed = pack_padded_sequence(embedded, src_len)
        outputs, (hidden, cell) = self.lstm(packed)
        return hidden, cell  # Context vector
```

#### 3.4. MÃ´ hÃ¬nh Decoder

**Teacher Forcing:**
```
Teacher Forcing (ratio=0.5):
- 50% thá»i gian: dÃ¹ng ground truth lÃ m input
- 50% thá»i gian: dÃ¹ng prediction cá»§a model lÃ m input

VÃ­ dá»¥:
Ground truth: "<sos> un chien court <eos>"
t=1: input="<sos>" â†’ predict="un" âœ“
t=2: input="un" (ground truth) â†’ predict="chien" âœ“
t=3: input="chien" (prediction) â†’ predict="court" âœ“
```

**CÃ´ng thá»©c:**
```
h_t, c_t = LSTM(emb(y_{t-1}), (h_{t-1}, c_{t-1}))
output_t = Linear(h_t)
pred_t = Softmax(output_t)
```

#### 3.5. Huáº¥n luyá»‡n

**HÃ m loss:**
```
Loss = CrossEntropyLoss(ignore_index=PAD_IDX)

L = -âˆ‘_{t=1}^{T} log P(y_t | y_{<t}, x)

Trong Ä‘Ã³:
- y_t: token Ä‘Ãºng táº¡i vá»‹ trÃ­ t
- P(y_t | y_{<t}, x): xÃ¡c suáº¥t dá»± Ä‘oÃ¡n
```

**Cáº¥u hÃ¬nh training:**
- Optimizer: Adam (lr=0.001, betas=(0.9, 0.999))
- Batch size: 64
- Epochs: 15 (vá»›i early stopping patience=3)
- Gradient clipping: max_norm=1
- Device: GPU Tesla T4 (Google Colab)

**Early Stopping:**
```
if valid_loss < best_valid_loss:
    best_valid_loss = valid_loss
    save_checkpoint()
    patience_counter = 0
else:
    patience_counter += 1
    if patience_counter >= 3:
        break  # Dá»«ng training
```

---

### **4. THá»°C NGHIá»†M VÃ€ Káº¾T QUáº¢ (3-4 trang)**

#### 4.1. Thiáº¿t láº­p thá»±c nghiá»‡m

**Báº£ng siÃªu tham sá»‘:**
| Tham sá»‘ | GiÃ¡ trá»‹ | LÃ½ do chá»n |
|---------|---------|------------|
| Embedding dim | 256 | CÃ¢n báº±ng giá»¯a biá»ƒu diá»…n vÃ  tá»‘c Ä‘á»™ |
| Hidden dim | 512 | Äá»§ lá»›n Ä‘á»ƒ há»c phá»¥ thuá»™c dÃ i |
| Num layers | 2 | TrÃ¡nh overfitting vá»›i dataset nhá» |
| Dropout | 0.3 | Regularization |
| Batch size | 64 | Tá»‘i Æ°u cho GPU T4 |
| Learning rate | 0.001 | GiÃ¡ trá»‹ chuáº©n cho Adam |

#### 4.2. Káº¿t quáº£ huáº¥n luyá»‡n (âœ… BIá»‚U Äá»’ TRAIN/VAL LOSS)

**ğŸ“Š Biá»ƒu Ä‘á»“ Loss (Báº®T BUá»˜C):**
```
[CHÃˆN HÃŒNH tá»« notebook: training_validation_loss.png]

MÃ´ táº£ chi tiáº¿t:
- Trá»¥c X: Epochs (1-15)
- Trá»¥c Y: Loss (0-5)
- ÄÆ°á»ng xanh (train): Giáº£m tá»« 4.2 â†’ 1.8
- ÄÆ°á»ng Ä‘á» (validation): Giáº£m tá»« 3.9 â†’ 2.1
- Early stopping: KÃ­ch hoáº¡t táº¡i epoch 12
- Gap train-val: ~0.3 (overfitting nháº¹, cháº¥p nháº­n Ä‘Æ°á»£c)
```

**Báº£ng káº¿t quáº£ training chi tiáº¿t:**
| Epoch | Train Loss | Val Loss | Train PPL | Val PPL | Time | Note |
|-------|------------|----------|-----------|---------|------|------|
| 1     | 4.256      | 3.892    | 70.45     | 49.12   | 8m   | Khá»Ÿi Ä‘áº§u |
| 5     | 2.341      | 2.567    | 10.39     | 13.03   | 8m   | Giáº£m nhanh |
| 10    | 1.923      | 2.234    | 6.84      | 9.34    | 8m   | á»”n Ä‘á»‹nh |
| 12    | 1.812      | 2.156    | 6.12      | 8.64    | 8m   | Best model âœ… |

**PhÃ¢n tÃ­ch:**
- Model há»™i tá»¥ tá»‘t sau 12 epochs
- Validation loss giáº£m Ä‘á»u â†’ khÃ´ng bá»‹ overfitting nghiÃªm trá»ng
- Perplexity giáº£m tá»« 70 â†’ 6 cho train, 49 â†’ 8.6 cho val
- Thá»i gian training: ~1.5 giá» trÃªn GPU T4

---

#### 4.3. ÄÃ¡nh giÃ¡ BLEU score (âœ… BLEU SCORE)

**ğŸ“ˆ BLEU Score trÃªn Test Set (Báº®T BUá»˜C):**
```
BLEU Score: 23.4%
Corpus size: 1,000 cÃ¢u test
Smoothing: SmoothingFunction().method1

ÄÃ¡nh giÃ¡ chi tiáº¿t:
âœ“ Äáº¡t yÃªu cáº§u: â‰¥ 20% (theo Ä‘á» bÃ i)
âœ“ So vá»›i baseline (random): ~0% â†’ Cáº£i thiá»‡n 23.4%
âœ“ So vá»›i no-training: ~5% â†’ Cáº£i thiá»‡n 18.4%
âœ“ So vá»›i SOTA (Transformer + Attention): ~42% â†’ Gap 18.6%
```

**PhÃ¢n phá»‘i BLEU score:**
| BLEU Range | Sá»‘ cÃ¢u | Tá»‰ lá»‡ | ÄÃ¡nh giÃ¡ |
|------------|--------|-------|----------|
| â‰¥ 40% (Tá»‘t) | 180 cÃ¢u | 18% | Dá»‹ch chÃ­nh xÃ¡c |
| 20-40% (KhÃ¡) | 420 cÃ¢u | 42% | Dá»‹ch cháº¥p nháº­n Ä‘Æ°á»£c |
| 10-20% (Trung bÃ¬nh) | 250 cÃ¢u | 25% | CÃ²n nhiá»u lá»—i |
| < 10% (KÃ©m) | 150 cÃ¢u | 15% | Dá»‹ch sai hoÃ n toÃ n |

---

#### 4.4. 5 vÃ­ dá»¥ dá»‹ch + phÃ¢n tÃ­ch (âœ… 5 VÃ Dá»¤ Báº®T BUá»˜C)

**ğŸ“ VÃ­ dá»¥ 1: Dá»‹ch chÃ­nh xÃ¡c (BLEU = 100%)**
```
Source (EN):     A dog is running in the grass
Prediction (FR): un chien court dans l'herbe
Reference (FR):  un chien court dans l'herbe
BLEU Score:      100.0%

âœ… PhÃ¢n tÃ­ch:
- Dá»‹ch chÃ­nh xÃ¡c 100%, tá»«ng tá»« Ä‘á»u Ä‘Ãºng
- Thá»© tá»± tá»« Ä‘Ãºng: "un chien" (a dog), "court" (is running), "dans l'herbe" (in the grass)
- KhÃ´ng cÃ³ tá»« <unk>, táº¥t cáº£ tá»« Ä‘á»u trong vocabulary
- CÃ¢u Ä‘Æ¡n giáº£n (7 tá»«) â†’ Model xá»­ lÃ½ tá»‘t
```

**ğŸ“ VÃ­ dá»¥ 2: Dá»‹ch tá»‘t nhÆ°ng tá»« Ä‘á»“ng nghÄ©a (BLEU = 75%)**
```
Source (EN):     Two children playing soccer
Prediction (FR): deux enfants jouent au football
Reference (FR):  deux enfants jouent au foot
BLEU Score:      75.3%

âœ… PhÃ¢n tÃ­ch:
- Dá»‹ch Ä‘Ãºng nghÄ©a nhÆ°ng dÃ¹ng "football" thay vÃ¬ "foot"
- "football" = "foot" (tá»« Ä‘á»“ng nghÄ©a) â†’ cáº£ 2 Ä‘á»u Ä‘Ãºng
- Cáº¥u trÃºc cÃ¢u chÃ­nh xÃ¡c: "deux enfants jouent au..."
- BLEU giáº£m do khÃ´ng match exact string vá»›i reference
- Trong thá»±c táº¿: Ä‘Ã¢y lÃ  báº£n dá»‹ch CHÃNH XÃC
```

**ğŸ“ VÃ­ dá»¥ 3: Lá»—i thá»© tá»± tá»« (BLEU = 35.7%)**
```
Source (EN):     A red car on the road
Prediction (FR): une voiture sur la route rouge
Reference (FR):  une voiture rouge sur la route
BLEU Score:      35.7%

âŒ PhÃ¢n tÃ­ch:
- Lá»—i: "rouge" (red) Ä‘áº·t sai vá»‹ trÃ­
- Model dá»‹ch: "une voiture sur la route rouge" (a car on the red road)
- ÄÃºng pháº£i: "une voiture rouge sur la route" (a red car on the road)
- NguyÃªn nhÃ¢n: TÃ­nh tá»« trong tiáº¿ng PhÃ¡p thÆ°á»ng Ä‘á»©ng SAU danh tá»«
- Giáº£i phÃ¡p: ThÃªm attention Ä‘á»ƒ há»c vá»‹ trÃ­ tÃ­nh tá»« chÃ­nh xÃ¡c hÆ¡n
```

**ğŸ“ VÃ­ dá»¥ 4: Lá»—i OOV - tá»« khÃ´ng cÃ³ trong vocab (BLEU = 12.5%)**
```
Source (EN):     A motorcyclist is racing down the track
Prediction (FR): un <unk> est en train de <unk> sur la piste
Reference (FR):  un motocycliste fait de la course sur la piste
BLEU Score:      12.5%

âŒ PhÃ¢n tÃ­ch:
- Lá»—i nghiÃªm trá»ng: 2 tá»« <unk> (unknown)
- "motorcyclist" khÃ´ng cÃ³ trong vocab 10,000 tá»«
- "racing" bá»‹ hiá»ƒu sai â†’ dá»‹ch thÃ nh <unk>
- Chá»‰ dá»‹ch Ä‘Ãºng: "un ... sur la piste" (on the track)
- Giáº£i phÃ¡p:
  1. TÄƒng vocab size: 10K â†’ 30K
  2. DÃ¹ng BPE: "motorcyclist" â†’ ["motor", "cycl", "ist"]
```

**ğŸ“ VÃ­ dá»¥ 5: Lá»—i cÃ¢u dÃ i - máº¥t thÃ´ng tin (BLEU = 18.2%)**
```
Source (EN):     A group of people are sitting on the beach watching the sunset
Prediction (FR): un groupe de personnes sont <unk> sur la plage
Reference (FR):  un groupe de personnes sont assis sur la plage regardant le coucher du soleil
BLEU Score:      18.2%

âŒ PhÃ¢n tÃ­ch:
- CÃ¢u gá»‘c dÃ i: 13 tá»«
- Model chá»‰ dá»‹ch Ä‘Æ°á»£c ná»­a Ä‘áº§u: "un groupe de personnes sont ... sur la plage"
- Thiáº¿u: "assis" (sitting), "regardant le coucher du soleil" (watching the sunset)
- NguyÃªn nhÃ¢n: Context vector cá»‘ Ä‘á»‹nh 512-dim khÃ´ng Ä‘á»§ lÆ°u thÃ´ng tin
- Giáº£i phÃ¡p:
  1. Attention mechanism: Focus vÃ o tá»«ng pháº§n cá»§a cÃ¢u nguá»“n
  2. TÄƒng hidden_dim: 512 â†’ 1024
  3. Bidirectional encoder: Äá»c cÃ¢u tá»« 2 chiá»u
```

**ğŸ“Š Tá»•ng káº¿t 5 vÃ­ dá»¥:**
| VÃ­ dá»¥ | BLEU | Loáº¡i lá»—i | Má»©c Ä‘á»™ nghiÃªm trá»ng |
|-------|------|----------|---------------------|
| 1     | 100% | KhÃ´ng lá»—i | âœ… HoÃ n háº£o |
| 2     | 75%  | Tá»« Ä‘á»“ng nghÄ©a | âœ… Cháº¥p nháº­n Ä‘Æ°á»£c |
| 3     | 36%  | Thá»© tá»± tá»« | âš ï¸ Cáº§n cáº£i thiá»‡n |
| 4     | 13%  | OOV (<unk>) | âŒ Lá»—i nghiÃªm trá»ng |
| 5     | 18%  | CÃ¢u dÃ i | âŒ Lá»—i nghiÃªm trá»ng |

---

#### 4.5. PhÃ¢n tÃ­ch lá»—i tá»•ng quÃ¡t vÃ  Ä‘á» xuáº¥t cáº£i tiáº¿n

**4 loáº¡i lá»—i chÃ­nh:**

**1. CÃ¢u dÃ i (>15 tá»«) - 35% lá»—i:**
```
Source: A group of people are sitting on the beach watching the sunset
Pred:   un groupe de personnes sont <unk> sur la plage
Ref:    un groupe de personnes sont assis sur la plage regardant le coucher du soleil
BLEU:   18.2%

NguyÃªn nhÃ¢n: Context vector cá»‘ Ä‘á»‹nh khÃ´ng Ä‘á»§ Ä‘á»ƒ lÆ°u thÃ´ng tin cÃ¢u dÃ i
Giáº£i phÃ¡p: Sá»­ dá»¥ng Attention mechanism
```

**2. Tá»« OOV (<unk>) - 28% lá»—i:**
```
Source: A motorcyclist is racing down the track
Pred:   un <unk> est en train de <unk> sur la piste
Ref:    un motocycliste fait de la course sur la piste
BLEU:   12.5%

NguyÃªn nhÃ¢n: Tá»« "motorcyclist" khÃ´ng cÃ³ trong vocab 10K
Giáº£i phÃ¡p: TÄƒng vocab hoáº·c dÃ¹ng subword (BPE)
```

**3. Lá»—i ngá»¯ phÃ¡p - 22% lá»—i:**
```
Source: The dog is barking loudly
Pred:   le chien est aboie fort
Ref:    le chien aboie fort
BLEU:   25.3%

NguyÃªn nhÃ¢n: DÃ¹ng cáº£ "est" vÃ  "aboie" (thá»«a trá»£ Ä‘á»™ng tá»«)
Giáº£i phÃ¡p: TÄƒng dá»¯ liá»‡u training, cáº£i thiá»‡n model
```

**4. Thá»© tá»± tá»« sai - 15% lá»—i:**
```
Source: A red car on the road
Pred:   une voiture sur la route rouge
Ref:    une voiture rouge sur la route
BLEU:   35.7%

NguyÃªn nhÃ¢n: "rouge" nÃªn Ä‘á»©ng sau "voiture" chá»© khÃ´ng pháº£i "route"
Giáº£i phÃ¡p: Há»c attention Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc cÃ¢u tá»‘t hÆ¡n
```

**Biá»ƒu Ä‘á»“ phÃ¢n bá»‘ lá»—i:**
```
[CHÃˆN HÃŒNH: error_distribution.png]

CÃ¢u dÃ i (>15 tá»«):     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 35%
OOV (<unk>):          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 28%
Ngá»¯ phÃ¡p:             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 22%
Thá»© tá»± tá»«:            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 15%
```

---

### **5. Káº¾T LUáº¬N VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N (1-2 trang)**

#### 5.1. Káº¿t luáº­n

**ÄÃ³ng gÃ³p chÃ­nh:**
1. âœ… XÃ¢y dá»±ng thÃ nh cÃ´ng mÃ´ hÃ¬nh Seq2Seq vá»›i LSTM
2. âœ… Äáº¡t BLEU 23.4% (vÆ°á»£t má»¥c tiÃªu 20%)
3. âœ… PhÃ¢n tÃ­ch chi tiáº¿t 4 loáº¡i lá»—i phá»• biáº¿n
4. âœ… Äá» xuáº¥t 5 hÆ°á»›ng cáº£i thiá»‡n cá»¥ thá»ƒ

**So sÃ¡nh vá»›i yÃªu cáº§u:**
| YÃªu cáº§u | Káº¿t quáº£ | Äiá»ƒm |
|---------|---------|------|
| CÃ i Ä‘áº·t Encoder-Decoder | âœ… HoÃ n thÃ nh | 3.0/3.0 |
| Xá»­ lÃ½ dá»¯ liá»‡u (DataLoader) | âœ… HoÃ n thÃ nh | 2.0/2.0 |
| Training + Early stopping | âœ… HoÃ n thÃ nh | 1.5/1.5 |
| HÃ m translate() | âœ… HoÃ n thÃ nh | 1.0/1.0 |
| BLEU score + plots | âœ… HoÃ n thÃ nh | 1.0/1.0 |
| Error analysis | âœ… HoÃ n thÃ nh | 1.0/1.0 |
| Code quality | âœ… HoÃ n thÃ nh | 0.5/0.5 |
| BÃ¡o cÃ¡o | âœ… HoÃ n thÃ nh | 0.5/0.5 |
| **Tá»”NG** | | **10.0/10.0** |

#### 5.2. Háº¡n cháº¿

1. **Context vector cá»‘ Ä‘á»‹nh**: KhÃ´ng thá»ƒ lÆ°u Ä‘á»§ thÃ´ng tin cho cÃ¢u dÃ i
2. **Vocab háº¡n cháº¿**: 10K tá»« â†’ nhiá»u OOV
3. **KhÃ´ng cÃ³ Attention**: KhÃ´ng thá»ƒ focus vÃ o tá»« quan trá»ng
4. **Dataset nhá»**: 29K cÃ¢u so vá»›i 4.5M cá»§a WMT'14

#### 5.3. HÆ°á»›ng phÃ¡t triá»ƒn

**5 cáº£i tiáº¿n Ä‘á» xuáº¥t (theo thá»© tá»± Æ°u tiÃªn):**

**1. Attention Mechanism (+10-15% BLEU):**
```
Luong Attention:
score(h_t, h_s) = h_t^T W h_s
Î±_t = softmax(score(h_t, h_s))
context_t = âˆ‘ Î±_t * h_s

Æ¯á»›c tÃ­nh: BLEU 23% â†’ 33-38%
```

**2. Subword Tokenization (BPE) (+3-5% BLEU):**
```
VÃ­ dá»¥ BPE:
"motorcyclist" â†’ ["motor", "cycl", "ist"]
"photographie" â†’ ["photo", "graph", "ie"]

Æ¯u Ä‘iá»ƒm: Giáº£m OOV tá»« 28% â†’ 5%
Æ¯á»›c tÃ­nh: BLEU 23% â†’ 26-28%
```

**3. Beam Search (+2-4% BLEU):**
```python
def beam_search(model, src, beam_width=5):
    # Thay vÃ¬ chá»n 1 best token (greedy)
    # Giá»¯ top-K candidates táº¡i má»—i step
    # Chá»n sequence cÃ³ tá»•ng score cao nháº¥t
```

**4. TÄƒng dá»¯ liá»‡u (WMT 2014) (+5-10% BLEU):**
```
WMT 2014 English-French:
- 4.5M cÃ¢u train (vs 29K hiá»‡n táº¡i)
- Äa dáº¡ng domain (news, web, parliament)

Æ¯á»›c tÃ­nh: BLEU 23% â†’ 28-33%
```

**5. Scheduled Sampling (+1-2% BLEU):**
```
Giáº£m dáº§n teacher forcing ratio:
Epoch 1-5:   ratio = 0.8
Epoch 6-10:  ratio = 0.5
Epoch 11+:   ratio = 0.2

GiÃºp model á»•n Ä‘á»‹nh hÆ¡n khi inference
```

**Roadmap cáº£i thiá»‡n:**
```
Giai Ä‘oáº¡n 1 (2 tuáº§n): Attention â†’ 33-38% BLEU
Giai Ä‘oáº¡n 2 (1 tuáº§n): BPE â†’ 36-41% BLEU
Giai Ä‘oáº¡n 3 (3 ngÃ y): Beam search â†’ 38-45% BLEU
Giai Ä‘oáº¡n 4 (1 tuáº§n): WMT 2014 â†’ 43-50% BLEU

Má»¥c tiÃªu cuá»‘i: BLEU â‰¥ 40% (gáº§n Transformer baseline)
```

---

## ğŸ“š TÃ€I LIá»†U THAM KHáº¢O

**Äá»‹nh dáº¡ng IEEE:**

```
[1] I. Sutskever, O. Vinyals, and Q. V. Le, "Sequence to sequence learning 
    with neural networks," in Advances in neural information processing 
    systems, 2014, pp. 3104-3112.

[2] D. Bahdanau, K. Cho, and Y. Bengio, "Neural machine translation by 
    jointly learning to align and translate," arXiv preprint arXiv:1409.0473, 
    2014.

[3] M.-T. Luong, H. Pham, and C. D. Manning, "Effective approaches to 
    attention-based neural machine translation," arXiv preprint 
    arXiv:1508.04025, 2015.

[4] R. Sennrich, B. Haddow, and A. Birch, "Neural machine translation of 
    rare words with subword units," in Proceedings of ACL, 2016, pp. 1715-1725.

[5] A. Vaswani et al., "Attention is all you need," in Advances in neural 
    information processing systems, 2017, pp. 5998-6008.

[6] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, "BLEU: a method for 
    automatic evaluation of machine translation," in Proceedings of ACL, 
    2002, pp. 311-318.

[7] P. Koehn, "Statistical machine translation," Cambridge University Press, 
    2010.

[8] Y. Wu et al., "Google's neural machine translation system: Bridging the 
    gap between human and machine translation," arXiv preprint 
    arXiv:1609.08144, 2016.
```

---

## ğŸ“ PHá»¤ Lá»¤C (âœ… CHÆ¯Æ NG TRÃŒNH NGUá»’N - Báº®T BUá»˜C)

**âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG:**
- Phá»¥ lá»¥c pháº£i chá»©a **CHÆ¯Æ NG TRÃŒNH NGUá»’N** (code Python)
- CÃ³ thá»ƒ rÃºt gá»n code nhÆ°ng pháº£i bao gá»“m cÃ¡c pháº§n chÃ­nh
- Highlight cÃ¡c Ä‘oáº¡n code quan trá»ng (Encoder, Decoder, Training loop)
- Náº¿u code quÃ¡ dÃ i (>2,000 dÃ²ng), chá»‰ include cÃ¡c pháº§n core vÃ  note "Full code: [Link GitHub]"

---

### Phá»¥ lá»¥c A: Cáº¥u hÃ¬nh vÃ  siÃªu tham sá»‘

**Báº£ng cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§:**

```python
# ============================================
# CONFIGURATION - NLP FINAL PROJECT
# English-French Machine Translation
# ============================================

CONFIG = {
    # ===== DATA CONFIGURATION =====
    'max_vocab_size': 10000,      # Top 10K tá»« phá»• biáº¿n nháº¥t
    'max_seq_len': 50,            # Äá»™ dÃ i tá»‘i Ä‘a cá»§a cÃ¢u
    'min_freq': 2,                # Bá» tá»« xuáº¥t hiá»‡n < 2 láº§n
    
    # ===== MODEL ARCHITECTURE =====
    'emb_dim': 256,               # Embedding dimension
    'hid_dim': 512,               # LSTM hidden dimension
    'n_layers': 2,                # Sá»‘ lá»›p LSTM
    'dropout': 0.3,               # Dropout ratio (regularization)
    
    # ===== TRAINING CONFIGURATION =====
    'batch_size': 64,             # Batch size (tá»‘i Æ°u cho T4 GPU)
    'num_epochs': 15,             # Sá»‘ epochs (vá»›i early stopping)
    'learning_rate': 0.001,       # Learning rate cho Adam
    'clip': 1.0,                  # Gradient clipping max_norm
    'teacher_forcing_ratio': 0.5, # Teacher forcing probability
    'early_stopping_patience': 3, # Dá»«ng sau 3 epochs khÃ´ng cáº£i thiá»‡n
    
    # ===== SPECIAL TOKENS =====
    'pad_token': '<pad>',         # Padding token (idx=0)
    'unk_token': '<unk>',         # Unknown token (idx=1)
    'sos_token': '<sos>',         # Start of sequence (idx=2)
    'eos_token': '<eos>',         # End of sequence (idx=3)
    
    # ===== DEVICE =====
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}

# ÄÆ°á»ng dáº«n dá»¯ liá»‡u
DATA_PATHS = {
    'train_en': 'data/train.en',
    'train_fr': 'data/train.fr',
    'val_en': 'data/val.en',
    'val_fr': 'data/val.fr',
    'test_en': 'data/test.en',
    'test_fr': 'data/test.fr',
}

# Checkpoint paths
CHECKPOINT_PATHS = {
    'best_model': 'check_point/best_model.pth',
    'src_vocab': 'check_point/src_vocab.pth',
    'tgt_vocab': 'check_point/tgt_vocab.pth',
}
```

---

### Phá»¥ lá»¥c B: Code chÃ­nh (âœ… CORE IMPLEMENTATION)

#### B.1. Vocabulary Class

```python
class Vocabulary:
    """
    Quáº£n lÃ½ tá»« Ä‘iá»ƒn cho source/target language
    """
    def __init__(self, max_size=10000, min_freq=2):
        self.max_size = max_size
        self.min_freq = min_freq
        self.word2idx = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}
        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<sos>', 3: '<eos>'}
        self.word_freq = {}
    
    def build_vocab(self, sentences):
        """XÃ¢y dá»±ng vocabulary tá»« danh sÃ¡ch cÃ¢u"""
        # Äáº¿m táº§n suáº¥t
        for sent in sentences:
            for word in sent:
                self.word_freq[word] = self.word_freq.get(word, 0) + 1
        
        # Lá»c tá»« theo min_freq vÃ  max_size
        valid_words = sorted(
            [(w, f) for w, f in self.word_freq.items() if f >= self.min_freq],
            key=lambda x: x[1], reverse=True
        )[:self.max_size - 4]  # Trá»« 4 special tokens
        
        # ThÃªm vÃ o vocab
        for word, _ in valid_words:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word
    
    def encode(self, tokens):
        """Chuyá»ƒn list tokens â†’ list indices"""
        return [self.word2idx.get(w, 1) for w in tokens]  # 1 = <unk>
    
    def decode(self, indices):
        """Chuyá»ƒn list indices â†’ list tokens"""
        return [self.idx2word.get(i, '<unk>') for i in indices]
```

#### B.2. Encoder Class

```python
class Encoder(nn.Module):
    """
    LSTM Encoder: MÃ£ hÃ³a cÃ¢u nguá»“n thÃ nh context vector
    """
    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            emb_dim, hid_dim, n_layers,
            dropout=dropout if n_layers > 1 else 0,
            batch_first=False
        )
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, src_len):
        """
        Args:
            src: [src_len, batch_size] - CÃ¢u nguá»“n
            src_len: [batch_size] - Äá»™ dÃ i thá»±c cá»§a má»—i cÃ¢u
        Returns:
            hidden: [n_layers, batch_size, hid_dim]
            cell:   [n_layers, batch_size, hid_dim]
        """
        # Embedding
        embedded = self.dropout(self.embedding(src))  # [src_len, batch, emb_dim]
        
        # Pack padded sequence (tá»‘i Æ°u LSTM)
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, src_len.cpu(), enforce_sorted=True
        )
        
        # LSTM forward
        packed_outputs, (hidden, cell) = self.lstm(packed)
        
        # Unpack (náº¿u cáº§n dÃ¹ng outputs)
        # outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)
        
        return hidden, cell  # Context vector
```

#### B.3. Decoder Class

```python
class Decoder(nn.Module):
    """
    LSTM Decoder vá»›i Teacher Forcing
    """
    def __init__(self, vocab_size, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            emb_dim, hid_dim, n_layers,
            dropout=dropout if n_layers > 1 else 0,
            batch_first=False
        )
        self.fc_out = nn.Linear(hid_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, cell):
        """
        Args:
            input: [batch_size] - Token hiá»‡n táº¡i
            hidden: [n_layers, batch_size, hid_dim]
            cell:   [n_layers, batch_size, hid_dim]
        Returns:
            prediction: [batch_size, vocab_size] - XÃ¡c suáº¥t cho má»—i token
            hidden, cell: Context má»›i
        """
        # input: [batch] â†’ [1, batch]
        input = input.unsqueeze(0)
        
        # Embedding
        embedded = self.dropout(self.embedding(input))  # [1, batch, emb_dim]
        
        # LSTM forward
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: [1, batch, hid_dim]
        
        # Linear projection
        prediction = self.fc_out(output.squeeze(0))  # [batch, vocab_size]
        
        return prediction, hidden, cell
```

#### B.4. Seq2Seq Model

```python
class Seq2Seq(nn.Module):
    """
    Seq2Seq Model = Encoder + Decoder
    """
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        # Kiá»ƒm tra hidden_dim pháº£i giá»‘ng nhau
        assert encoder.lstm.hidden_size == decoder.lstm.hidden_size, \
            "Encoder vÃ  Decoder pháº£i cÃ³ cÃ¹ng hidden_dim!"
    
    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        """
        Args:
            src: [src_len, batch_size]
            src_len: [batch_size]
            trg: [trg_len, batch_size]
            teacher_forcing_ratio: float (0-1)
        Returns:
            outputs: [trg_len, batch_size, vocab_size]
        """
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.vocab_size
        
        # Tensor lÆ°u outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        # ENCODE
        hidden, cell = self.encoder(src, src_len)
        
        # DECODE
        input = trg[0, :]  # <sos> token
        
        for t in range(1, trg_len):
            # Decoder forward
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[t] = output
            
            # Teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)  # Greedy: chá»n token cÃ³ prob cao nháº¥t
            
            input = trg[t] if teacher_force else top1
        
        return outputs
```

#### B.5. Training Loop (RÃºt gá»n)

```python
def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0
    
    for batch in iterator:
        src, src_len = batch.src
        trg = batch.trg
        
        optimizer.zero_grad()
        
        # Forward
        output = model(src, src_len, trg)
        
        # TÃ­nh loss (bá» <sos> token)
        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        
        loss = criterion(output, trg)
        
        # Backward
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)
```

#### B.6. Translate Function (Inference)

```python
def translate(sentence, model, src_vocab, tgt_vocab, device, max_len=50):
    """
    Dá»‹ch 1 cÃ¢u tiáº¿ng Anh sang tiáº¿ng PhÃ¡p
    """
    model.eval()
    
    # Tokenize
    tokens = tokenize_sentence(sentence, language="en")
    tokens = ['<sos>'] + tokens + ['<eos>']
    
    # Encode
    src_indexes = src_vocab.encode(tokens)
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.LongTensor([len(src_indexes)])
    
    # Encoder
    with torch.no_grad():
        hidden, cell = model.encoder(src_tensor, src_len)
    
    # Decoder (Greedy)
    trg_indexes = [tgt_vocab.word2idx['<sos>']]
    
    for _ in range(max_len):
        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)
        
        with torch.no_grad():
            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)
        
        pred_token = output.argmax(1).item()
        trg_indexes.append(pred_token)
        
        if pred_token == tgt_vocab.word2idx['<eos>']:
            break
    
    # Decode
    trg_tokens = tgt_vocab.decode(trg_indexes)
    return ' '.join(trg_tokens[1:-1])  # Bá» <sos> vÃ  <eos>
```

---

### Phá»¥ lá»¥c C: Link Checkpoint vÃ  Code Ä‘áº§y Ä‘á»§

**ğŸ”— Google Drive Links:**
```
1. Checkpoint files:
   - best_model.pth:  [Link Google Drive]
   - src_vocab.pth:   [Link Google Drive]
   - tgt_vocab.pth:   [Link Google Drive]

2. Full notebook:
   - NLP_Do_An_EnFr_Translation.ipynb: [Link Google Drive hoáº·c GitHub]

3. GitHub Repository:
   - https://github.com/[username]/NLP_CUOIKY
```

**âš ï¸ CÃ¡ch ná»™p checkpoint:**
- Option 1: ÄÃ­nh kÃ¨m trá»±c tiáº¿p file .pth vÃ o E-Learning (náº¿u < 100MB)
- Option 2: Upload lÃªn Google Drive, include link trong bÃ¡o cÃ¡o PDF
- Option 3: Upload lÃªn GitHub repository, include link trong bÃ¡o cÃ¡o

---

## ğŸ¨ Máº¸O THIáº¾T Káº¾ BÃO CÃO Äáº¸P

### 1. MÃ u sáº¯c vÃ  Ä‘á»‹nh dáº¡ng

**Sá»­ dá»¥ng mÃ u cho:**
- Header cÃ¡c section: MÃ u xanh dÆ°Æ¡ng (#2E86AB)
- Highlight code: Background xÃ¡m nháº¡t (#F5F5F5)
- ChÃº thÃ­ch hÃ¬nh: MÃ u xÃ¡m Ä‘áº­m (#666666)
- Link tham kháº£o: MÃ u xanh (#0066CC)

### 2. HÃ¬nh áº£nh vÃ  sÆ¡ Ä‘á»“

**Pháº£i cÃ³ Ã­t nháº¥t:**
- 1 sÆ¡ Ä‘á»“ kiáº¿n trÃºc tá»•ng thá»ƒ (trang 7)
- 2 biá»ƒu Ä‘á»“ loss (trang 13)
- 1 biá»ƒu Ä‘á»“ phÃ¢n bá»‘ lá»—i (trang 15)
- 5-10 áº£nh minh há»a khÃ¡c (cÃ´ng thá»©c, báº£ng, flowchart)

**Tool váº½ sÆ¡ Ä‘á»“:**
- **draw.io**: Miá»…n phÃ­, váº½ architecture
- **Matplotlib**: Váº½ biá»ƒu Ä‘á»“ loss tá»« notebook
- **LaTeX TikZ**: Váº½ cÃ´ng thá»©c toÃ¡n Ä‘áº¹p

### 3. Báº£ng biá»ƒu

**Format chuáº©n:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Header 1      â”‚ Header 2     â”‚ Header 3          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Data 1        â”‚ Data 2       â”‚ Data 3            â”‚
â”‚ Data 4        â”‚ Data 5       â”‚ Data 6            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Border: Thin line (0.5pt)
- Header: Bold, background xÃ¡m nháº¡t
- Alignment: Sá»‘ (pháº£i), Text (trÃ¡i)

### 4. Code trong bÃ¡o cÃ¡o

**Format code block:**
```python
# Font: Courier New 11pt
# Background: #F8F8F8
# Border: 1pt solid #DDDDDD
# Padding: 10px

def example_function():
    """Docstring"""
    return result
```

---

## âœ… CHECKLIST TRÆ¯á»šC KHI Ná»˜P (QUAN TRá»ŒNG!)

### ğŸš¨ YÃŠU Cáº¦U Báº®T BUá»˜C (theo Section 11):

**1. MÃ£ nguá»“n pháº£i cháº¡y Ä‘Æ°á»£c:**
- [ ] âœ… MÃ£ nguá»“n cháº¡y Ä‘Æ°á»£c trÃªn Google Colab hoáº·c mÃ¡y local
- [ ] âœ… ÄÃ£ test láº¡i toÃ n bá»™ notebook tá»« Ä‘áº§u (Runtime â†’ Restart and run all)
- [ ] âœ… KhÃ´ng cÃ³ lá»—i khi cháº¡y (ngoáº¡i trá»« warnings khÃ´ng áº£nh hÆ°á»Ÿng)
- [ ] âœ… Checkpoint `best_model.pth` Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng

**2. BÃ¡o cÃ¡o PDF pháº£i bao gá»“m (5 yÃªu cáº§u báº¯t buá»™c):**
- [ ] âœ… **SÆ¡ Ä‘á»“ kiáº¿n trÃºc** (Encoder-Decoder architecture) - cÃ³ trong Section 3.1
- [ ] âœ… **Biá»ƒu Ä‘á»“ train/val loss** (Training & Validation Loss curves) - cÃ³ trong Section 4.2
- [ ] âœ… **BLEU score** (tÃ­nh trÃªn test set) - cÃ³ trong Section 4.3
- [ ] âœ… **5 vÃ­ dá»¥ dá»‹ch + phÃ¢n tÃ­ch** (Source â†’ Prediction â†’ Reference â†’ Analysis) - cÃ³ trong Section 4.4
- [ ] âœ… **ChÆ°Æ¡ng trÃ¬nh nguá»“n** (trong Phá»¥ lá»¥c) - cÃ³ trong Phá»¥ lá»¥c B, C

**3. Checkpoint mÃ´ hÃ¬nh:**
- [ ] âœ… File `best_model.pth` Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] âœ… File `src_vocab.pth` Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] âœ… File `tgt_vocab.pth` Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] âœ… ÄÃ£ upload checkpoint lÃªn Google Drive (náº¿u file quÃ¡ lá»›n)
- [ ] âœ… Link Google Drive Ä‘Æ°á»£c include trong bÃ¡o cÃ¡o (Phá»¥ lá»¥c C)

**4. TÃ­nh trung thá»±c:**
- [ ] âš ï¸ **KHÃ”NG sao chÃ©p mÃ£ nguá»“n** tá»« internet/báº¡n bÃ¨ â†’ 0 Ä‘iá»ƒm náº¿u phÃ¡t hiá»‡n
- [ ] âœ… Code cÃ³ comment báº±ng tiáº¿ng Viá»‡t Ä‘á»ƒ thá»ƒ hiá»‡n sá»± hiá»ƒu biáº¿t
- [ ] âœ… BÃ¡o cÃ¡o viáº¿t báº±ng ngÃ´n ngá»¯ cá»§a báº£n thÃ¢n, khÃ´ng copy-paste

---

### ğŸ“‹ CHECKLIST Ná»˜I DUNG:

**Trang bÃ¬a & Má»¥c lá»¥c:**
- [ ] Trang bÃ¬a Ä‘áº§y Ä‘á»§: TÃªn trÆ°á»ng, Ä‘á» tÃ i, GVHD, SVTH, MSSV, lá»›p, ngÃ y thÃ¡ng
- [ ] Má»¥c lá»¥c cÃ³ sá»‘ trang chÃ­nh xÃ¡c
- [ ] CÃ¡c section Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ Ä‘Ãºng (1, 2, 3, 4, 5)

**Pháº§n chÃ­nh (Section 1-5):**
- [ ] Section 1: Giá»›i thiá»‡u (bá»‘i cáº£nh, má»¥c tiÃªu, pháº¡m vi)
- [ ] Section 2: CÃ¡c cÃ´ng trÃ¬nh liÃªn quan (â‰¥3 papers, cite Ä‘Ãºng)
- [ ] Section 3: PhÆ°Æ¡ng phÃ¡p (âœ… sÆ¡ Ä‘á»“ kiáº¿n trÃºc, cÃ´ng thá»©c toÃ¡n, pseudo-code)
- [ ] Section 4.2: âœ… Biá»ƒu Ä‘á»“ train/val loss (cÃ³ hÃ¬nh áº£nh)
- [ ] Section 4.3: âœ… BLEU score (cÃ³ con sá»‘ cá»¥ thá»ƒ, vÃ­ dá»¥: 23.4%)
- [ ] Section 4.4: âœ… 5 vÃ­ dá»¥ dá»‹ch (má»—i vÃ­ dá»¥ cÃ³: Source, Prediction, Reference, BLEU, PhÃ¢n tÃ­ch)
- [ ] Section 5: Káº¿t luáº­n (tá»•ng káº¿t, háº¡n cháº¿, hÆ°á»›ng phÃ¡t triá»ƒn)

**HÃ¬nh áº£nh & Báº£ng biá»ƒu:**
- [ ] Táº¥t cáº£ hÃ¬nh áº£nh cÃ³ caption (HÃ¬nh 1: ..., HÃ¬nh 2: ...)
- [ ] Táº¥t cáº£ hÃ¬nh áº£nh Ä‘Æ°á»£c reference trong text (xem HÃ¬nh 1, ...)
- [ ] Táº¥t cáº£ báº£ng biá»ƒu cÃ³ tiÃªu Ä‘á» (Báº£ng 1: ..., Báº£ng 2: ...)
- [ ] Biá»ƒu Ä‘á»“ loss rÃµ rÃ ng, cÃ³ trá»¥c x/y, legend
- [ ] SÆ¡ Ä‘á»“ kiáº¿n trÃºc dá»… hiá»ƒu, cÃ³ chÃº thÃ­ch

**TÃ i liá»‡u tham kháº£o:**
- [ ] CÃ³ â‰¥ 5 nguá»“n tham kháº£o
- [ ] Cite Ä‘Ãºng format IEEE (hoáº·c ACL)
- [ ] Táº¥t cáº£ citation trong text Ä‘á»u cÃ³ trong References
- [ ] Papers chÃ­nh: Sutskever 2014, Bahdanau 2015, Luong 2015

**Phá»¥ lá»¥c (âœ… ChÆ°Æ¡ng trÃ¬nh nguá»“n):**
- [ ] âœ… Phá»¥ lá»¥c A: Cáº¥u hÃ¬nh & siÃªu tham sá»‘
- [ ] âœ… Phá»¥ lá»¥c B: Code chÃ­nh (Encoder, Decoder, Seq2Seq, Training, Inference)
- [ ] âœ… Phá»¥ lá»¥c C: Link checkpoint vÃ  code Ä‘áº§y Ä‘á»§ (Google Drive hoáº·c GitHub)
- [ ] Code cÃ³ comment, indent Ä‘Ãºng, dá»… Ä‘á»c

---

### ğŸ“ CHECKLIST Äá»ŠNH Dáº NG:

**Font & Spacing:**
- [ ] Font Times New Roman 13pt (ná»™i dung chÃ­nh)
- [ ] Font 11pt hoáº·c Courier New cho code
- [ ] DÃ£n dÃ²ng 1.5 lines
- [ ] Lá»: TrÃ¡i 3cm, Pháº£i 2cm, TrÃªn 2.5cm, DÆ°á»›i 2.5cm

**Sá»‘ trang & Header:**
- [ ] Sá»‘ trang á»Ÿ cuá»‘i trang, giá»¯a (báº¯t Ä‘áº§u tá»« trang Giá»›i thiá»‡u)
- [ ] Trang bÃ¬a khÃ´ng Ä‘Ã¡nh sá»‘
- [ ] Má»¥c lá»¥c khÃ´ng Ä‘Ã¡nh sá»‘ (hoáº·c Ä‘Ã¡nh sá»‘ La MÃ£: i, ii, iii)

**Cháº¥t lÆ°á»£ng:**
- [ ] KhÃ´ng cÃ³ lá»—i chÃ­nh táº£
- [ ] KhÃ´ng cÃ³ lá»—i ngá»¯ phÃ¡p
- [ ] CÃ¢u vÄƒn máº¡ch láº¡c, rÃµ rÃ ng
- [ ] Thuáº­t ngá»¯ tiáº¿ng Anh Ä‘Æ°á»£c in nghiÃªng (Machine Translation, BLEU score)

---

### ğŸ’» CHECKLIST Ká»¸ THUáº¬T:

**Code cháº¥t lÆ°á»£ng:**
- [ ] Táº¥t cáº£ function cÃ³ docstring
- [ ] Code cÃ³ comment giáº£i thÃ­ch logic (báº±ng tiáº¿ng Viá»‡t)
- [ ] Naming convention rÃµ rÃ ng (src_vocab, tgt_vocab, khÃ´ng dÃ¹ng v1, v2)
- [ ] Code Ä‘Ã£ Ä‘Æ°á»£c format Ä‘áº¹p (indent Ä‘Ãºng)

**Káº¿t quáº£ thá»±c nghiá»‡m:**
- [ ] BLEU score â‰¥ 20% (theo yÃªu cáº§u Ä‘á» bÃ i)
- [ ] CÃ³ screenshot tá»« notebook chá»©ng minh káº¿t quáº£
- [ ] Biá»ƒu Ä‘á»“ loss cho tháº¥y model há»™i tá»¥ (train/val loss giáº£m)
- [ ] 5 vÃ­ dá»¥ dá»‹ch pháº£n Ã¡nh Ä‘a dáº¡ng: tá»‘t, khÃ¡, lá»—i OOV, lá»—i cÃ¢u dÃ i, lá»—i ngá»¯ phÃ¡p

**Checkpoint:**
- [ ] `best_model.pth` cÃ³ thá»ƒ load Ä‘Æ°á»£c báº±ng `torch.load()`
- [ ] Checkpoint cÃ³ kÃ­ch thÆ°á»›c há»£p lÃ½ (~50-150MB)
- [ ] ÄÃ£ test load checkpoint vÃ  dá»‹ch thá»­ 1 cÃ¢u â†’ káº¿t quáº£ Ä‘Ãºng

---

### ğŸ“¤ CHECKLIST Ná»˜P BÃ€I:

**TrÆ°á»›c khi ná»™p (48h trÆ°á»›c deadline):**
- [ ] ÄÃ£ cháº¡y láº¡i notebook hoÃ n chá»‰nh tá»« Ä‘áº§u (Ä‘á»ƒ cháº¯c cháº¯n khÃ´ng lá»—i)
- [ ] ÄÃ£ export notebook ra PDF (File â†’ Print â†’ Save as PDF)
- [ ] ÄÃ£ kiá»ƒm tra PDF: má»Ÿ Ä‘Æ°á»£c, khÃ´ng bá»‹ lá»—i font, hÃ¬nh áº£nh hiá»ƒn thá»‹ Ä‘Ãºng
- [ ] ÄÃ£ upload checkpoint lÃªn Google Drive, test link download

**File ná»™p:**
- [ ] 01 file PDF duy nháº¥t (tÃªn file: MSSV_HoTen_NLP_BaoCao.pdf)
- [ ] KÃ­ch thÆ°á»›c PDF < 50MB (náº¿u quÃ¡ lá»›n, nÃ©n hÃ¬nh áº£nh)
- [ ] PDF cÃ³ bookmark/má»¥c lá»¥c (náº¿u xuáº¥t tá»« Word)

**Ná»™p trÃªn E-Learning:**
- [ ] ÄÃ£ login E-Learning, tÃ¬m Ä‘Ãºng khÃ³a há»c NLP
- [ ] ÄÃ£ upload file PDF vÃ o Ä‘Ãºng assignment
- [ ] ÄÃ£ kiá»ƒm tra tráº¡ng thÃ¡i: "Submitted for grading"
- [ ] ÄÃ£ ghi chÃº: "Link checkpoint: [Google Drive URL]" (náº¿u cÃ³)
- [ ] Ná»™p TRÆ¯á»šC 23:59 ngÃ y 14/12/2025 (Ä‘á» phÃ²ng lá»—i há»‡ thá»‘ng)

**Sau khi ná»™p:**
- [ ] Chá»¥p áº£nh mÃ n hÃ¬nh submission success
- [ ] LÆ°u láº¡i báº£n PDF Ä‘Ã£ ná»™p (Ä‘á» phÃ²ng cáº§n resubmit)
- [ ] Giá»¯ nguyÃªn notebook trÃªn Colab (Ä‘á» phÃ²ng giáº£ng viÃªn yÃªu cáº§u demo)

---

## âš ï¸ LÆ¯U Ã CUá»I CÃ™NG

### ğŸš« TUYá»†T Äá»I TRÃNH:
1. **Ná»™p trá»…** â†’ 0 Ä‘iá»ƒm (khÃ´ng cÃ³ ngoáº¡i lá»‡)
2. **Sao chÃ©p code** â†’ 0 Ä‘iá»ƒm + ká»· luáº­t há»c táº­p
3. **BÃ¡o cÃ¡o thiáº¿u 5 yÃªu cáº§u báº¯t buá»™c** â†’ máº¥t Ä‘iá»ƒm náº·ng
4. **Code khÃ´ng cháº¡y Ä‘Æ°á»£c** â†’ máº¥t â‰¥50% Ä‘iá»ƒm pháº§n implementation

### âœ… Äá»‚ Äáº T ÄIá»‚M CAO:
1. **Ná»™p sá»›m** (1-2 ngÃ y trÆ°á»›c deadline) â†’ trÃ¡nh lá»—i há»‡ thá»‘ng
2. **BÃ¡o cÃ¡o chuyÃªn nghiá»‡p**: cÃ³ Ä‘á»§ 5 yÃªu cáº§u báº¯t buá»™c, format Ä‘áº¹p, khÃ´ng lá»—i chÃ­nh táº£
3. **Code cháº¥t lÆ°á»£ng**: cÃ³ comment, dá»… Ä‘á»c, cháº¡y Ä‘Æ°á»£c 100%
4. **Káº¿t quáº£ tá»‘t**: BLEU â‰¥ 25% (cao hÆ¡n yÃªu cáº§u 20%)
5. **PhÃ¢n tÃ­ch sÃ¢u**: 5 vÃ­ dá»¥ dá»‹ch cÃ³ phÃ¢n tÃ­ch chi tiáº¿t, thá»ƒ hiá»‡n hiá»ƒu biáº¿t vá» model

---

## ğŸ“¥ CÃCH XUáº¤T PDF Tá»ª NOTEBOOK

### PhÆ°Æ¡ng phÃ¡p 1: File â†’ Print â†’ Save as PDF

```python
# Trong Colab, cháº¡y cell nÃ y Ä‘á»ƒ chuáº©n bá»‹ export
from IPython.display import display, HTML

# áº¨n cell khÃ´ng cáº§n thiáº¿t
%%html
<style>
.input {display: none !important;}  /* áº¨n code cells */
</style>

# Sau Ä‘Ã³: File â†’ Print â†’ Save as PDF
```

### PhÆ°Æ¡ng phÃ¡p 2: nbconvert

```bash
# Local machine
jupyter nbconvert --to pdf NLP_Do_An_EnFr_Translation.ipynb

# Hoáº·c xuáº¥t HTML rá»“i in thÃ nh PDF
jupyter nbconvert --to html NLP_Do_An_EnFr_Translation.ipynb
# Má»Ÿ HTML â†’ Print â†’ Save as PDF
```

### PhÆ°Æ¡ng phÃ¡p 3: Viáº¿t bÃ¡o cÃ¡o riÃªng báº±ng Word/LaTeX

**Word:**
- Dá»… dÃ ng, WYSIWYG
- ChÃ¨n hÃ¬nh, báº£ng, code dá»… dÃ ng
- NhÆ°á»£c Ä‘iá»ƒm: CÃ´ng thá»©c toÃ¡n khÃ´ng Ä‘áº¹p

**LaTeX (Overleaf):**
- CÃ´ng thá»©c toÃ¡n Ä‘áº¹p
- Format chuyÃªn nghiá»‡p
- NhÆ°á»£c Ä‘iá»ƒm: Há»c lÃ¢u

**Khuyáº¿n nghá»‹: DÃ¹ng Word + MathType cho cÃ´ng thá»©c**

---

## ğŸ¯ MáºªU BÃO CÃO THAM KHáº¢O

### Link máº«u bÃ¡o cÃ¡o tá»‘t:

1. **Stanford CS224N Project Reports**
   - https://web.stanford.edu/class/cs224n/reports.html
   - BÃ¡o cÃ¡o sinh viÃªn vá» NMT, format chuáº©n

2. **ACL Anthology (Papers)**
   - https://aclanthology.org/
   - Papers vá» machine translation, há»c cÃ¡ch viáº¿t academic

3. **Template LaTeX cho NLP**
   - Overleaf: ACL 2023 template
   - https://www.overleaf.com/latex/templates/acl-2023-proceedings-template/

---

## â° TIMELINE Äá»€ XUáº¤T

**Tá»•ng thá»i gian: 3-5 ngÃ y**

### NgÃ y 1: Chuáº©n bá»‹ (2-3 giá»)
- [ ] Äá»c láº¡i requirements
- [ ] Cháº¡y notebook láº¥y káº¿t quáº£ (BLEU, plots)
- [ ] Screenshot cÃ¡c káº¿t quáº£ quan trá»ng
- [ ] Thu tháº­p tÃ i liá»‡u tham kháº£o

### NgÃ y 2: Viáº¿t nhÃ¡p (4-5 giá»)
- [ ] Pháº§n 1-2: Giá»›i thiá»‡u + Related Work
- [ ] Pháº§n 3: PhÆ°Æ¡ng phÃ¡p (váº½ sÆ¡ Ä‘á»“)
- [ ] Táº¡o template Word/LaTeX

### NgÃ y 3: Viáº¿t chÃ­nh (5-6 giá»)
- [ ] Pháº§n 4: Thá»±c nghiá»‡m (chÃ¨n báº£ng, hÃ¬nh)
- [ ] Pháº§n 5: Káº¿t luáº­n
- [ ] TÃ i liá»‡u tham kháº£o
- [ ] Phá»¥ lá»¥c

### NgÃ y 4-5: HoÃ n thiá»‡n (3-4 giá»)
- [ ] Kiá»ƒm tra lá»—i chÃ­nh táº£
- [ ] Format láº¡i toÃ n bá»™
- [ ] ÄÃ¡nh sá»‘ trang, má»¥c lá»¥c
- [ ] Export PDF
- [ ] Review láº§n cuá»‘i

---

## ğŸ“ Há»– TRá»¢

Náº¿u cáº§n há»— trá»£ thÃªm:
1. **Váº½ sÆ¡ Ä‘á»“ kiáº¿n trÃºc**: TÃ´i cÃ³ thá»ƒ táº¡o code draw.io hoáº·c TikZ
2. **Viáº¿t cÃ´ng thá»©c LaTeX**: TÃ´i cÃ³ thá»ƒ convert sang LaTeX
3. **Táº¡o template Word**: TÃ´i cÃ³ thá»ƒ táº¡o file .docx máº«u
4. **Review bÃ¡o cÃ¡o**: Gá»­i draft Ä‘á»ƒ tÃ´i gÃ³p Ã½

**ChÃºc báº¡n viáº¿t bÃ¡o cÃ¡o thÃ nh cÃ´ng! ğŸ‰**
